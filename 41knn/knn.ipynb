{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a32d9b7e",
   "metadata": {},
   "source": [
    "# K-Means Clustering Algorithm\n",
    "\n",
    "In this tutorial, we will understand the **geometric intuition** behind the **K-Means clustering algorithm**, a popular **unsupervised machine learning algorithm**.\n",
    "\n",
    "---\n",
    "\n",
    "## Geometric Intuition\n",
    "\n",
    "Let's consider we have some data points in **2D space** (x and y axes).  \n",
    "\n",
    "By visually inspecting the data, we might see that there are **two groups of points**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3442e97",
   "metadata": {},
   "source": [
    "Cluster 1: ● ● ●\n",
    "Cluster 2: ● ● ●\n",
    "\n",
    "\n",
    "After applying K-Means clustering, the points will be grouped into clusters:\n",
    "\n",
    "\n",
    "\n",
    "Cluster 1: ● ● ● (Centroid: ⊙)\n",
    "Cluster 2: ● ● ● (Centroid: ⊙)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5213631",
   "metadata": {},
   "source": [
    "\n",
    "Similarly, if the data has **three groups**, K-Means will output three clusters with three centroids.\n",
    "\n",
    "**Key Idea:** K-Means clusters **similar points together**.\n",
    "\n",
    "---\n",
    "\n",
    "## Steps in K-Means Clustering\n",
    "\n",
    "### Step 1: Initialize Centroids\n",
    "\n",
    "We start by selecting **k centroids**, where `k` is the number of clusters we want.  \n",
    "\n",
    "- Centroids can be **randomly initialized**.\n",
    "- For example, if `k=2`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48aa848",
   "metadata": {},
   "source": [
    "Centroid 1: ⊙\n",
    "Centroid 2: ⊙"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79c714f",
   "metadata": {},
   "source": [
    "\n",
    "> How to choose `k` will be discussed later.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Assign Points to Nearest Centroid\n",
    "\n",
    "For each data point, we calculate its **distance** to all centroids and assign it to the nearest one.\n",
    "\n",
    "- **Distance metrics** can be:\n",
    "  - **Euclidean distance**\n",
    "  - **Manhattan distance**\n",
    "\n",
    "$$\n",
    "\\text{Euclidean Distance between } P(x_1, y_1) \\text{ and } Q(x_2, y_2) = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Manhattan Distance} = |x_2 - x_1| + |y_2 - y_1|\n",
    "$$\n",
    "\n",
    "Points assigned to centroids are marked with the **centroid's color**.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Move Centroids\n",
    "\n",
    "Once points are assigned:\n",
    "\n",
    "1. Compute the **average of all points** in each cluster.\n",
    "2. Update the **centroid location** to this average.\n",
    "\n",
    "$$\n",
    "\\text{New Centroid} = \\frac{\\sum_{i=1}^{n} P_i}{n}\n",
    "$$\n",
    "\n",
    "Repeat **Step 2** and **Step 3** until:\n",
    "\n",
    "- **Centroids stop moving** OR\n",
    "- **Points no longer change clusters**\n",
    "\n",
    "At convergence, we get the **final clusters**.\n",
    "\n",
    "---\n",
    "\n",
    "### Example of Iteration\n",
    "\n",
    "- **Iteration 1:**\n",
    "  - Centroids randomly initialized.\n",
    "  - Points assigned to nearest centroid.\n",
    "\n",
    "- **Iteration 2:**\n",
    "  - Compute average of points → move centroids.\n",
    "  - Reassign points to nearest centroid.\n",
    "\n",
    "- **Iteration 3 (Convergence):**\n",
    "  - Centroids stabilize.\n",
    "  - Final clusters obtained.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of K-Means Steps\n",
    "\n",
    "1. Initialize `k` centroids (randomly).\n",
    "2. Assign each point to its nearest centroid.\n",
    "3. Update centroids by computing the average of points in the cluster.\n",
    "4. Repeat Steps 2 and 3 until convergence.\n",
    "\n",
    "> Once points stop changing clusters, the algorithm **stops**.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Topic\n",
    "\n",
    "**How to select the `k` value** effectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a46d03",
   "metadata": {},
   "source": [
    "Question: How do we select the k value?\n",
    "\n",
    "\n",
    "We will discuss this in the next video.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- K-Means clusters similar points together.\n",
    "- Centroids represent the **center of each cluster**.\n",
    "- Distance metrics like **Euclidean** or **Manhattan** are used.\n",
    "- Iterative process: Assign → Update → Repeat → Converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e542d",
   "metadata": {},
   "source": [
    "# Selecting the k Value in K-Means Clustering\n",
    "\n",
    "In the previous discussion, we applied K-Means clustering with a known `k` value (e.g., `k=2`).  \n",
    "In real-world scenarios, **overlapping clusters** make it hard to choose `k`. Here, we discuss how to select `k`.\n",
    "\n",
    "---\n",
    "\n",
    "## Within-Cluster Sum of Squares (WCSS)\n",
    "\n",
    "We introduce a key metric: **Within-Cluster Sum of Squares (WCSS)**.\n",
    "\n",
    "$$\n",
    "\\text{WCSS} = \\sum_{i=1}^{n} \\| x_i - \\mu_{c(i)} \\|^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\(x_i\\) = data point\n",
    "- \\(\\mu_{c(i)}\\) = centroid of the cluster to which \\(x_i\\) belongs\n",
    "- \\(n\\) = total number of points\n",
    "\n",
    "**Intuition:** WCSS measures **compactness** of clusters. Smaller WCSS → points are closer to centroids.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Compute WCSS for Different k Values\n",
    "\n",
    "1. Initialize `k` from 1 to some maximum (e.g., 20).\n",
    "2. For each `k`:\n",
    "   - Run K-Means clustering.\n",
    "   - Compute WCSS.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- **k = 1** → WCSS is high (all points in one cluster).\n",
    "- **k = 2** → WCSS decreases (points split into 2 clusters).\n",
    "- **k = 3, 4, …** → WCSS continues decreasing, eventually stabilizing.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Use the Elbow Method\n",
    "\n",
    "Plot **WCSS vs k**.  \n",
    "\n",
    "- The plot usually looks like an **arm**.\n",
    "- The point where **WCSS abruptly decreases** and then stabilizes is called the **elbow**.\n",
    "- The corresponding `k` at the elbow is selected as the optimal number of clusters.\n",
    "\n",
    "**Elbow Method Illustration:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38a7b11",
   "metadata": {},
   "source": [
    "\n",
    "> The \"elbow\" indicates diminishing returns in reducing WCSS. Select `k` at the elbow.\n",
    "\n",
    "---\n",
    "\n",
    "## Distance Metrics in K-Means\n",
    "\n",
    "### 1. Euclidean Distance\n",
    "\n",
    "Used to compute the **straight-line distance** between two points.\n",
    "\n",
    "For 2D points \\(P_1=(x_1, y_1)\\) and \\(P_2=(x_2, y_2)\\):\n",
    "\n",
    "$$\n",
    "d_{Euclidean} = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n",
    "$$\n",
    "\n",
    "For 3D points, extend to include the z-coordinate:\n",
    "\n",
    "$$\n",
    "d_{Euclidean} = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2 + (z_2 - z_1)^2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Manhattan Distance\n",
    "\n",
    "Used when movement is restricted to **grid-like paths** (e.g., city streets).\n",
    "\n",
    "For 2D points:\n",
    "\n",
    "$$\n",
    "d_{Manhattan} = |x_2 - x_1| + |y_2 - y_1|\n",
    "$$`\n",
    "\n",
    "**Intuition:** Travel along axes rather than straight line.\n",
    "\n",
    "**Example:**  \n",
    "- Grid-like city → use Manhattan distance.  \n",
    "- Open plane → use Euclidean distance.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Insights\n",
    "\n",
    "- Use **Elbow Method** to determine `k`.\n",
    "- Use **Euclidean distance** when points can move freely.\n",
    "- Use **Manhattan distance** when movement is along grid lines.\n",
    "- Initialization matters: Randomly placing centroids **too close** may cause poor clustering. Techniques like **k-means++** help with better initialization (to be discussed in the next video).\n",
    "\n",
    "---\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **WCSS** measures cluster compactness.\n",
    "2. **Elbow Method** helps find the optimal `k`.\n",
    "3. **Euclidean vs Manhattan distance** depends on geometry and constraints.\n",
    "4. Proper initialization improves clustering quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c9b72c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
