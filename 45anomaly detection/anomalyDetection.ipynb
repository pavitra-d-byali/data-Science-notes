{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550e7b13",
   "metadata": {},
   "source": [
    "# üß† Unsupervised Machine Learning ‚Äì Anomaly Detection (Isolation Forest)\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Introduction\n",
    " \n",
    "We‚Äôre going to continue our discussion on **Unsupervised Machine Learning**, focusing today on **<span style=\"color:orange\">Anomaly Detection</span>**.\n",
    "\n",
    "Whenever we talk about anomaly detection, it basically means detecting **<span style=\"color:red\">outliers</span>** ‚Äî data points that deviate significantly from the rest.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Real-World Examples\n",
    "\n",
    "1. **Bank Account Security**  \n",
    "   Imagine your bank account is in India.  \n",
    "   If someone tries to log in using your credentials from another country, your bank immediately sends a security alert.  \n",
    "   How is this detected?  \n",
    "   ‚Üí By identifying this login as an **<span style=\"color:red\">anomaly</span>**, since it‚Äôs an unusual event.\n",
    "\n",
    "2. **Cancer Detection**  \n",
    "   Only a small subset of patients have cancer.  \n",
    "   Those rare cases act as **outliers**.  \n",
    "   Detecting these anomalies helps save lives.\n",
    "\n",
    "3. **Cybersecurity (Fake IPs)**  \n",
    "   If a hacker uses a suspicious IP to access a server, anomaly detection can flag it as an outlier.\n",
    "\n",
    "4. **Cricket Example (IPL)**  \n",
    "   Suppose in IPL, the runs per over are:  \n",
    "   15, 10, 12, **100**.  \n",
    "   Clearly, scoring **100 runs in one over** is impossible.  \n",
    "   Hence, it‚Äôs an **outlier** ‚Äî an anomaly.\n",
    "\n",
    "---\n",
    "\n",
    "## üå≤ Isolation Forest (Concept)\n",
    "\n",
    "The **<span style=\"color:orange\">Isolation Forest</span>** is an **unsupervised anomaly detection technique** based on **decision trees**.\n",
    "\n",
    "Even though it uses trees internally, it **does not require labeled data**.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∏ Basic Idea\n",
    "\n",
    "Consider two features:  \n",
    "$$\n",
    "f_1, f_2\n",
    "$$\n",
    "\n",
    "Now, plot your data points ‚Äî most will form **clusters**, but a few will be far away.  \n",
    "Those isolated points are **potential anomalies**.\n",
    "\n",
    "Isolation Forest works by **isolating** these data points.\n",
    "\n",
    "- It builds multiple **Isolation Trees**.  \n",
    "- Each tree **randomly splits features** until every data point becomes a **leaf node**.\n",
    "\n",
    "If a data point is **isolated in fewer splits**, it‚Äôs more likely an **outlier**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Example Visualization\n",
    "\n",
    "Let‚Äôs imagine a 2D feature space:\n",
    "\n",
    "- Clustered region ‚Üí Normal points  \n",
    "- A few distant points ‚Üí Outliers\n",
    "\n",
    "The algorithm splits the data like a decision tree:\n",
    "- Some splits isolate normal points deeper in the tree.  \n",
    "- Outliers are isolated **quickly**, in **fewer paths (shallow depth)**.\n",
    "\n",
    "Hence:\n",
    "- **Fewer splits ‚Üí Higher anomaly likelihood**\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Mathematical Formulation\n",
    "\n",
    "To compute the **anomaly score**, we use:\n",
    "\n",
    "$$\n",
    "s(x, m) = 2^{-\\frac{E[h(x)]}{c(m)}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ú≥Ô∏è Where:\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|:--|:--|\n",
    "| $$x$$ | Data point for which we calculate anomaly score |\n",
    "| $$m$$ | Sample size (number of data points) |\n",
    "| $$h(x)$$ | Path length (depth to isolate $$x$$ in an isolation tree) |\n",
    "| $$E[h(x)]$$ | Average path length for $$x$$ across all trees |\n",
    "| $$c(m)$$ | Average path length of unsuccessful searches in a Binary Search Tree of size $$m$$ |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Interpretation\n",
    "\n",
    "### üß† Interpretation\n",
    "\n",
    "- If a point is isolated **quickly**:\n",
    "\n",
    "  $$\n",
    "  E[h(x)] \\ll c(m)\n",
    "  $$\n",
    "\n",
    "  then\n",
    "\n",
    "  $$\n",
    "  s(x, m) \\approx 1\n",
    "  $$\n",
    "\n",
    "  ‚Üí meaning **high anomaly score** ‚Üí **outlier**\n",
    "\n",
    "---\n",
    "\n",
    "- If a point is isolated **slowly**:\n",
    "\n",
    "  $$\n",
    "  E[h(x)] \\gg c(m)\n",
    "  $$\n",
    "\n",
    "  then\n",
    "\n",
    "  $$\n",
    "  s(x, m) < 0.5\n",
    "  $$\n",
    "\n",
    "  ‚Üí meaning **normal point**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Threshold Rule\n",
    "\n",
    "We can set a **threshold (œÑ)** for anomaly classification:\n",
    "\n",
    "$$\n",
    "\\text{If } s(x, m) > \\tau \\Rightarrow \\text{Outlier}\n",
    "$$\n",
    "\n",
    "Usually,  \n",
    "$$\n",
    "\\tau = 0.5\n",
    "$$  \n",
    "but it can be tuned depending on the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Implementation Example (Python)\n",
    "\n",
    "Let‚Äôs use the **Isolation Forest** from **scikit-learn**.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example health dataset with two features\n",
    "df = pd.read_csv(\"health_data.csv\")\n",
    "\n",
    "# Initialize Isolation Forest\n",
    "clf = IsolationForest(contamination=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "clf.fit(df)\n",
    "\n",
    "# Predict anomalies\n",
    "pred = clf.predict(df)\n",
    "\n",
    "# 1 ‚Üí normal, -1 ‚Üí anomaly\n",
    "print(pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85109cab",
   "metadata": {},
   "source": [
    "# Get indices of outliers\n",
    "outlier_index = np.where(pred < 0)\n",
    "\n",
    "print(outlier_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb1f67",
   "metadata": {},
   "source": [
    "Visualizing Outliers\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.scatter(df.iloc[:, 0], df.iloc[:, 1], color='blue', label='Normal Data')\n",
    "\n",
    "\n",
    "# Highlight anomalies\n",
    "plt.scatter(df.iloc[outlier_index, 0],\n",
    "\n",
    "            df.iloc[outlier_index, 1],\n",
    "            \n",
    "            edgecolor='red',\n",
    "            \n",
    "            facecolor='none',\n",
    "            \n",
    "            s=80,\n",
    "            \n",
    "            label='Outliers')\n",
    "            \n",
    "\n",
    "plt.title(\"Isolation Forest ‚Äì Anomaly Detection\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "üéØ Output Interpretation\n",
    "\n",
    "Blue dots ‚Üí Normal data points\n",
    "\n",
    "Red-circled dots ‚Üí Detected anomalies\n",
    "\n",
    "These isolated points are the ones identified by the Isolation Forest as potential outliers.\n",
    "\n",
    "üß≠ Summary\n",
    "Concept\tDescription\n",
    "\n",
    "Algorithm\tIsolation Forest\n",
    "\n",
    "Type\tUnsupervised\n",
    "\n",
    "Core Idea\tIsolate anomalies quickly using random splits\n",
    "\n",
    "Output\tAnomaly score between 0 and 1\n",
    "\n",
    "Threshold\t> 0.5 ‚Üí Outlier\n",
    "\n",
    "Advantage\tFast, efficient, and scalable for high-dimensional data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1ed9cb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
