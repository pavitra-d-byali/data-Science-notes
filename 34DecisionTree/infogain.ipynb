{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24e8977",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier ‚Äî Information Gain\n",
    "\n",
    "In the previous discussion, we understood **Gini Index**, **Gini Impurity**, and **Entropy** ‚Äî all of which help us **measure the purity of a split** in a Decision Tree.\n",
    "\n",
    "Now, let‚Äôs understand **Information Gain**, which helps us **decide which feature to split on**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What Is Information Gain?\n",
    "\n",
    "When we have multiple features (say ( F_1, F_2, F_3 )), we need to decide **which feature to use first** for splitting the tree.\n",
    "\n",
    "The **Information Gain (IG)** helps us determine this by measuring **the reduction in entropy** after the dataset is split on a feature.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula for Information Gain\n",
    "\n",
    "$$\n",
    "Gain(S, \\text{feature}) = H(S) - \\sum_{v \\in \\text{Values(feature)}} \\frac{|S_v|}{|S|} , H(S_v)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* ( H(S) ): Entropy of the **root node** (before the split)\n",
    "* ( S_v ): Subset of data for which feature = ( v )\n",
    "* ( H(S_v) ): Entropy of subset ( S_v )\n",
    "* ( \\frac{|S_v|}{|S|} ): Weighted proportion of samples in subset ( S_v )\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Example ‚Äî Calculating Information Gain\n",
    "\n",
    "Suppose we have a dataset with **feature F1** and the following class distribution:\n",
    "\n",
    "| Node                | Yes | No | Total |\n",
    "| ------------------- | --- | -- | ----- |\n",
    "| Root                | 9   | 5  | 14    |\n",
    "| C1 (after F1 split) | 6   | 2  | 8     |\n",
    "| C2 (after F1 split) | 3   | 3  | 6     |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Calculate Root Entropy\n",
    "\n",
    "$$\n",
    "H(S) = -p_{+} \\log_2(p_{+}) - p_{-} \\log_2(p_{-})\n",
    "$$\n",
    "\n",
    "Substitute:\n",
    "\n",
    "$$\n",
    "p_{+} = \\frac{9}{14}, \\quad p_{-} = \\frac{5}{14}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "H(S) = -\\frac{9}{14}\\log_2\\left(\\frac{9}{14}\\right) - \\frac{5}{14}\\log_2\\left(\\frac{5}{14}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "H(S) \\approx 0.94\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Calculate Entropy for Each Child Node\n",
    "\n",
    "#### For Category C1 (6 Yes, 2 No)\n",
    "\n",
    "$$\n",
    "H(C1) = -\\frac{6}{8}\\log_2\\left(\\frac{6}{8}\\right) - \\frac{2}{8}\\log_2\\left(\\frac{2}{8}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "H(C1) \\approx 0.81\n",
    "$$\n",
    "\n",
    "#### For Category C2 (3 Yes, 3 No)\n",
    "\n",
    "Since this is a perfectly impure split (( p_{+} = p_{-} = 0.5 )):\n",
    "\n",
    "$$\n",
    "H(C2) = 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Calculate Weighted Average Entropy\n",
    "\n",
    "$$\n",
    "\\sum_{v \\in \\text{Values(F1)}} \\frac{|S_v|}{|S|} , H(S_v)\n",
    "$$\n",
    "\n",
    "Substitute values:\n",
    "\n",
    "$$\n",
    "= \\frac{8}{14} \\times 0.81 + \\frac{6}{14} \\times 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.463 + 0.429 = 0.892\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Compute Information Gain\n",
    "\n",
    "$$\n",
    "Gain(S, F1) = H(S) - \\sum_{v} \\frac{|S_v|}{|S|}H(S_v)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Gain(S, F1) = 0.94 - 0.892 = 0.049\n",
    "$$\n",
    "\n",
    "So, the **Information Gain for Feature F1 = 0.049**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Comparing Features\n",
    "\n",
    "Let‚Äôs assume another feature ( F2 ) gives the following result after splitting:\n",
    "\n",
    "$$\n",
    "Gain(S, F2) > Gain(S, F1)\n",
    "$$\n",
    "\n",
    "Then we **choose ( F2 )** as the **root split feature** for our Decision Tree, because it **reduces entropy the most**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 5. Key Intuition\n",
    "\n",
    "* Higher **Information Gain** ‚Üí better split ‚Üí higher purity in child nodes\n",
    "* Decision Trees use Information Gain (or Gini Index) to **select features and build hierarchy**\n",
    "* Entropy focuses on **information content**, Gini focuses on **misclassification**\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Next Topic\n",
    "\n",
    "In the next discussion, we‚Äôll answer a common interview question:\n",
    "\n",
    "> **When should we use Entropy, and when should we use Gini Impurity?**\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "* Information Gain tells **which feature to split first**.\n",
    "* It uses **Entropy** (or Gini Impurity) as a base.\n",
    "* Decision Tree internally calculates IG for all features and **chooses the one with maximum gain**.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Video:** *Entropy vs Gini ‚Äî When to Use Which*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a925ed",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier ‚Äî Entropy vs Gini Impurity\n",
    "\n",
    "In the previous discussion, we learned about **Entropy** and **Gini Impurity** ‚Äî two different techniques used to measure the **purity of a split** in Decision Trees.\n",
    "\n",
    "Now, let‚Äôs understand **when to use Entropy** and **when to use Gini Impurity**, which is a very common and important question in interviews.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Recap: Entropy Formula\n",
    "\n",
    "The **Entropy** formula measures the amount of disorder or uncertainty in a dataset.\n",
    "\n",
    "For a binary classification:\n",
    "\n",
    "$$\n",
    "H(S) = -p_{+}\\log_2(p_{+}) - p_{-}\\log_2(p_{-})\n",
    "$$\n",
    "\n",
    "If there are more than two categories, the formula expands as:\n",
    "\n",
    "$$\n",
    "H(S) = -\\sum_{i=1}^{k} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "where ( k ) = number of classes (categories).\n",
    "\n",
    "For example, if there are three output categories:\n",
    "\n",
    "$$\n",
    "H(S) = -p_1\\log_2(p_1) - p_2\\log_2(p_2) - p_3\\log_2(p_3)\n",
    "$$\n",
    "\n",
    "As the number of output categories increases, this formula simply expands ‚Äî no conceptual change.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Recap: Gini Impurity Formula\n",
    "\n",
    "The **Gini Impurity** is defined as:\n",
    "\n",
    "$$\n",
    "G(S) = 1 - \\sum_{i=1}^{k} p_i^2\n",
    "$$\n",
    "\n",
    "Like Entropy, this also expands automatically as the number of output categories increases.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. When to Use Entropy vs Gini Impurity\n",
    "\n",
    "Let‚Äôs answer the key question.\n",
    "\n",
    "### üîπ Entropy\n",
    "\n",
    "* **When to use:**\n",
    "  Use **Entropy** when your dataset is **small** (for example, up to ~10,000 records).\n",
    "\n",
    "* **Why:**\n",
    "  The Entropy formula involves **logarithmic computations**, which are **more computationally expensive**.\n",
    "  However, for smaller datasets, the time difference between using Entropy and Gini is **negligible**.\n",
    "\n",
    "* **Interpretation:**\n",
    "  Entropy gives a **richer sense of uncertainty** and is based on **information theory**, so it‚Äôs often used when **interpretability** is important.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Gini Impurity\n",
    "\n",
    "* **When to use:**\n",
    "  Use **Gini Impurity** when your dataset is **large** (hundreds of thousands or millions of records).\n",
    "\n",
    "* **Why:**\n",
    "  Gini does **not involve log computations**, making it **faster and more efficient** for large datasets.\n",
    "\n",
    "* **Interpretation:**\n",
    "  Gini focuses on **misclassification probability** and is computationally simpler.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Practical Guidelines\n",
    "\n",
    "| Aspect                  | Entropy                                             | Gini Impurity                     |\n",
    "| ----------------------- | --------------------------------------------------- | --------------------------------- |\n",
    "| Formula                 | ( -\\sum p_i \\log_2(p_i) )                           | ( 1 - \\sum p_i^2 )                |\n",
    "| Computational Speed     | Slower (uses log)                                   | Faster                            |\n",
    "| When to Use             | Small datasets                                      | Large datasets                    |\n",
    "| Default in scikit-learn | No (use `criterion='entropy'`)                      | Yes (default: `criterion='gini'`) |\n",
    "| Theoretical Basis       | Information Theory                                  | Probability Theory                |\n",
    "| Sensitivity             | Slightly more sensitive to changes in probabilities | Slightly less sensitive           |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Key Takeaways\n",
    "\n",
    "* Both Entropy and Gini measure **node impurity**.\n",
    "* **Gini** is the **default** in most implementations like `DecisionTreeClassifier` in scikit-learn.\n",
    "* **Entropy** can be used when you want a **theoretically stronger measure** of information.\n",
    "* For **most problem statements**, **Gini Impurity** works perfectly fine.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. What‚Äôs Next?\n",
    "\n",
    "In the next section, we‚Äôll discuss **how Decision Trees handle continuous features** ‚Äî i.e.,\n",
    "\n",
    "> How to split when the feature values are continuous (numerical) rather than categorical.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "* **Entropy** ‚Üí better interpretability, slower computation\n",
    "* **Gini Impurity** ‚Üí faster computation, widely used by default\n",
    "\n",
    "---\n",
    "\n",
    "**Next Topic:** *Handling Continuous Features in Decision Trees*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6766e4e1",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier ‚Äî Splitting for Numerical (Continuous) Features\n",
    "\n",
    "In our previous discussion, we saw how **Decision Trees** handle **categorical features**, where it‚Äôs easy to divide the dataset based on category values (e.g., ‚ÄúYes‚Äù, ‚ÄúNo‚Äù, ‚ÄúRed‚Äù, ‚ÄúBlue‚Äù).\n",
    "\n",
    "But what if one of our features is **continuous (numerical)**?\n",
    "How should the Decision Tree decide **where to split** such a feature?\n",
    "\n",
    "That‚Äôs what we‚Äôll learn in this section.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Problem Statement\n",
    "\n",
    "Suppose we have a numerical feature ( X ) with the following sorted values:\n",
    "\n",
    "| Record | Feature Value (X) | Output |\n",
    "| ------ | ----------------- | ------ |\n",
    "| 1      | 2.3               | Yes    |\n",
    "| 2      | 3.6               | Yes    |\n",
    "| 3      | 4.1               | No     |\n",
    "| 4      | 4.5               | Yes    |\n",
    "| 5      | 5.0               | No     |\n",
    "| 6      | 6.2               | Yes    |\n",
    "| 7      | 6.8               | No     |\n",
    "\n",
    "You can see that the feature values are **continuous** and **sorted in ascending order**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Step 1: Sort the Feature Values\n",
    "\n",
    "Before creating splits, we always **sort** the feature values.\n",
    "This ensures we can efficiently find the best **thresholds** between adjacent values.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Step 2: Choose Thresholds and Create Candidate Splits\n",
    "\n",
    "Now, we consider each possible **threshold value** between consecutive samples.\n",
    "\n",
    "For example:\n",
    "\n",
    "| Split No. | Threshold | Split Condition | Left Subset | Right Subset |\n",
    "| --------- | --------- | --------------- | ----------- | ------------ |\n",
    "| 1         | 2.3       | ( X \\le 2.3 )   | 1 record    | 6 records    |\n",
    "| 2         | 3.6       | ( X \\le 3.6 )   | 2 records   | 5 records    |\n",
    "| 3         | 4.1       | ( X \\le 4.1 )   | 3 records   | 4 records    |\n",
    "| 4         | 4.5       | ( X \\le 4.5 )   | 4 records   | 3 records    |\n",
    "| 5         | 5.0       | ( X \\le 5.0 )   | 5 records   | 2 records    |\n",
    "| 6         | 6.2       | ( X \\le 6.2 )   | 6 records   | 1 record     |\n",
    "\n",
    "Each of these thresholds creates **two groups**:\n",
    "\n",
    "* Left node: records where ( X \\le \\text{threshold} )\n",
    "* Right node: records where ( X > \\text{threshold} )\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Step 3: Calculate Entropy or Gini Impurity for Each Split\n",
    "\n",
    "For each candidate threshold, we calculate:\n",
    "\n",
    "1. The **entropy (or Gini impurity)** of the left and right nodes.\n",
    "2. The **weighted average impurity** after the split.\n",
    "3. The **Information Gain** for that split.\n",
    "\n",
    "The formula remains the same:\n",
    "\n",
    "$$\n",
    "Gain(S, \\text{feature}) = H(S) - \\sum_{v \\in \\text{splits}} \\frac{|S_v|}{|S|} H(S_v)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Step 4: Select the Best Threshold\n",
    "\n",
    "After calculating the **Information Gain** for each possible threshold,\n",
    "we choose the **threshold with the highest gain**.\n",
    "\n",
    "For example, if the highest gain is obtained when ( X = 4.0 ),\n",
    "then the split condition will be:\n",
    "\n",
    "$$\n",
    "X \\le 4.0\n",
    "$$\n",
    "\n",
    "This becomes the **root node condition** for that feature.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Step 5: Repeat the Process Recursively\n",
    "\n",
    "After choosing the best split:\n",
    "\n",
    "* Repeat the same process for each **child node**,\n",
    "* Continue until a **stopping condition** (like max depth or pure node) is met.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Disadvantage\n",
    "\n",
    "This method is **computationally expensive**, especially for large datasets.\n",
    "\n",
    "If we have **millions of records** and **multiple numerical features**,\n",
    "the model must evaluate **many possible thresholds** for each feature ‚Äî\n",
    "making the **time complexity very high**.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Key Takeaways\n",
    "\n",
    "| Step | Description                                       |\n",
    "| ---- | ------------------------------------------------- |\n",
    "| 1    | Sort numerical feature values                     |\n",
    "| 2    | Create all possible threshold splits              |\n",
    "| 3    | Calculate entropy or Gini impurity for each split |\n",
    "| 4    | Compute information gain                          |\n",
    "| 5    | Select the threshold with maximum gain            |\n",
    "| 6    | Repeat recursively for all nodes                  |\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Summary\n",
    "\n",
    "* Decision Trees handle **numerical features** by finding **optimal threshold splits**.\n",
    "* The **Information Gain** (or **Gini decrease**) determines which threshold is best.\n",
    "* This process is **computationally expensive**, but **essential** for decision trees to work correctly.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Topic:** *Optimizing Decision Trees and Handling Overfitting (Pruning Techniques)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4485d3d0",
   "metadata": {},
   "source": [
    "# üå≥ Pre-Pruning and Post-Pruning in Decision Trees\n",
    "\n",
    "In this discussion, we‚Äôll explore two important techniques used in Decision Trees: **Pre-pruning** and **Post-pruning**.\n",
    "These methods are crucial for preventing **overfitting** and improving the model‚Äôs generalization on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## üå± What is Pruning?\n",
    "\n",
    "Just like a gardener prunes a plant to maintain its shape and promote healthy growth, **pruning** in Decision Trees involves **cutting down branches** that add unnecessary complexity and do not contribute significantly to model accuracy.\n",
    "\n",
    "When a Decision Tree grows to its maximum depth without restriction, it tends to **overfit** ‚Äî meaning it performs extremely well on training data but poorly on test data.\n",
    "\n",
    "---\n",
    "\n",
    "## üö® Overfitting in Decision Trees\n",
    "\n",
    "If we train a Decision Tree with default parameters, it continues splitting the data **until every leaf node is pure** (i.e., all samples in that node belong to the same class).\n",
    "This often leads to:\n",
    "\n",
    "* **Very high training accuracy**\n",
    "* **Low test accuracy**\n",
    "* A situation known as **Overfitting**\n",
    "\n",
    "In overfitting:\n",
    "\n",
    "* **Bias** is *low* (the model fits the training data too well)\n",
    "* **Variance** is *high* (the model performs poorly on unseen data)\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÇÔ∏è Techniques to Reduce Overfitting\n",
    "\n",
    "To prevent overfitting, we can use two types of pruning:\n",
    "\n",
    "1. **Post-Pruning (Cost Complexity Pruning)**\n",
    "2. **Pre-Pruning (Early Stopping)**\n",
    "\n",
    "---\n",
    "\n",
    "## üåø Post-Pruning (a.k.a. Cost Complexity Pruning)\n",
    "\n",
    "### üîç Concept\n",
    "\n",
    "* In **Post-Pruning**, we **first construct the full Decision Tree** (allowing it to grow completely).\n",
    "* Then, we **cut back** some branches that do not add significant predictive power.\n",
    "\n",
    "### üß† Example\n",
    "\n",
    "Suppose a node contains **9 \"Yes\"** and **2 \"No\"** samples.\n",
    "The tree continues to split it further until it creates two pure leaf nodes:\n",
    "\n",
    "* One node with **9 Yes, 0 No**\n",
    "* Another with **0 Yes, 2 No**\n",
    "\n",
    "However, this additional split may not improve accuracy much.\n",
    "Instead, we can **prune** this branch and **treat the parent node (9 Yes, 2 No)** as a **leaf node** with the label **\"Yes\"**.\n",
    "\n",
    "### ‚öôÔ∏è Steps\n",
    "\n",
    "1. Build the **complete Decision Tree**.\n",
    "2. Evaluate the **accuracy** (or cost complexity score) for each subtree.\n",
    "3. **Remove branches** that increase model complexity without significant gain in accuracy.\n",
    "\n",
    "### üí° When to Use\n",
    "\n",
    "* Effective for **small datasets**.\n",
    "* Takes more computation time because the tree is built fully before pruning.\n",
    "\n",
    "---\n",
    "\n",
    "## üåæ Pre-Pruning (a.k.a. Early Stopping)\n",
    "\n",
    "### üîç Concept\n",
    "\n",
    "* In **Pre-Pruning**, we stop the tree **early during its construction** ‚Äî before it becomes too complex.\n",
    "* This is done by **setting constraints** on the tree growth through **hyperparameters**.\n",
    "\n",
    "### ‚öôÔ∏è Common Hyperparameters in scikit-learn\n",
    "\n",
    "| Parameter           | Description                                                                    |\n",
    "| ------------------- | ------------------------------------------------------------------------------ |\n",
    "| `max_depth`         | Maximum depth of the tree. Limits how deep the splits can go.                  |\n",
    "| `min_samples_split` | Minimum number of samples required to split a node.                            |\n",
    "| `min_samples_leaf`  | Minimum samples required at a leaf node.                                       |\n",
    "| `max_features`      | Maximum number of features considered for splitting.                           |\n",
    "| `criterion`         | Function to measure the quality of a split (`gini`, `entropy`, or `log_loss`). |\n",
    "| `splitter`          | Strategy used to choose the split (`best` or `random`).                        |\n",
    "\n",
    "These hyperparameters can be tuned using **GridSearchCV** to find the optimal balance between bias and variance.\n",
    "\n",
    "### üí° When to Use\n",
    "\n",
    "* Ideal for **large datasets**.\n",
    "* More computationally efficient than post-pruning since it avoids building an unnecessarily deep tree.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è Comparison Table\n",
    "\n",
    "| Aspect                 | **Pre-Pruning**                        | **Post-Pruning**                      |\n",
    "| :--------------------- | :------------------------------------- | :------------------------------------ |\n",
    "| **When Applied**       | During tree construction               | After full tree construction          |\n",
    "| **Computation Time**   | Less                                   | More                                  |\n",
    "| **Used For**           | Large datasets                         | Small datasets                        |\n",
    "| **Approach**           | Restrict growth using hyperparameters  | Grow fully, then prune                |\n",
    "| **Risk**               | May underfit if stopped too early      | May overfit before pruning            |\n",
    "| **Example Parameters** | `max_depth`, `min_samples_split`, etc. | Cost-complexity pruning (`ccp_alpha`) |\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Summary\n",
    "\n",
    "| Concept          | Description                                              |\n",
    "| :--------------- | :------------------------------------------------------- |\n",
    "| **Overfitting**  | Model fits training data too well but fails on test data |\n",
    "| **Pruning**      | Reducing tree complexity to improve generalization       |\n",
    "| **Pre-Pruning**  | Stops tree growth early using hyperparameters            |\n",
    "| **Post-Pruning** | Builds full tree first, then removes weak branches       |\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Reference (scikit-learn)\n",
    "\n",
    "[DecisionTreeClassifier ‚Äî scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **In summary:**\n",
    "\n",
    "* **Pre-pruning** prevents the tree from growing too complex.\n",
    "* **Post-pruning** simplifies a fully grown tree.\n",
    "  Both help achieve better **generalization** and **prevent overfitting**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39291c7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
