{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c775a5a3",
   "metadata": {},
   "source": [
    "# ðŸŒ³ Decision Tree Regressor\n",
    "\n",
    "In this section, weâ€™ll discuss the **Decision Tree Regressor**, which is the regression counterpart of the Decision Tree Classifier.\n",
    "Previously, we explored how Decision Trees solve **classification problems** using **entropy**, **Gini impurity**, and **information gain**.\n",
    "Now, weâ€™ll see how Decision Trees can also handle **regression problems**, where the **target variable is continuous**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ˆ What is a Regression Problem?\n",
    "\n",
    "In a regression problem, the **output feature** is **continuous** â€” for example, predicting **salary**, **house price**, or **temperature**.\n",
    "\n",
    "So, whenever our target (output) feature is a continuous value, we use a **Decision Tree Regressor** instead of a classifier.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Example Dataset\n",
    "\n",
    "Letâ€™s consider a simple dataset with two features:\n",
    "\n",
    "| Experience | Career Gap | Salary (Target) |\n",
    "| ---------- | ---------- | --------------- |\n",
    "| 1          | 0          | 40K             |\n",
    "| 2          | 0          | 42K             |\n",
    "| 3          | 1          | 52K             |\n",
    "| 4          | 1          | 60K             |\n",
    "| 5          | 1          | 56K             |\n",
    "\n",
    "We want our Decision Tree Regressor to **predict the salary** based on **experience** and **career gap**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ¿ How the Splitting Works\n",
    "\n",
    "In Decision Tree Classifier, we used measures such as:\n",
    "\n",
    "* **Entropy**\n",
    "* **Gini Impurity**\n",
    "* **Information Gain**\n",
    "\n",
    "However, in Decision Tree Regressor, our output is **continuous**, so these measures are **not applicable**.\n",
    "\n",
    "Instead, we use **Variance Reduction** (or equivalently, **Mean Squared Error Reduction**) to determine the **best split**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® Step 1: Variance Formula\n",
    "\n",
    "The **variance** for a node is given by:\n",
    "\n",
    "$$\n",
    "Var = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* ( y_i ) = individual sample output\n",
    "* ( \\bar{y} ) = mean (average) of all outputs in that node\n",
    "* ( n ) = number of samples in the node\n",
    "\n",
    "This is equivalent to the **Mean Squared Error (MSE)**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© Step 2: Compute Variance for the Root Node\n",
    "\n",
    "For our dataset:\n",
    "\n",
    "$$\n",
    "y = [40, 42, 52, 60, 56]\n",
    "$$\n",
    "\n",
    "Mean of the root node:\n",
    "\n",
    "$$\n",
    "\\bar{y} = 50\n",
    "$$\n",
    "\n",
    "Now compute variance:\n",
    "\n",
    "$$\n",
    "Var_{root} = \\frac{1}{5}[(40-50)^2 + (42-50)^2 + (52-50)^2 + (60-50)^2 + (56-50)^2]\n",
    "$$\n",
    "\n",
    "$$\n",
    "Var_{root} = \\frac{1}{5}(100 + 64 + 4 + 100 + 36) = 60.8\n",
    "$$\n",
    "\n",
    "So,\n",
    "**Variance of the root node = 60.8**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ± Step 3: Make the First Split\n",
    "\n",
    "Letâ€™s consider two possible thresholds for the feature â€œExperienceâ€:\n",
    "\n",
    "1. Split at **â‰¤ 2**\n",
    "2. Split at **â‰¤ 2.5**\n",
    "\n",
    "Weâ€™ll calculate variance reduction for both and compare.\n",
    "\n",
    "---\n",
    "\n",
    "### **Split 1: Experience â‰¤ 2**\n",
    "\n",
    "* **Left Node (â‰¤ 2)** â†’ [40, 42]\n",
    "* **Right Node (> 2)** â†’ [52, 60, 56]\n",
    "\n",
    "#### Variance (Left Node)\n",
    "\n",
    "Mean = 41\n",
    "$$\n",
    "Var_L = \\frac{1}{2}[(40-41)^2 + (42-41)^2] = \\frac{1}{2}(1 + 1) = 1\n",
    "$$\n",
    "\n",
    "#### Variance (Right Node)\n",
    "\n",
    "Mean = 56\n",
    "$$\n",
    "Var_R = \\frac{1}{3}[(52-56)^2 + (60-56)^2 + (56-56)^2] = \\frac{1}{3}(16 + 16 + 0) = 10.67\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Split 2: Experience â‰¤ 2.5**\n",
    "\n",
    "* **Left Node (â‰¤ 2.5)** â†’ [40, 42]\n",
    "* **Right Node (> 2.5)** â†’ [52, 60, 56]\n",
    "\n",
    "(Same as Split 1 numerically â€” but for understanding, letâ€™s proceed with the formal variance reduction formula.)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® Step 4: Variance Reduction Formula\n",
    "\n",
    "The **Variance Reduction (VR)** is computed as:\n",
    "\n",
    "$$\n",
    "VR = Var_{root} - \\sum_i w_i Var_i\n",
    "$$\n",
    "\n",
    "Where ( w_i ) is the **proportion of samples** in each child node.\n",
    "\n",
    "---\n",
    "\n",
    "### For Split at â‰¤ 2.5\n",
    "\n",
    "Weights:\n",
    "\n",
    "* Left node: ( w_L = \\frac{2}{5} )\n",
    "* Right node: ( w_R = \\frac{3}{5} )\n",
    "\n",
    "So,\n",
    "\n",
    "$$\n",
    "VR = 60.8 - \\left[\\frac{2}{5} \\times 1 + \\frac{3}{5} \\times 10.67 \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "VR = 60.8 - (0.4 + 6.4) = 60.8 - 6.8 = 54.0\n",
    "$$\n",
    "\n",
    "Hence, **Variance Reduction = 54.0**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© Step 5: Choosing the Best Split\n",
    "\n",
    "We compare variance reductions from different splits â€”\n",
    "the **split with the highest Variance Reduction** is selected.\n",
    "\n",
    "Since the split at **Experience â‰¤ 2.5** gives the highest reduction, it becomes our **best split**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Step 6: Prediction in Decision Tree Regressor\n",
    "\n",
    "Once the Decision Tree is trained, for a **new data point**, we traverse the tree and reach a **leaf node**.\n",
    "\n",
    "At that leaf node:\n",
    "\n",
    "* The predicted value is the **average of all target values** in that node.\n",
    "\n",
    "Example:\n",
    "\n",
    "* If a test record falls into the left node with salaries [40, 42],\n",
    "  the predicted output = ( (40 + 42) / 2 = 41K ).\n",
    "\n",
    "* If it falls into the right node with [52, 60, 56],\n",
    "  the predicted output = ( (52 + 60 + 56) / 3 = 56K ).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¾ Summary\n",
    "\n",
    "| Concept            | Decision Tree Classifier   | Decision Tree Regressor  |\n",
    "| ------------------ | -------------------------- | ------------------------ |\n",
    "| Output Type        | Categorical                | Continuous               |\n",
    "| Splitting Criteria | Entropy / Gini / Info Gain | Variance Reduction / MSE |\n",
    "| Impurity Measure   | Gini Impurity              | Mean Squared Error       |\n",
    "| Prediction         | Class label                | Mean of values           |\n",
    "| Goal               | Maximize Information Gain  | Minimize Variance        |\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **Key Takeaway:**\n",
    "\n",
    "* Decision Tree Regressor replaces **Entropy** or **Gini** with **Variance Reduction**.\n",
    "* Each split minimizes the **MSE (variance)** within child nodes.\n",
    "* The prediction at a leaf node is the **average value** of the target feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e138c0e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
