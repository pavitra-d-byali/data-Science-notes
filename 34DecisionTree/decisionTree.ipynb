{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c77dd6",
   "metadata": {},
   "source": [
    "#  Decision Tree Classifier ‚Äî Mathematical Intuition and Working\n",
    "\n",
    "##  Introduction\n",
    "\n",
    "A **Decision Tree** is a supervised machine learning algorithm that can be used for both **classification** and **regression** problems.  \n",
    "In this document, we'll focus on **classification**.\n",
    "\n",
    "Decision Trees follow a structure similar to nested `if‚Äìelif‚Äìelse` conditions in programming.  \n",
    "They split data based on certain conditions until the output becomes **pure** (contains only one class).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Example: Simple Logical Structure\n",
    "\n",
    "```python\n",
    "age = 14\n",
    "\n",
    "if age <= 14:\n",
    "    print(\"The person is in school\")\n",
    "elif 15 <= age <= 21:\n",
    "    print(\"The person is in college\")\n",
    "else:\n",
    "    print(\"The person has completed college\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a162b",
   "metadata": {},
   "source": [
    "# üåø Decision Tree Structure\n",
    "\n",
    "The **root node** represents the first feature used for splitting (e.g., `age`).  \n",
    "Each **internal node** represents a condition (e.g., `age <= 15`).  \n",
    "Each **leaf node** represents a final decision/output (e.g., `\"school\"`).\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df597fc6",
   "metadata": {},
   "source": [
    "         Age <= 15\n",
    "          /      \\\n",
    "      Yes          No\n",
    "   \"School\"    Age <= 21\n",
    "                 /     \\\n",
    "             Yes         No\n",
    "          \"College\"   \"Graduated\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e216939f",
   "metadata": {},
   "source": [
    "\n",
    "A Decision Tree continues splitting until it reaches **pure leaf nodes** (where all samples belong to the same class).\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Types of Decision Tree Algorithms\n",
    "\n",
    "| Type | Full Form | Splitting Type | Library |\n",
    "|------|------------|----------------|----------|\n",
    "| **ID3** | Iterative Dichotomiser 3 | Multi-split | Historical (used in older versions) |\n",
    "| **CART** | Classification and Regression Trees | **Binary splits only** | ‚úÖ Used in `sklearn` |\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Example Dataset ‚Äî Play Tennis\n",
    "\n",
    "| Outlook  | Temperature | Humidity | Wind  | PlayTennis |\n",
    "|-----------|-------------|-----------|--------|-------------|\n",
    "| Sunny     | Hot         | High      | Weak   | No          |\n",
    "| Sunny     | Hot         | High      | Strong | No          |\n",
    "| Overcast  | Hot         | High      | Weak   | Yes         |\n",
    "| Rain      | Mild        | High      | Weak   | Yes         |\n",
    "| Rain      | Cool        | Normal    | Weak   | Yes         |\n",
    "| Rain      | Cool        | Normal    | Strong | No          |\n",
    "| Overcast  | Cool        | Normal    | Strong | Yes         |\n",
    "| Sunny     | Mild        | High      | Weak   | No          |\n",
    "| Sunny     | Cool        | Normal    | Weak   | Yes         |\n",
    "| Rain      | Mild        | Normal    | Weak   | Yes         |\n",
    "| Sunny     | Mild        | Normal    | Strong | Yes         |\n",
    "| Overcast  | Mild        | High      | Strong | Yes         |\n",
    "| Overcast  | Hot         | Normal    | Weak   | Yes         |\n",
    "| Rain      | Mild        | High      | Strong | No          |\n",
    "\n",
    "Here, the **target variable** is `PlayTennis (Yes/No)` and the independent features are **Outlook**, **Temperature**, **Humidity**, and **Wind**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Step 1: Initial Split Example\n",
    "\n",
    "Let's assume we choose **Outlook** as the root node.\n",
    "\n",
    "Unique values in `Outlook`:\n",
    "- Sunny  \n",
    "- Overcast  \n",
    "- Rain\n",
    "\n",
    "So, we split the dataset into **3 branches**.\n",
    "\n",
    "| Outlook | Yes | No |\n",
    "|----------|-----|----|\n",
    "| Sunny    | 2   | 3  |\n",
    "| Overcast | 4   | 0  |\n",
    "| Rain     | 3   | 2  |\n",
    "\n",
    "---\n",
    "\n",
    "## üåó Pure vs Impure Splits\n",
    "\n",
    "- **Pure Split:** Only one class (e.g., all ‚ÄúYes‚Äù or all ‚ÄúNo‚Äù)  \n",
    "- **Impure Split:** Contains a mix of classes (e.g., 2 ‚ÄúYes‚Äù and 3 ‚ÄúNo‚Äù)\n",
    "\n",
    "Example:\n",
    "- `Overcast ‚Üí Pure (All Yes)`\n",
    "- `Sunny ‚Üí Impure`\n",
    "- `Rain ‚Üí Impure`\n",
    "\n",
    "The **Decision Tree** keeps splitting impure nodes until all become pure (leaf nodes).\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Step 2: Measuring Purity ‚Äî Entropy & Gini Impurity\n",
    "\n",
    "To mathematically quantify **purity**, we use two metrics:\n",
    "\n",
    "1. **Entropy**\n",
    "2. **Gini Impurity**\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Entropy\n",
    "\n",
    "Entropy measures the **impurity or randomness** in a dataset.\n",
    "\n",
    "$$\n",
    "Entropy(S) = -p_{yes} \\log_2(p_{yes}) - p_{no} \\log_2(p_{no})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( p_{yes} \\): proportion of ‚ÄúYes‚Äù outcomes  \n",
    "- \\( p_{no} \\): proportion of ‚ÄúNo‚Äù outcomes\n",
    "\n",
    "**Entropy ranges from 0 to 1:**\n",
    "- 0 ‚Üí Pure (completely homogeneous)\n",
    "- 1 ‚Üí Impure (completely random)\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Gini Impurity\n",
    "\n",
    "Gini measures the **probability of incorrectly classifying** a randomly chosen element.\n",
    "\n",
    "$$\n",
    "Gini(S) = 1 - (p_{yes}^2 + p_{no}^2)\n",
    "$$\n",
    "\n",
    "Like Entropy:\n",
    "- 0 ‚Üí Pure\n",
    "- Closer to 0 ‚Üí More pure\n",
    "\n",
    "---\n",
    "\n",
    "## üí° Step 3: Selecting the Best Feature ‚Äî Information Gain\n",
    "\n",
    "To decide **which feature to split on**, we use **Information Gain (IG)**.\n",
    "\n",
    "$$\n",
    "IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\times Entropy(S_v)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( S \\): entire dataset  \n",
    "- \\( A \\): attribute being split  \n",
    "- \\( S_v \\): subset of \\( S \\) where attribute \\( A \\) takes the value \\( v \\)  \n",
    "- \\( \\frac{|S_v|}{|S|} \\): proportion (weight) of samples\n",
    "\n",
    "‚û°Ô∏è A **higher Information Gain** means the feature provides a **better split** (more pure subsets).\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Summary\n",
    "\n",
    "| Concept | Purpose | Formula |\n",
    "|----------|----------|----------|\n",
    "| **Entropy** | Measure impurity | \\( -p_1 \\log_2(p_1) - p_2 \\log_2(p_2) \\) |\n",
    "| **Gini Impurity** | Alternate purity metric | \\( 1 - (p_1^2 + p_2^2) \\) |\n",
    "| **Information Gain** | Choose best feature | \\( Entropy(S) - \\sum \\frac{|S_v|}{|S|} Entropy(S_v) \\) |\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ In the **next section**, we‚Äôll calculate these values step-by-step using the ‚ÄúPlay Tennis‚Äù dataset and Python code!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856365dd",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier ‚Äî Entropy and Gini Impurity\n",
    "\n",
    "In this session, we‚Äôll discuss two important concepts used in Decision Trees:\n",
    "\n",
    "* **Purity** ‚Äî How to check whether a split is pure or not\n",
    "* **Measures of impurity:** *Entropy* and *Gini Impurity*\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Entropy\n",
    "\n",
    "Entropy is a measure of impurity or randomness in a dataset.\n",
    "\n",
    "For a binary classification problem, the formula for entropy is:\n",
    "\n",
    "$$\n",
    "H(S) = -p_{+} \\log_2(p_{+}) - p_{-} \\log_2(p_{-})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* ( p_{+} ): Probability of being in the **positive** class (e.g., class = 1)\n",
    "* ( p_{-} ): Probability of being in the **negative** class (e.g., class = 0)\n",
    "\n",
    "For a **multi-class** classification problem, the formula generalizes to:\n",
    "\n",
    "$$\n",
    "H(S) = - \\sum_{i=1}^{n} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "where ( n ) is the number of classes.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Binary Split\n",
    "\n",
    "Suppose we have a dataset that splits as follows:\n",
    "\n",
    "| Node | Yes | No | Total |\n",
    "| ---- | --- | -- | ----- |\n",
    "| C1   | 3   | 3  | 6     |\n",
    "| C2   | 3   | 0  | 3     |\n",
    "\n",
    "#### Entropy for Node C1\n",
    "\n",
    "$$\n",
    "H(C1) = -p_{+} \\log_2(p_{+}) - p_{-} \\log_2(p_{-})\n",
    "$$\n",
    "\n",
    "Substitute values:\n",
    "\n",
    "$$\n",
    "p_{+} = \\frac{3}{6} = 0.5, \\quad p_{-} = \\frac{3}{6} = 0.5\n",
    "$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\n",
    "H(C1) = -0.5 \\log_2(0.5) - 0.5 \\log_2(0.5) = 1\n",
    "$$\n",
    "\n",
    "So this is a **completely impure split** (maximum entropy).\n",
    "\n",
    "---\n",
    "\n",
    "#### Entropy for Node C2\n",
    "\n",
    "$$\n",
    "p_{+} = \\frac{3}{3} = 1, \\quad p_{-} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "H(C2) = -1 \\log_2(1) - 0 \\log_2(0)\n",
    "$$\n",
    "\n",
    "Since ( \\log_2(1) = 0 ) and ( 0 \\times \\log_2(0) = 0 ):\n",
    "\n",
    "$$\n",
    "H(C2) = 0\n",
    "$$\n",
    "\n",
    "This is a **pure split**.\n",
    "\n",
    "---\n",
    "\n",
    "### Entropy Curve\n",
    "\n",
    "When we plot entropy vs probability of the positive class ( p_{+} ):\n",
    "\n",
    "* X-axis: ( p_{+} ) (0 to 1)\n",
    "* Y-axis: ( H(S) ) (0 to 1)\n",
    "\n",
    "The graph looks like this:\n",
    "\n",
    "```\n",
    "H(S)\n",
    " ‚Üë\n",
    " |        *\n",
    " |      *   *\n",
    " |    *       *\n",
    " |  *           *\n",
    " |*               *\n",
    " ------------------------‚Üí p(+)\n",
    " 0   0.5           1\n",
    "```\n",
    "\n",
    "* Entropy is **maximum (1)** when ( p_{+} = 0.5 )\n",
    "* Entropy is **minimum (0)** when ( p_{+} = 0 ) or ( p_{+} = 1 )\n",
    "\n",
    "Hence, **entropy ranges from 0 to 1.**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Gini Impurity\n",
    "\n",
    "Gini impurity measures how often a randomly chosen element would be incorrectly labeled.\n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$\n",
    "G(S) = 1 - \\sum_{i=1}^{n} p_i^2\n",
    "$$\n",
    "\n",
    "For binary classification:\n",
    "\n",
    "$$\n",
    "G(S) = 1 - (p_{+}^2 + p_{-}^2)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "\n",
    "#### Case 1: Node C1 (3 Yes, 3 No)\n",
    "\n",
    "$$\n",
    "p_{+} = 0.5, \\quad p_{-} = 0.5\n",
    "$$\n",
    "\n",
    "$$\n",
    "G(C1) = 1 - (0.5^2 + 0.5^2) = 1 - 0.5 = 0.5\n",
    "$$\n",
    "\n",
    "So the **maximum Gini impurity** is **0.5** for a completely impure split.\n",
    "\n",
    "---\n",
    "\n",
    "#### Case 2: Node C2 (3 Yes, 0 No)\n",
    "\n",
    "$$\n",
    "p_{+} = 1, \\quad p_{-} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "G(C2) = 1 - (1^2 + 0^2) = 0\n",
    "$$\n",
    "\n",
    "Hence, this is a **pure split**.\n",
    "\n",
    "---\n",
    "\n",
    "### Gini Impurity Curve\n",
    "\n",
    "When plotted:\n",
    "\n",
    "```\n",
    "G(S)\n",
    " ‚Üë\n",
    " |       *\n",
    " |     *   *\n",
    " |   *       *\n",
    " | *           *\n",
    " ------------------------‚Üí p(+)\n",
    " 0   0.5           1\n",
    "```\n",
    "\n",
    "* Gini impurity ranges from **0 to 0.5**\n",
    "* Maximum impurity (0.5) occurs when both classes are equally mixed\n",
    "* Minimum impurity (0) occurs when the node is completely pure\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Entropy vs Gini Impurity ‚Äî Comparison\n",
    "\n",
    "| Property       | Entropy                                          | Gini Impurity                          |\n",
    "| -------------- | ------------------------------------------------ | -------------------------------------- |\n",
    "| Formula        | ( -\\sum p_i \\log_2(p_i) )                        | ( 1 - \\sum p_i^2 )                     |\n",
    "| Range          | 0 to 1                                           | 0 to 0.5                               |\n",
    "| Interpretation | Measures information (bits) required to classify | Measures misclassification probability |\n",
    "| Smoothness     | Slightly more computationally expensive          | Faster to compute                      |\n",
    "| Used In        | ID3, C4.5, C5.0                                  | CART                                   |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Information Gain (Preview)\n",
    "\n",
    "Once we compute entropy or Gini impurity for each split, the next step is to decide **which feature to split on**.\n",
    "\n",
    "This is done using **Information Gain (IG)**.\n",
    "\n",
    "Information Gain helps us determine which feature gives the **maximum reduction in impurity** after splitting.\n",
    "\n",
    "In the next section, we‚Äôll derive the **Information Gain** formula and see how it helps in feature selection during Decision Tree construction.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "* Entropy ‚Üí Range: 0 to 1\n",
    "* Gini Impurity ‚Üí Range: 0 to 0.5\n",
    "* Both measure impurity ‚Äî lower value means purer split.\n",
    "* Used to decide when to stop splitting and how to choose the best features.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdb4572",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier ‚Äî Information Gain\n",
    "\n",
    "In the previous discussion, we understood **Gini Index**, **Gini Impurity**, and **Entropy** ‚Äî all of which help us **measure the purity of a split** in a Decision Tree.\n",
    "\n",
    "Now, let‚Äôs understand **Information Gain**, which helps us **decide which feature to split on**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What Is Information Gain?\n",
    "\n",
    "When we have multiple features (say ( F_1, F_2, F_3 )), we need to decide **which feature to use first** for splitting the tree.\n",
    "\n",
    "The **Information Gain (IG)** helps us determine this by measuring **the reduction in entropy** after the dataset is split on a feature.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula for Information Gain\n",
    "\n",
    "$$\n",
    "Gain(S, \\text{feature}) = H(S) - \\sum_{v \\in \\text{Values(feature)}} \\frac{|S_v|}{|S|} , H(S_v)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* ( H(S) ): Entropy of the **root node** (before the split)\n",
    "* ( S_v ): Subset of data for which feature = ( v )\n",
    "* ( H(S_v) ): Entropy of subset ( S_v )\n",
    "* ( \\frac{|S_v|}{|S|} ): Weighted proportion of samples in subset ( S_v )\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Example ‚Äî Calculating Information Gain\n",
    "\n",
    "Suppose we have a dataset with **feature F1** and the following class distribution:\n",
    "\n",
    "| Node                | Yes | No | Total |\n",
    "| ------------------- | --- | -- | ----- |\n",
    "| Root                | 9   | 5  | 14    |\n",
    "| C1 (after F1 split) | 6   | 2  | 8     |\n",
    "| C2 (after F1 split) | 3   | 3  | 6     |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Calculate Root Entropy\n",
    "\n",
    "$$\n",
    "H(S) = -p_{+} \\log_2(p_{+}) - p_{-} \\log_2(p_{-})\n",
    "$$\n",
    "\n",
    "Substitute:\n",
    "\n",
    "$$\n",
    "p_{+} = \\frac{9}{14}, \\quad p_{-} = \\frac{5}{14}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "H(S) = -\\frac{9}{14}\\log_2\\left(\\frac{9}{14}\\right) - \\frac{5}{14}\\log_2\\left(\\frac{5}{14}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "H(S) \\approx 0.94\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Calculate Entropy for Each Child Node\n",
    "\n",
    "#### For Category C1 (6 Yes, 2 No)\n",
    "\n",
    "$$\n",
    "H(C1) = -\\frac{6}{8}\\log_2\\left(\\frac{6}{8}\\right) - \\frac{2}{8}\\log_2\\left(\\frac{2}{8}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "H(C1) \\approx 0.81\n",
    "$$\n",
    "\n",
    "#### For Category C2 (3 Yes, 3 No)\n",
    "\n",
    "Since this is a perfectly impure split (( p_{+} = p_{-} = 0.5 )):\n",
    "\n",
    "$$\n",
    "H(C2) = 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Calculate Weighted Average Entropy\n",
    "\n",
    "$$\n",
    "\\sum_{v \\in \\text{Values(F1)}} \\frac{|S_v|}{|S|} , H(S_v)\n",
    "$$\n",
    "\n",
    "Substitute values:\n",
    "\n",
    "$$\n",
    "= \\frac{8}{14} \\times 0.81 + \\frac{6}{14} \\times 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.463 + 0.429 = 0.892\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Compute Information Gain\n",
    "\n",
    "$$\n",
    "Gain(S, F1) = H(S) - \\sum_{v} \\frac{|S_v|}{|S|}H(S_v)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Gain(S, F1) = 0.94 - 0.892 = 0.049\n",
    "$$\n",
    "\n",
    "So, the **Information Gain for Feature F1 = 0.049**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Comparing Features\n",
    "\n",
    "Let‚Äôs assume another feature ( F2 ) gives the following result after splitting:\n",
    "\n",
    "$$\n",
    "Gain(S, F2) > Gain(S, F1)\n",
    "$$\n",
    "\n",
    "Then we **choose ( F2 )** as the **root split feature** for our Decision Tree, because it **reduces entropy the most**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Conceptual Summary\n",
    "\n",
    "| Concept          | Purpose                                              | Formula                  | Range |    |   |            |     |\n",
    "| ---------------- | ---------------------------------------------------- | ------------------------ | ----- | -- | - | ---------- | --- |\n",
    "| Entropy          | Measures impurity (information required to classify) | ( -\\sum p_i \\log_2 p_i ) | 0‚Äì1   |    |   |            |     |\n",
    "| Gini Impurity    | Measures misclassification probability               | ( 1 - \\sum p_i^2 )       | 0‚Äì0.5 |    |   |            |     |\n",
    "| Information Gain | Measures reduction in impurity after split           | ( H(S) - \\sum \\frac{     | S_v   | }{ | S | } H(S_v) ) | ‚â• 0 |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Key Intuition\n",
    "\n",
    "* Higher **Information Gain** ‚Üí better split ‚Üí higher purity in child nodes\n",
    "* Decision Trees use Information Gain (or Gini Index) to **select features and build hierarchy**\n",
    "* Entropy focuses on **information content**, Gini focuses on **misclassification**\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Next Topic\n",
    "\n",
    "In the next discussion, we‚Äôll answer a common interview question:\n",
    "\n",
    "> **When should we use Entropy, and when should we use Gini Impurity?**\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "* Information Gain tells **which feature to split first**.\n",
    "* It uses **Entropy** (or Gini Impurity) as a base.\n",
    "* Decision Tree internally calculates IG for all features and **chooses the one with maximum gain**.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Video:** *Entropy vs Gini ‚Äî When to Use Which*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01802e0",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier ‚Äî Information Gain\n",
    "\n",
    "In the previous discussion, we understood **Gini Index**, **Gini Impurity**, and **Entropy** ‚Äî all of which help us **measure the purity of a split** in a Decision Tree.\n",
    "\n",
    "Now, let‚Äôs understand **Information Gain**, which helps us **decide which feature to split on**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What Is Information Gain?\n",
    "\n",
    "When we have multiple features (say ( F_1, F_2, F_3 )), we need to decide **which feature to use first** for splitting the tree.\n",
    "\n",
    "The **Information Gain (IG)** helps us determine this by measuring **the reduction in entropy** after the dataset is split on a feature.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula for Information Gain\n",
    "\n",
    "$$\n",
    "Gain(S, \\text{feature}) = H(S) - \\sum_{v \\in \\text{Values(feature)}} \\frac{|S_v|}{|S|} , H(S_v)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* ( H(S) ): Entropy of the **root node** (before the split)\n",
    "* ( S_v ): Subset of data for which feature = ( v )\n",
    "* ( H(S_v) ): Entropy of subset ( S_v )\n",
    "* ( \\frac{|S_v|}{|S|} ): Weighted proportion of samples in subset ( S_v )\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Example ‚Äî Calculating Information Gain\n",
    "\n",
    "Suppose we have a dataset with **feature F1** and the following class distribution:\n",
    "\n",
    "| Node                | Yes | No | Total |\n",
    "| ------------------- | --- | -- | ----- |\n",
    "| Root                | 9   | 5  | 14    |\n",
    "| C1 (after F1 split) | 6   | 2  | 8     |\n",
    "| C2 (after F1 split) | 3   | 3  | 6     |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Calculate Root Entropy\n",
    "\n",
    "$$\n",
    "H(S) = -p_{+} \\log_2(p_{+}) - p_{-} \\log_2(p_{-})\n",
    "$$\n",
    "\n",
    "Substitute:\n",
    "\n",
    "$$\n",
    "p_{+} = \\frac{9}{14}, \\quad p_{-} = \\frac{5}{14}\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "H(S) = -\\frac{9}{14}\\log_2\\left(\\frac{9}{14}\\right) - \\frac{5}{14}\\log_2\\left(\\frac{5}{14}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "H(S) \\approx 0.94\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Calculate Entropy for Each Child Node\n",
    "\n",
    "#### For Category C1 (6 Yes, 2 No)\n",
    "\n",
    "$$\n",
    "H(C1) = -\\frac{6}{8}\\log_2\\left(\\frac{6}{8}\\right) - \\frac{2}{8}\\log_2\\left(\\frac{2}{8}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "H(C1) \\approx 0.81\n",
    "$$\n",
    "\n",
    "#### For Category C2 (3 Yes, 3 No)\n",
    "\n",
    "Since this is a perfectly impure split (( p_{+} = p_{-} = 0.5 )):\n",
    "\n",
    "$$\n",
    "H(C2) = 1\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Calculate Weighted Average Entropy\n",
    "\n",
    "$$\n",
    "\\sum_{v \\in \\text{Values(F1)}} \\frac{|S_v|}{|S|} , H(S_v)\n",
    "$$\n",
    "\n",
    "Substitute values:\n",
    "\n",
    "$$\n",
    "= \\frac{8}{14} \\times 0.81 + \\frac{6}{14} \\times 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.463 + 0.429 = 0.892\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Compute Information Gain\n",
    "\n",
    "$$\n",
    "Gain(S, F1) = H(S) - \\sum_{v} \\frac{|S_v|}{|S|}H(S_v)\n",
    "$$\n",
    "\n",
    "$$\n",
    "Gain(S, F1) = 0.94 - 0.892 = 0.049\n",
    "$$\n",
    "\n",
    "So, the **Information Gain for Feature F1 = 0.049**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Comparing Features\n",
    "\n",
    "Let‚Äôs assume another feature ( F2 ) gives the following result after splitting:\n",
    "\n",
    "$$\n",
    "Gain(S, F2) > Gain(S, F1)\n",
    "$$\n",
    "\n",
    "Then we **choose ( F2 )** as the **root split feature** for our Decision Tree, because it **reduces entropy the most**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Conceptual Summary\n",
    "\n",
    "| Concept          | Purpose                                              | Formula                  | Range |    |   |            |     |\n",
    "| ---------------- | ---------------------------------------------------- | ------------------------ | ----- | -- | - | ---------- | --- |\n",
    "| Entropy          | Measures impurity (information required to classify) | ( -\\sum p_i \\log_2 p_i ) | 0‚Äì1   |    |   |            |     |\n",
    "| Gini Impurity    | Measures misclassification probability               | ( 1 - \\sum p_i^2 )       | 0‚Äì0.5 |    |   |            |     |\n",
    "| Information Gain | Measures reduction in impurity after split           | ( H(S) - \\sum \\frac{     | S_v   | }{ | S | } H(S_v) ) | ‚â• 0 |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Key Intuition\n",
    "\n",
    "* Higher **Information Gain** ‚Üí better split ‚Üí higher purity in child nodes\n",
    "* Decision Trees use Information Gain (or Gini Index) to **select features and build hierarchy**\n",
    "* Entropy focuses on **information content**, Gini focuses on **misclassification**\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Next Topic\n",
    "\n",
    "In the next discussion, we‚Äôll answer a common interview question:\n",
    "\n",
    "> **When should we use Entropy, and when should we use Gini Impurity?**\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "* Information Gain tells **which feature to split first**.\n",
    "* It uses **Entropy** (or Gini Impurity) as a base.\n",
    "* Decision Tree internally calculates IG for all features and **chooses the one with maximum gain**.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Video:** *Entropy vs Gini ‚Äî When to Use Which*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61006969",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
