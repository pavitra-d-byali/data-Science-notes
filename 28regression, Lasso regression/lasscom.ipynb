{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e75944f8",
   "metadata": {},
   "source": [
    "# Lasso Regression (L1 Regularization)\n",
    "\n",
    "Lasso regression is also called **L1 Regularization**.\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "* Lasso is primarily used for **feature selection**.\n",
    "* It can reduce coefficients of less important features **exactly to zero**, effectively removing them from the model.\n",
    "\n",
    "---\n",
    "\n",
    "## Lasso Regression Cost Function\n",
    "\n",
    "The cost function for Lasso Regression is:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\Big(h_\\theta(x^{(i)}) - y^{(i)} \\Big)^2 + \\lambda \\sum_{j=1}^{n} |\\theta_j|\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\lambda \\geq 0$ is the **regularization parameter** (hyperparameter).\n",
    "* $|\\theta_j|$ is the **L1 penalty term** (magnitude of coefficients).\n",
    "* $\\theta_0$ (intercept) is **not penalized**.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition Behind Lasso Regression\n",
    "\n",
    "* **Lambda ($\\lambda$) = 0:** Normal linear regression (no regularization).\n",
    "* **Increasing $\\lambda$:** Coefficients shrink.\n",
    "* **Some coefficients become exactly 0:**\n",
    "\n",
    "  * Features with little contribution to the target are **removed automatically**.\n",
    "  * Strongly correlated features remain with non-zero coefficients.\n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose a model with 4 features:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\theta_4 x_4\n",
    "$$\n",
    "\n",
    "Initial coefficients (before Lasso):\n",
    "\n",
    "$$\n",
    "\\theta_0 = 0.52, \\quad \\theta_1 = 0.65, \\quad \\theta_2 = 0.72, \\quad \\theta_3 = 0.34, \\quad \\theta_4 = 0.12\n",
    "$$\n",
    "\n",
    "After applying Lasso (with suitable $\\lambda$):\n",
    "\n",
    "$$\n",
    "\\theta_0 = 0.50, \\quad \\theta_1 = 0.55, \\quad \\theta_2 = 0.68, \\quad \\theta_3 = 0.14, \\quad \\theta_4 = 0\n",
    "$$\n",
    "\n",
    "* $\\theta_4 = 0$ â†’ $x_4$ is **removed**.\n",
    "* Stronger features ($x_1, x_2$) retain larger coefficients.\n",
    "* This is why Lasso is **useful for automatic feature selection**.\n",
    "\n",
    "---\n",
    "\n",
    "# Elastic Net Regression\n",
    "\n",
    "Elastic Net is a combination of **Ridge (L2)** and **Lasso (L1)**.\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "1. Reduce overfitting (Ridge effect)\n",
    "2. Perform feature selection (Lasso effect)\n",
    "\n",
    "---\n",
    "\n",
    "## Elastic Net Cost Function\n",
    "\n",
    "The cost function for Elastic Net is:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\Big(h_\\theta(x^{(i)}) - y^{(i)} \\Big)^2\n",
    "\n",
    "* \\lambda_1 \\sum_{j=1}^{n} \\theta_j^2 \\quad \\text{(Ridge term)}\n",
    "* \\lambda_2 \\sum_{j=1}^{n} |\\theta_j| \\quad \\text{(Lasso term)}\n",
    "  $$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\lambda_1$ controls **overfitting reduction** (Ridge effect).\n",
    "* $\\lambda_2$ controls **feature selection** (Lasso effect).\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Regression Series\n",
    "\n",
    "| Regression  | Regularization Type | Main Purpose                           | Effect on Coefficients                        |\n",
    "| ----------- | ------------------- | -------------------------------------- | --------------------------------------------- |\n",
    "| Ridge       | L2                  | Reduce overfitting                     | Shrinks coefficients but **never zero**       |\n",
    "| Lasso       | L1                  | Feature selection                      | Shrinks some coefficients **exactly to zero** |\n",
    "| Elastic Net | L1 + L2             | Reduce overfitting + feature selection | Combination of both effects                   |\n",
    "\n",
    "**Key Insight:**\n",
    "\n",
    "* These methods allow **hyperparameter tuning** of linear regression to improve **generalization** and **interpretability**.\n",
    "* Interview questions often focus on:\n",
    "\n",
    "  * Why we use Ridge/Lasso/Elastic Net\n",
    "  * Relationship between **lambda ($\\lambda$)** and **coefficients ($\\theta$)**\n",
    "  * Feature selection capabilities\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410d8fac",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
