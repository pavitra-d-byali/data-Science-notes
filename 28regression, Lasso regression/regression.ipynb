{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdae0d59",
   "metadata": {},
   "source": [
    "# Ridge Regression (L2 Regularization)\n",
    "\n",
    "\n",
    "\n",
    "* We derived the **equation of the best fit line**:\n",
    "  $$\n",
    "  h_\\theta(x) = \\theta_0 + \\theta_1 x\n",
    "  $$\n",
    "  where $x$ is a single independent feature.\n",
    "\n",
    "* For multiple features, the equation becomes:\n",
    "  $$\n",
    "  h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
    "  $$\n",
    "\n",
    "* We also saw the **cost function** for linear regression, which is the **Mean Squared Error (MSE):**\n",
    "  $$\n",
    "  J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\Big(h_\\theta(x^{(i)}) - y^{(i)} \\Big)^2\n",
    "  $$\n",
    "\n",
    "Our objective is to minimize this cost function using **Gradient Descent** to reach the **global minimum**.\n",
    "\n",
    "---\n",
    "\n",
    "## Problem of Overfitting in Linear Regression\n",
    "\n",
    "Consider a dataset with very few points. If we apply linear regression, the model can perfectly fit those points, making the error **zero** on the training data.\n",
    "\n",
    "* **Training Accuracy:** Very high (close to $100%$)\n",
    "* **Training Error:** Very low (close to $0$)\n",
    "* **Test Error:** High (poor performance on unseen data)\n",
    "\n",
    "This is the classic case of **overfitting**.\n",
    "\n",
    "* **Bias (on training data):** Low\n",
    "* **Variance (on test data):** High\n",
    "\n",
    "---\n",
    "\n",
    "## Ridge Regression (L2 Regularization)\n",
    "\n",
    "To address **overfitting**, we introduce **Ridge Regression**.\n",
    "\n",
    "* Ridge Regression is also called **L2 Regularization**.\n",
    "* It modifies the cost function by adding a **penalty term** proportional to the square of the coefficients.\n",
    "\n",
    "### Ridge Regression Cost Function\n",
    "\n",
    "The new cost function is:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\Big(h_\\theta(x^{(i)}) - y^{(i)} \\Big)^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\lambda \\geq 0$ is the **regularization parameter (hyperparameter)**.\n",
    "* $\\sum_{j=1}^{n} \\theta_j^2$ is the penalty term (squared coefficients).\n",
    "* Note: $\\theta_0$ (bias term) is **not penalized**.\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition Behind Ridge Regression\n",
    "\n",
    "1. **Without Regularization ($\\lambda = 0$):**\n",
    "\n",
    "   * Cost function reduces to standard linear regression.\n",
    "   * High risk of overfitting.\n",
    "\n",
    "2. **With Regularization ($\\lambda > 0$):**\n",
    "\n",
    "   * Large coefficients ($\\theta_j$) are penalized.\n",
    "   * Model is forced to keep coefficients small.\n",
    "   * Prevents the model from fitting noise in the data.\n",
    "   * Reduces **variance** (better generalization).\n",
    "\n",
    "---\n",
    "\n",
    "## Relationship Between $\\lambda$ and $\\theta$ (Slope)\n",
    "\n",
    "* As $\\lambda$ increases:\n",
    "\n",
    "  * Coefficients ($\\theta_j$) decrease.\n",
    "  * Model becomes simpler.\n",
    "  * Overfitting is reduced.\n",
    "\n",
    "* But if $\\lambda$ is too large:\n",
    "\n",
    "  * Coefficients shrink too much.\n",
    "  * Model underfits the data.\n",
    "\n",
    "**Key Relationship:**\n",
    "$$\n",
    "\\lambda \\uparrow \\quad \\Rightarrow \\quad |\\theta_j| \\downarrow\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Example with Multiple Features\n",
    "\n",
    "Suppose we have a multiple linear regression model:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3\n",
    "$$\n",
    "\n",
    "Assume initial coefficients:\n",
    "\n",
    "$$\n",
    "\\theta_0 = 0.3, \\quad \\theta_1 = 0.52, \\quad \\theta_2 = 0.48, \\quad \\theta_3 = 0.24\n",
    "$$\n",
    "\n",
    "After applying Ridge Regression with some $\\lambda$:\n",
    "\n",
    "$$\n",
    "\\theta_0 = 0.3, \\quad \\theta_1 = 0.40, \\quad \\theta_2 = 0.38, \\quad \\theta_3 = 0.14\n",
    "$$\n",
    "\n",
    "Notice:\n",
    "\n",
    "* All coefficients shrink, but **none become zero**.\n",
    "* Features with **less impact** (e.g., $x_3$) get reduced further.\n",
    "* This prevents weakly correlated features from dominating the model.\n",
    "\n",
    "---\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "* Ridge Regression **never eliminates features** (coefficients don’t become exactly zero).\n",
    "* It only **shrinks them** to reduce their impact.\n",
    "* This makes Ridge Regression different from **Lasso Regression**, which can set coefficients exactly to zero.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "* **Linear Regression** may overfit, especially with high-dimensional data.\n",
    "* **Ridge Regression** introduces an $L2$ penalty to reduce overfitting.\n",
    "* Cost Function:\n",
    "  $$\n",
    "  J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} \\Big(h_\\theta(x^{(i)}) - y^{(i)} \\Big)^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2\n",
    "  $$\n",
    "* **Effect of $\\lambda$:**\n",
    "\n",
    "  * Small $\\lambda \\to$ similar to linear regression (risk of overfitting).\n",
    "  * Large $\\lambda \\to$ stronger penalty, smaller coefficients (risk of underfitting).\n",
    "\n",
    "Next, we’ll discuss **Lasso Regression (L1 Regularization)** and compare it with Ridge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31158f8",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf53857",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8e2af71",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
