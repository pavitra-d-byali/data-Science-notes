{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f16e530c",
   "metadata": {},
   "source": [
    "# AdaBoost Machine Learning Algorithm\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Recap: Bagging and Boosting\n",
    " we discussed **two ensemble techniques**:\n",
    "\n",
    "1. **Bagging**\n",
    "2. **Boosting**\n",
    "\n",
    "### Bagging\n",
    "- We covered the **Random Forest Classifier** and **Random Forest Regressor**.\n",
    "- Bagging uses **base learners** (like decision trees) and aims to reduce **variance** by combining multiple learners trained on random samples.\n",
    "\n",
    "### Boosting\n",
    "- Boosting uses **weak learners** sequentially to form a strong learner.\n",
    "- We will cover the following boosting algorithms:\n",
    "  1. **AdaBoost**\n",
    "  2. **Gradient Boosting**\n",
    "  3. **XGBoost**\n",
    "- Both bagging and boosting can solve **classification** and **regression** problems.\n",
    "- **Decision trees** are commonly used as the base learner.\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Trees and Overfitting\n",
    "\n",
    "- A **Decision Tree** grown to full depth can lead to **overfitting**:\n",
    "  - **High training accuracy** → Low bias\n",
    "  - **Low test accuracy** → High variance\n",
    "\n",
    "### Random Forest vs Single Decision Tree\n",
    "- Random Forest uses **multiple decision trees** (base learners).\n",
    "- Samples of the dataset are provided to each tree.\n",
    "- Random Forest reduces **variance**, achieving **low bias and low variance**.\n",
    "\n",
    "---\n",
    "\n",
    "## Boosting\n",
    "\n",
    "- Boosting is sequential:\n",
    "  1. Train a **weak learner** (e.g., shallow decision tree).\n",
    "  2. Focus on records wrongly predicted by the previous learner.\n",
    "  3. Pass misclassified records to the next learner.\n",
    "- The final model combines all weak learners into a **strong learner**.\n",
    "\n",
    "### Weak Learner\n",
    "- A weak learner is a model that **has not learned much** from the training data.\n",
    "- Commonly, **decision stumps** (trees of depth 1) are used as weak learners.\n",
    "\n",
    "### AdaBoost\n",
    "- Assigns **weights** to weak learners.\n",
    "- Weighted combination forms the final prediction.\n",
    "\n",
    "The AdaBoost function can be represented as:\n",
    "\n",
    "$$\n",
    "F(x) = \\alpha_1 \\cdot M_1(x) + \\alpha_2 \\cdot M_2(x) + \\dots + \\alpha_N \\cdot M_N(x)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\(M_1, M_2, \\dots, M_N\\) are **decision stumps**.\n",
    "- \\(\\alpha_1, \\alpha_2, \\dots, \\alpha_N\\) are **weights assigned** to each weak learner.\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Tree Stumps\n",
    "\n",
    "- A **decision tree stump** is a tree with **depth = 1**.\n",
    "- Individually, stumps are weak learners → **underfitting**:\n",
    "  - High bias\n",
    "  - Low variance\n",
    "\n",
    "- Combining multiple stumps sequentially with weights:\n",
    "  - Reduces **bias**\n",
    "  - Increases **variance**\n",
    "  - Results in a **strong learner**\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Bagging reduces **variance** by using multiple base learners.\n",
    "- Boosting reduces **bias** by using multiple weak learners sequentially.\n",
    "- AdaBoost:\n",
    "  - Uses **decision stumps** as weak learners.\n",
    "  - Assigns **weights** to learners based on performance.\n",
    "  - Sequentially focuses on **misclassified records**.\n",
    "  - Can solve both **classification** and **regression** problems.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce24cb9",
   "metadata": {},
   "source": [
    "# AdaBoost: Constructing Decision Tree Stumps\n",
    "\n",
    "In the previous video, we discussed the **main aim of AdaBoost**:\n",
    "\n",
    "- Combine multiple **weak learners** to create a **strong learner**.\n",
    "- In AdaBoost, a **weak learner** is typically a **decision tree stump** (depth = 1).\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Create Decision Tree Stumps\n",
    "\n",
    "Consider the dataset with features:\n",
    "\n",
    "| Salary (K) | Credit Score | Credit Approval |\n",
    "|------------|--------------|----------------|\n",
    "| ≤50        | Bad          | No             |\n",
    "| ≤50        | Good         | Yes            |\n",
    "| >50        | Good         | Yes            |\n",
    "| >50        | Bad          | No             |\n",
    "| ...        | ...          | ...            |\n",
    "\n",
    "### Decision Tree Stump 1: Feature `Salary`\n",
    "\n",
    "- Condition: `Salary ≤ 50K`\n",
    "- Two outcomes: **Yes** or **No**\n",
    "  \n",
    "| Salary ≤50K | Count Yes | Count No |\n",
    "|-------------|-----------|----------|\n",
    "| True        | 2         | 2        |\n",
    "| False       | 2         | 1        |\n",
    "\n",
    "- **Misclassifications** exist because the split is not perfect.\n",
    "\n",
    "### Decision Tree Stump 2: Feature `Credit Score`\n",
    "\n",
    "- Condition: `Credit = Good`\n",
    "- Outcomes: **Yes** or **No**\n",
    "\n",
    "| Credit = Good | Count Yes | Count No |\n",
    "|---------------|-----------|----------|\n",
    "| True          | 3         | 0        |\n",
    "| False         | 1         | 3        |\n",
    "\n",
    "- This stump has a **more informative split** than the first.\n",
    "\n",
    "---\n",
    "\n",
    "## Selecting the Best Decision Tree Stump\n",
    "\n",
    "- AdaBoost selects the **best stump** based on a measure of impurity:\n",
    "  - **Entropy**\n",
    "  - **Gini Index / Gini Impurity**\n",
    "\n",
    "### Entropy Calculation\n",
    "\n",
    "- For a node with 50% Yes and 50% No:\n",
    "\n",
    "$$\n",
    "Entropy = -\\sum_{i} p_i \\log_2(p_i) = 1\n",
    "$$\n",
    "\n",
    "- For a node with 2 Yes and 1 No:\n",
    "\n",
    "$$\n",
    "Entropy = -\\left(\\frac{2}{3}\\log_2\\frac{2}{3} + \\frac{1}{3}\\log_2\\frac{1}{3}\\right) \\approx 0.918\n",
    "$$\n",
    "\n",
    "### Gini Index Calculation\n",
    "\n",
    "- For the same 50% Yes / 50% No node:\n",
    "\n",
    "$$\n",
    "Gini = 1 - \\sum_i p_i^2 = 0.5\n",
    "$$\n",
    "\n",
    "- For 2 Yes / 1 No node:\n",
    "\n",
    "$$\n",
    "Gini = 1 - \\left(\\frac{2}{3}^2 + \\frac{1}{3}^2\\right) \\approx 0.444\n",
    "$$\n",
    "\n",
    "> **Observation:** The stump with the **lower entropy or Gini index** is selected as the **first weak learner**.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Step 1\n",
    "\n",
    "1. Generate **decision tree stumps** for each feature.\n",
    "2. Calculate **impurity** using **Entropy** or **Gini Index**.\n",
    "3. Select the **best stump** as the **first weak learner**.\n",
    "\n",
    "> Next step: We will see how AdaBoost updates weights and constructs the sequential model using weak learners.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664f85ff",
   "metadata": {},
   "source": [
    "# AdaBoost Step 2: Calculating Error and Performance of Stump\n",
    "\n",
    "In Step 2 of AdaBoost, we perform two key operations:\n",
    "\n",
    "1. **Calculate the total error of the selected stump**\n",
    "2. **Determine the performance of the stump**\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2.1: Assign Sample Weights\n",
    "\n",
    "- Total records in dataset: 7\n",
    "- Assign equal weights to all records:\n",
    "\n",
    "$$\n",
    "w_i = \\frac{1}{7}, \\quad i = 1, 2, ..., 7\n",
    "$$\n",
    "\n",
    "> These sample weights will help calculate the total error of the stump.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2.2: Identify Errors\n",
    "\n",
    "Consider the first selected decision tree stump based on **Credit Score = Good**:\n",
    "\n",
    "| Credit Score | Predicted | Actual | Error? |\n",
    "|--------------|-----------|--------|--------|\n",
    "| Good         | Yes       | Yes    | No     |\n",
    "| Not Good     | No        | Yes    | Yes    |\n",
    "\n",
    "- **Error** occurs when the stump misclassifies a record.\n",
    "- In this example, **1 record is misclassified**.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2.3: Calculate Total Error\n",
    "\n",
    "- Total error is the **sum of sample weights** for all misclassified records:\n",
    "\n",
    "$$\n",
    "\\text{Total Error} (\\varepsilon) = \\sum_{i \\in \\text{wrong}} w_i\n",
    "$$`\n",
    "\n",
    "- Here, only **1 record is wrong**:\n",
    "\n",
    "$$\n",
    "\\varepsilon = \\frac{1}{7} \\approx 0.1429\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2.4: Calculate Performance of Stump\n",
    "\n",
    "- The **performance (weight) of the stump** is calculated using:\n",
    "\n",
    "$$\n",
    "\\alpha = \\frac{1}{2} \\ln \\frac{1 - \\varepsilon}{\\varepsilon}\n",
    "$$\n",
    "\n",
    "- Substitute \\(\\varepsilon = \\frac{1}{7}\\):\n",
    "\n",
    "$$\n",
    "\\alpha = \\frac{1}{2} \\ln \\frac{1 - \\frac{1}{7}}{\\frac{1}{7}} \n",
    "= \\frac{1}{2} \\ln \\frac{6/7}{1/7} \n",
    "= \\frac{1}{2} \\ln 6 \n",
    "\\approx 0.896\n",
    "$$\n",
    "\n",
    "- This value of \\(\\alpha\\) is the **weight assigned** to the first weak learner in AdaBoost.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2.5: Update AdaBoost Model\n",
    "\n",
    "- The AdaBoost model function:\n",
    "\n",
    "$$\n",
    "F(x) = \\alpha_1 M_1(x) + \\alpha_2 M_2(x) + \\dots + \\alpha_N M_N(x)\n",
    "$$\n",
    "\n",
    "- For the first stump:\n",
    "\n",
    "  - \\(M_1(x)\\) = Decision tree stump 1\n",
    "  - \\(\\alpha_1 = 0.896\\)\n",
    "\n",
    "- Misclassified records from \\(M_1\\) will be **passed to the next stump**.\n",
    "- Each subsequent stump is trained on **updated sample weights** emphasizing previous errors.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Step 2\n",
    "\n",
    "1. Assign **equal weights** to all records initially.\n",
    "2. Identify **misclassified records**.\n",
    "3. Compute **total error** as the sum of sample weights of wrong records.\n",
    "4. Calculate **performance (α)** of the stump:\n",
    "\n",
    "$$\n",
    "\\alpha = \\frac{1}{2} \\ln \\frac{1 - \\varepsilon}{\\varepsilon}\n",
    "$$\n",
    "\n",
    "5. Pass misclassified records to the **next weak learner**.\n",
    "6. Repeat the process to build a **strong learner** sequentially.\n",
    "\n",
    "> Next step: Construct the second stump, update weights, and calculate \\(\\alpha_2\\).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e62ff3",
   "metadata": {},
   "source": [
    "# AdaBoost Step 3: Updating Weights for Correctly and Incorrectly Classified Points\n",
    "\n",
    "In Step 3, we update the **sample weights** based on the performance of the first decision tree stump.  \n",
    "\n",
    "- Previously, we computed:\n",
    "  1. **Total errors** of the first stump\n",
    "  2. **Performance (α₁)** of the first stump, which determines its weight in AdaBoost.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3.1: Update Weights\n",
    "\n",
    "### Key Idea\n",
    "\n",
    "- **Correctly classified points:** Decrease their weights  \n",
    "  → Reduce their probability of being selected in the next stump.  \n",
    "- **Incorrectly classified points:** Increase their weights  \n",
    "  → Increase their probability of being selected in the next stump.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3.2: Formulas for Weight Update\n",
    "\n",
    "Let:  \n",
    "- \\(w_i\\) = current weight of record \\(i\\)  \n",
    "- \\(\\alpha\\) = performance of the stump\n",
    "\n",
    "### Correctly Classified Points:\n",
    "\n",
    "$$\n",
    "w_i^{\\text{new}} = w_i \\cdot e^{-\\alpha}\n",
    "$$\n",
    "\n",
    "- Example: \\(w_i = \\frac{1}{7}\\), \\(\\alpha = 0.896\\)\n",
    "\n",
    "$$\n",
    "w_i^{\\text{new}} = \\frac{1}{7} \\cdot e^{-0.896} \\approx 0.058\n",
    "$$\n",
    "\n",
    "### Incorrectly Classified Points:\n",
    "\n",
    "$$\n",
    "w_i^{\\text{new}} = w_i \\cdot e^{\\alpha}\n",
    "$$\n",
    "\n",
    "- Example: \\(w_i = \\frac{1}{7}\\), \\(\\alpha = 0.896\\)\n",
    "\n",
    "$$\n",
    "w_i^{\\text{new}} = \\frac{1}{7} \\cdot e^{0.896} \\approx 0.349\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3.3: Updated Weights Table\n",
    "\n",
    "| Record | Classified Correctly? | Old Weight | Updated Weight |\n",
    "|--------|----------------------|------------|----------------|\n",
    "| 1      | Yes                  | 1/7        | 0.058          |\n",
    "| 2      | Yes                  | 1/7        | 0.058          |\n",
    "| 3      | Yes                  | 1/7        | 0.058          |\n",
    "| 4      | Yes                  | 1/7        | 0.058          |\n",
    "| 5      | Yes                  | 1/7        | 0.058          |\n",
    "| 6      | Yes                  | 1/7        | 0.058          |\n",
    "| 7      | No (misclassified)   | 1/7        | 0.349          |\n",
    "\n",
    "> ✅ Correctly classified weights decrease, incorrectly classified weights increase.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3.4: Summary\n",
    "\n",
    "1. Update weights **to emphasize misclassified records**.\n",
    "2. Formulas:\n",
    "   - Correctly classified:  \n",
    "     $$\n",
    "     w_i^{\\text{new}} = w_i \\cdot e^{-\\alpha}\n",
    "     $$\n",
    "   - Incorrectly classified:  \n",
    "     $$\n",
    "     w_i^{\\text{new}} = w_i \\cdot e^{\\alpha}\n",
    "     $$\n",
    "3. This ensures that the **next decision stump focuses more on previously misclassified points**.\n",
    "\n",
    "---\n",
    "\n",
    "> Next Step (Step 4): Normalize the updated weights so that they sum to 1 and can be used for training the next weak learner.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45b588b",
   "metadata": {},
   "source": [
    "# AdaBoost Step 4: Normalizing Weights and Assigning Bins\n",
    "\n",
    "In Step 4, we normalize the updated weights from Step 3 and create bins to select records for the next decision tree stump.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4.1: Normalize Weights\n",
    "\n",
    "- After updating weights, the sum of all weights may **not be 1**.\n",
    "- Example:  \n",
    "\n",
    "$$\n",
    "\\text{Sum of updated weights} = 0.697\n",
    "$$\n",
    "\n",
    "- To normalize each weight:\n",
    "\n",
    "$$\n",
    "w_i^{\\text{normalized}} = \\frac{w_i^{\\text{updated}}}{\\sum_j w_j^{\\text{updated}}}\n",
    "$$\n",
    "\n",
    "- Example calculations:\n",
    "  - For \\(w_i = 0.058\\):\n",
    "\n",
    "  $$\n",
    "  w_i^{\\text{normalized}} = \\frac{0.058}{0.697} \\approx 0.083\n",
    "  $$\n",
    "\n",
    "  - For \\(w_i = 0.349\\):\n",
    "\n",
    "  $$\n",
    "  w_i^{\\text{normalized}} = \\frac{0.349}{0.697} \\approx 0.50\n",
    "  $$\n",
    "\n",
    "- After normalization, **all weights sum to 1**.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4.2: Assign Bins\n",
    "\n",
    "- **Purpose:** Ensure misclassified records are selected more frequently for the next stump.\n",
    "- Steps:\n",
    "  1. Assign ranges (bins) proportional to normalized weights.\n",
    "  2. Example:\n",
    "\n",
    "| Record | Normalized Weight | Bin Range  |\n",
    "|--------|-----------------|------------|\n",
    "| 1      | 0.08            | 0.00 – 0.08 |\n",
    "| 2      | 0.08            | 0.08 – 0.16 |\n",
    "| 3      | 0.08            | 0.16 – 0.24 |\n",
    "| 4      | 0.08            | 0.24 – 0.32 |\n",
    "| 5      | 0.08            | 0.32 – 0.40 |\n",
    "| 6      | 0.08            | 0.40 – 0.48 |\n",
    "| 7      | 0.50            | 0.48 – 0.98 |\n",
    "\n",
    "- **Observation:** The misclassified record (weight = 0.50) has a **larger bin**, increasing the probability of selection.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4.3: Purpose of Bin Assignment\n",
    "\n",
    "1. Records with **higher normalized weights** (misclassified points) are more likely to be selected for the next stump.\n",
    "2. This ensures the **next weak learner focuses on previously misclassified points**.\n",
    "3. After creating bins, the **next decision tree stump** can be trained effectively on these weighted records.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4.4: Summary\n",
    "\n",
    "1. **Normalized Weights:** Divide each updated weight by the total sum of weights so that all weights sum to 1.\n",
    "2. **Bin Assignment:** Create ranges proportional to normalized weights.\n",
    "3. **Selection Mechanism:** Misclassified records are selected more frequently due to larger bin sizes.\n",
    "4. This completes the preparation for training **Decision Tree Stump 2** in AdaBoost.\n",
    "\n",
    "> Next Step: Selecting records for the next stump based on bins and repeating the AdaBoost steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65f42ea",
   "metadata": {},
   "source": [
    "# AdaBoost Step 5: Selecting New Data Points for the Next Decision Tree Stump\n",
    "\n",
    "In Step 5, we focus on **selecting the new data points** that will be sent to the next decision tree stump using the normalized weights and bins from Step 4.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5.1: Random Value Generation\n",
    "\n",
    "- We generate **random numbers between 0 and 1** to select records based on their bin ranges.\n",
    "- Each random number corresponds to a data point selected according to its weight.\n",
    "\n",
    "| Salary      | Credit | Approval | Random Number |\n",
    "|------------|--------|----------|---------------|\n",
    "| >50K       | Normal | Yes      | 0.50          |\n",
    "| <50K       | Good   | Yes      | 0.10          |\n",
    "| >50K       | Normal | Yes      | 0.60          |\n",
    "| >50K       | Normal | Yes      | 0.75          |\n",
    "| <50K       | Good   | Yes      | 0.24          |\n",
    "| >50K       | Bad    | No       | 0.32          |\n",
    "| >50K       | Normal | Yes      | 0.87          |\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5.2: Mapping Random Numbers to Bins\n",
    "\n",
    "- Each random number is mapped to a **bin range**:\n",
    "\n",
    "| Bin Range | Selected Record               |\n",
    "|-----------|-------------------------------|\n",
    "| 0.00 – 0.08 | <50K, Good, Yes              |\n",
    "| 0.08 – 0.16 | <50K, Good, Yes              |\n",
    "| 0.16 – 0.24 | <50K, Good, Yes              |\n",
    "| 0.24 – 0.32 | <50K, Good, Yes              |\n",
    "| 0.32 – 0.40 | >50K, Bad, No                |\n",
    "| 0.40 – 0.90 | >50K, Normal, Yes            |\n",
    "\n",
    "- Misclassified points have **larger bins**, increasing the probability of being selected.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5.3: Iterative Selection Process\n",
    "\n",
    "1. Continue generating random numbers until **all new records** for the next stump are selected.\n",
    "2. Misclassified points are likely to appear multiple times due to their larger bin ranges.\n",
    "3. The selected dataset is now ready to **train the next decision tree stump**.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5.4: Preparing Data for the Next Stump\n",
    "\n",
    "- Steps for the next stump repeat the process:\n",
    "  1. Assign **sample weights** (e.g., \\( \\frac{1}{6} \\) for each of 6 new records).\n",
    "  2. Compute **total error** and **stump performance**.\n",
    "  3. Update weights, normalize, and assign bins.\n",
    "  4. Select new records based on bins for the next iteration.\n",
    "\n",
    "- Example:\n",
    "\n",
    "| Model      | Alpha (Performance) |\n",
    "|------------|-------------------|\n",
    "| Decision Tree Stump 1 | 0.896            |\n",
    "| Decision Tree Stump 2 | 0.65             |\n",
    "\n",
    "- The final AdaBoost prediction will **combine all stumps** weighted by their alpha values.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5.5: Summary\n",
    "\n",
    "- Step 5 ensures that **misclassified points from the previous stump** are more likely to be selected for the next stump.\n",
    "- This iterative process continues until the desired number of weak learners (stumps) is created.\n",
    "- The AdaBoost classifier **sequentially improves** by focusing on harder-to-classify points.\n",
    "\n",
    "> Next Step: Using all the stumps and their alpha values to **predict new test data**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529df27d",
   "metadata": {},
   "source": [
    "# AdaBoost Final Step: Making Predictions on New Test Data\n",
    "\n",
    "In the final step, we understand how AdaBoost makes predictions for a **new test data point** in a **classification problem**.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6.1: Example Test Data\n",
    "\n",
    "Suppose we have a new test data point:\n",
    "\n",
    "- **Salary:** <50K  \n",
    "- **Credit Score:** Good  \n",
    "\n",
    "We have multiple decision tree stumps (weak learners):\n",
    "\n",
    "- **Decision Tree Stump 1:** predicts `Yes`  \n",
    "- **Decision Tree Stump 2:** predicts `No`  \n",
    "- **Decision Tree Stump 3:** predicts `Yes`  \n",
    "- **Decision Tree Stump 4:** predicts `No`  \n",
    "\n",
    "---\n",
    "\n",
    "## Step 6.2: Alpha Values (Weights)\n",
    "\n",
    "Each stump has a **performance weight (\\(\\alpha\\))**:\n",
    "\n",
    "| Stump | Alpha (\\(\\alpha\\)) | Prediction |\n",
    "|-------|-----------------|------------|\n",
    "| 1     | 0.896           | Yes        |\n",
    "| 2     | 0.650           | No         |\n",
    "| 3     | 0.244           | Yes        |\n",
    "| 4     | -0.30           | No         |\n",
    "\n",
    "- Alpha indicates the **importance** of each weak learner.\n",
    "- Negative alpha can also occur, depending on error.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6.3: Combining Stumps\n",
    "\n",
    "The **final AdaBoost prediction function** is:\n",
    "\n",
    "$$\n",
    "F(x) = \\alpha_1 \\cdot \\text{model}_1(x) + \\alpha_2 \\cdot \\text{model}_2(x) + \\alpha_3 \\cdot \\text{model}_3(x) + \\alpha_4 \\cdot \\text{model}_4(x)\n",
    "$$\n",
    "\n",
    "- `model_i(x)` is the prediction of the i-th stump, represented as:\n",
    "  - +1 for `Yes`  \n",
    "  - -1 for `No`\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6.4: Calculating Weighted Votes\n",
    "\n",
    "- Sum the **weights of Yes predictions**:\n",
    "\n",
    "$$\n",
    "\\text{Yes weight} = 0.896 + 0.244 = 1.140\n",
    "$$\n",
    "\n",
    "- Sum the **weights of No predictions**:\n",
    "\n",
    "$$\n",
    "\\text{No weight} = 0.650 + (-0.30) = 0.350\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6.5: Determine Final Prediction\n",
    "\n",
    "- Compare the total weighted votes:\n",
    "\n",
    "$$\n",
    "\\text{Yes: } 1.140 \\quad \\text{vs} \\quad \\text{No: } 0.350\n",
    "$$\n",
    "\n",
    "- **Maximum weight wins:** Yes > No  \n",
    "- **Final prediction:** `Yes`  \n",
    "\n",
    "> Interpretation: For a test data point with salary <50K and credit score good, the credit card **will be approved**.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6.6: Regression Case (Optional)\n",
    "\n",
    "For **regression problems**:\n",
    "\n",
    "- We **do not use entropy**.  \n",
    "- Use **mean squared error (MSE)** to select the best decision stump.  \n",
    "- Predictions are **continuous values**, weighted by \\(\\alpha\\).  \n",
    "- All other steps remain the same.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- AdaBoost combines multiple **weak learners** into a **strong learner**.  \n",
    "- Each stump's prediction is weighted by its performance (\\(\\alpha\\)).  \n",
    "- Final prediction is based on the **weighted majority vote** for classification or **weighted sum** for regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070b573",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
