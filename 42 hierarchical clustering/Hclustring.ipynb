{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bef6090",
   "metadata": {},
   "source": [
    "# ğŸŒ³ Hierarchical Clustering â€” Complete Explanation\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Goal of Hierarchical Clustering\n",
    "\n",
    "Letâ€™s assume we have a dataset like this ğŸ‘‡  \n",
    "\n",
    "After applying **Hierarchical Clustering**, weâ€™ll be able to form **three distinct clusters** (groups of data points).  \n",
    "\n",
    "Now, letâ€™s compare this with **K-Means Clustering**:\n",
    "\n",
    "| Concept | K-Means | Hierarchical |\n",
    "|:--------|:---------|:-------------|\n",
    "| <span style=\"color:orange;\">Centroids</span> | Used (one per cluster) | <span style=\"color:red;\">Not used</span> |\n",
    "| <span style=\"color:blue;\">Approach</span> | Flat (requires k) | <span style=\"color:green;\">Tree-like (Dendrogram)</span> |\n",
    "| <span style=\"color:purple;\">Cluster Formation</span> | Iterative | Step-by-step merging or splitting |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Types of Hierarchical Clustering\n",
    "\n",
    "There are **two types** of Hierarchical Clustering:\n",
    "\n",
    "1. <span style=\"color:orange;\">Agglomerative Clustering</span> â€” **Bottom-Up Approach**\n",
    "2. <span style=\"color:red;\">Divisive Clustering</span> â€” **Top-Down Approach**\n",
    "\n",
    "If you understand one, you can easily understand the other since they are **opposites**.\n",
    "\n",
    "- **Agglomerative:** Start with individual points and **merge** them into clusters.  \n",
    "- **Divisive:** Start with one big cluster and **divide** it into smaller ones.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© Agglomerative Clustering â€” Step-by-Step Intuition\n",
    "\n",
    "Letâ€™s consider we have **six data points**:  \n",
    "<span style=\"color:orange;\">P1, P2, P3, P4, P5, P6</span>\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Step 1 â€” Treat Each Point as a Cluster\n",
    "\n",
    "Initially, every data point is considered its **own cluster**.\n",
    "\n",
    "So we have:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339fe601",
   "metadata": {},
   "source": [
    "{P1}, {P2}, {P3}, {P4}, {P5}, {P6}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16632f62",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Step 2 â€” Find the Nearest Points and Merge Them\n",
    "\n",
    "We use a **distance metric** (like Euclidean or Manhattan distance) to find the closest pairs.\n",
    "\n",
    "Suppose:\n",
    "- <span style=\"color:blue;\">(P4, P5)</span> are closest â†’ merge into one cluster  \n",
    "- <span style=\"color:orange;\">(P1, P2)</span> are next closest â†’ merge them too  \n",
    "- Then <span style=\"color:red;\">P6</span> joins the cluster of (P4, P5)\n",
    "- <span style=\"color:green;\">P3</span> merges with (P1, P2)\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”¹ Step 3 â€” Repeat Until One Big Cluster Forms\n",
    "\n",
    "We keep merging the **nearest clusters** until all points belong to a **single cluster**.\n",
    "\n",
    "This approach is called the  \n",
    "> **Agglomerative (Bottom-Up)** method ğŸ§©\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ² Dendrogram â€” Visual Representation\n",
    "\n",
    "A **Dendrogram** is a tree-like diagram showing the merging of points step by step.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸªœ Steps to Build a Dendrogram\n",
    "\n",
    "1. Place each data point on the **x-axis** (P1 to P6).  \n",
    "2. The **y-axis** represents the **distance (usually Euclidean)**.  \n",
    "3. Merge points with **smallest distances** first.  \n",
    "4. Draw **vertical lines** connecting the merged clusters at the corresponding **distance height**.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ Example\n",
    "\n",
    "Each time we merge clusters, we draw a line:\n",
    "\n",
    "- Merge <span style=\"color:orange;\">P4 & P5</span> â†’ small height  \n",
    "- Merge <span style=\"color:blue;\">P1 & P2</span> â†’ slightly larger height  \n",
    "- Merge <span style=\"color:red;\">P6</span> with (P4,P5) â†’ higher height  \n",
    "- Merge <span style=\"color:green;\">P3</span> with (P1,P2) â†’ even higher  \n",
    "- Finally, merge the two big clusters â†’ top of the dendrogram  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ³ï¸ Understanding Distance (Height)\n",
    "\n",
    "The **height** of each merge in the dendrogram corresponds to the **Euclidean Distance** at which clusters were joined.\n",
    "\n",
    "You can use either of the following:\n",
    "\n",
    "$$\n",
    "D(P_i, P_j) = \\sqrt{(x_i - x_j)^2 + (y_i - y_j)^2}\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "D(P_i, P_j) = |x_i - x_j| + |y_i - y_j|\n",
    "$$\n",
    "\n",
    "for **Manhattan Distance**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§® Determining the Number of Clusters (k)\n",
    "\n",
    "Now comes the key question:\n",
    "\n",
    "> â“ How do we decide the **number of clusters (k)** from the dendrogram?\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸšï¸ Threshold (Distance Cut)\n",
    "\n",
    "We select a **threshold distance** on the y-axis and draw a **horizontal line**.  \n",
    "Where this line cuts the dendrogram defines the **number of clusters**.\n",
    "\n",
    "For example:\n",
    "- If threshold = 4 â†’ line cuts **2 vertical bars** â†’ **k = 2**\n",
    "- If threshold = 2.5 â†’ line cuts **4 vertical bars** â†’ **k = 4**\n",
    "\n",
    "So,  \n",
    "> âœ… **Lower threshold â†’ More clusters**  \n",
    "> âœ… **Higher threshold â†’ Fewer clusters**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§­ Shortcut to Find Optimal k\n",
    "\n",
    "Thereâ€™s a simple **hack** for selecting k from a dendrogram:\n",
    "\n",
    "> âœ¨ **Select the longest vertical line**  \n",
    "> such that **no other horizontal line crosses it.**\n",
    "\n",
    "This vertical line represents the **largest distance gap** between merges â€”  \n",
    "the **optimal cluster separation** point.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ§± Visual Intuition\n",
    "\n",
    "- Longest vertical line = biggest gap = natural cluster separation  \n",
    "- Draw a horizontal line through it  \n",
    "- Count how many vertical bars it intersects â†’ **that number = optimal k**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ Divisive Clustering (Top-Down Approach)\n",
    "\n",
    "The **Divisive method** works in the **reverse direction**:\n",
    "\n",
    "- Start with **all points in one cluster**\n",
    "- Then **split** clusters recursively into smaller groups\n",
    "- Continue until each point becomes its own cluster\n",
    "\n",
    "So,\n",
    "- **Agglomerative:** Bottom â†’ Top  \n",
    "- **Divisive:** Top â†’ Bottom\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary Table\n",
    "\n",
    "| Concept | Agglomerative | Divisive |\n",
    "|:--------|:---------------|:----------|\n",
    "| <span style=\"color:orange;\">Direction</span> | Bottom â†’ Top | Top â†’ Bottom |\n",
    "| <span style=\"color:blue;\">Action</span> | Merge clusters | Split clusters |\n",
    "| <span style=\"color:green;\">Initial Condition</span> | Each point = own cluster | All points = one cluster |\n",
    "| <span style=\"color:red;\">Dendrogram</span> | Built by merging | Built by dividing |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Mathematical Reminder\n",
    "\n",
    "Threshold = Distance cut-off  \n",
    "If the **Euclidean Distance** between points is less than threshold, they are **merged** into one cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—£ï¸ Interview Tip\n",
    "\n",
    "> â“ *How do you decide the number of clusters in Hierarchical Clustering?*\n",
    "\n",
    "âœ… **Answer:**\n",
    "> By analyzing the **dendrogram** â€” we draw a **horizontal line** at the **largest vertical gap** (no horizontal cuts through it).  \n",
    "> The number of intersections gives the **optimal k** value.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ Conclusion\n",
    "\n",
    "In this video, we covered:\n",
    "- Difference between **K-Means** and **Hierarchical Clustering**\n",
    "- **Agglomerative vs Divisive** approach\n",
    "- Concept of **Dendrogram**\n",
    "- How to **select k value** using threshold and longest vertical line\n",
    "\n",
    "Next, weâ€™ll discuss the **differences between Hierarchical and K-Means Clustering** \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a149a30d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
