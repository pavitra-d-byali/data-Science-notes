{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6677a4a7",
   "metadata": {},
   "source": [
    "# Performance Metrics in Classification\n",
    "\n",
    "In this video, we are going to discuss **performance metrics** which are specifically used in:\n",
    "\n",
    "- **Binary Classification**\n",
    "- **Multi-class Classification**\n",
    "\n",
    "We will cover:\n",
    "\n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n",
    "\n",
    "## Logistic Regression Recap\n",
    "\n",
    "Logistic regression is used for **classification problems**.  \n",
    "We separate categories using a **decision boundary (linear line)**.  \n",
    "\n",
    "Example:  \n",
    "- If a point lies above the line â†’ category 1  \n",
    "- If a point lies below the line â†’ category 0  \n",
    "\n",
    "To evaluate how the model is performing, we need **performance metrics**.  \n",
    "\n",
    "For regression problems, we used:  \n",
    "- $R^2$ score  \n",
    "- Adjusted $R^2$ score  \n",
    "\n",
    "For classification problems, we use:  \n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "The **confusion matrix** is the foundation of classification metrics.  \n",
    "For **binary classification**, it is a **2 Ã— 2 matrix**.\n",
    "\n",
    "|                | **Predicted 1** | **Predicted 0** |\n",
    "|----------------|-----------------|-----------------|\n",
    "| **Actual 1**   | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual 0**   | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "- **TP (True Positive):** Model predicts **1**, actual is **1**  \n",
    "- **TN (True Negative):** Model predicts **0**, actual is **0**  \n",
    "- **FP (False Positive):** Model predicts **1**, actual is **0** (Type I Error)  \n",
    "- **FN (False Negative):** Model predicts **0**, actual is **1** (Type II Error)  \n",
    "\n",
    "---\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "Accuracy measures the fraction of **correct predictions**:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "Example: If  \n",
    "- $TP = 3$, $TN = 1$, $FP = 2$, $FN = 1$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{3 + 1}{3 + 1 + 2 + 1} = \\frac{4}{7}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Problem with Accuracy: Imbalanced Dataset\n",
    "\n",
    "If dataset has **imbalanced categories**, accuracy may give a **misleading result**.\n",
    "\n",
    "Example:  \n",
    "- 1000 samples â†’ 900 = Class 1, 100 = Class 0  \n",
    "- If a model predicts **all as Class 1**, accuracy = $90\\%$  \n",
    "- But the model is **useless**, since it never predicts Class 0 correctly.  \n",
    "\n",
    "Thus, in **imbalanced datasets**, we use **Precision and Recall**.\n",
    "\n",
    "---\n",
    "\n",
    "## Precision\n",
    "\n",
    "Precision focuses on **False Positives**.  \n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- Out of all **predicted positives**, how many are actually positive?  \n",
    "\n",
    "---\n",
    "\n",
    "## Recall\n",
    "\n",
    "Recall focuses on **False Negatives**.  \n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- Out of all **actual positives**, how many are correctly predicted?  \n",
    "\n",
    "---\n",
    "\n",
    "## Precision vs Recall: Use Cases\n",
    "\n",
    "### 1. Spam Classification\n",
    "- If a mail is **not spam**, but predicted as **spam** â†’ **False Positive**  \n",
    "- This is a **big blunder** (important mails lost).  \n",
    "- **Focus:** Reduce False Positives â†’ Use **Precision**  \n",
    "\n",
    "### 2. Medical Diagnosis (e.g., Diabetes Detection)\n",
    "- If a person **has diabetes**, but model predicts **no diabetes** â†’ **False Negative**  \n",
    "- This is **dangerous** (disease missed).  \n",
    "- **Focus:** Reduce False Negatives â†’ Use **Recall**\n",
    "\n",
    "---\n",
    "\n",
    "## F-beta Score\n",
    "\n",
    "When **both FP and FN are important**, we use **F-beta score**:\n",
    "\n",
    "$$\n",
    "F_\\beta = \\frac{(1 + \\beta^2) \\cdot (\\text{Precision} \\cdot \\text{Recall})}{(\\beta^2 \\cdot \\text{Precision}) + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "Special cases:\n",
    "\n",
    "1. **F1 Score** (balanced case, FP = FN important):  \n",
    "   $\\beta = 1$\n",
    "\n",
    "   $$\n",
    "   F_1 = \\frac{2 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{\\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "   â†’ Harmonic mean of Precision and Recall  \n",
    "\n",
    "2. **F0.5 Score** (FP more important than FN):  \n",
    "   $\\beta = 0.5$\n",
    "\n",
    "   $$\n",
    "   F_{0.5} = \\frac{1.25 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{0.25 \\cdot \\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "3. **F2 Score** (FN more important than FP):  \n",
    "   $\\beta = 2$\n",
    "\n",
    "   $$\n",
    "   F_2 = \\frac{5 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{4 \\cdot \\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Accuracy** â†’ Use when dataset is **balanced**  \n",
    "- **Precision** â†’ Use when **False Positives** are critical (e.g., spam classification)  \n",
    "- **Recall** â†’ Use when **False Negatives** are critical (e.g., medical diagnosis)  \n",
    "- **F-beta Score** â†’ Use when **both FP and FN are important**  \n",
    "\n",
    "---\n",
    "\n",
    "> Next: We will discuss the **ROC Curve** and AUC as performance metrics. ðŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd8b4d9",
   "metadata": {},
   "source": [
    "# Performance Metrics in Classification\n",
    "\n",
    "In this video, we are going to discuss **performance metrics** which are specifically used in:\n",
    "\n",
    "- **Binary Classification**\n",
    "- **Multi-class Classification**\n",
    "\n",
    "We will cover:\n",
    "\n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n",
    "\n",
    "## Logistic Regression Recap\n",
    "\n",
    "Logistic regression is used for **classification problems**.  \n",
    "We separate categories using a **decision boundary (linear line)**.  \n",
    "\n",
    "Example:  \n",
    "- If a point lies above the line â†’ category 1  \n",
    "- If a point lies below the line â†’ category 0  \n",
    "\n",
    "To evaluate how the model is performing, we need **performance metrics**.  \n",
    "\n",
    "For regression problems, we used:  \n",
    "- $R^2$ score  \n",
    "- Adjusted $R^2$ score  \n",
    "\n",
    "For classification problems, we use:  \n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "The **confusion matrix** is the foundation of classification metrics.  \n",
    "For **binary classification**, it is a **2 Ã— 2 matrix**.\n",
    "\n",
    "|                | **Predicted 1** | **Predicted 0** |\n",
    "|----------------|-----------------|-----------------|\n",
    "| **Actual 1**   | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual 0**   | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "- **TP (True Positive):** Model predicts **1**, actual is **1**  \n",
    "- **TN (True Negative):** Model predicts **0**, actual is **0**  \n",
    "- **FP (False Positive):** Model predicts **1**, actual is **0** (Type I Error)  \n",
    "- **FN (False Negative):** Model predicts **0**, actual is **1** (Type II Error)  \n",
    "\n",
    "---\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "Accuracy measures the fraction of **correct predictions**:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "Example: If  \n",
    "- $TP = 3$, $TN = 1$, $FP = 2$, $FN = 1$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{3 + 1}{3 + 1 + 2 + 1} = \\frac{4}{7}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173d5231",
   "metadata": {},
   "source": [
    "# Logistic Regression for Multi-Class Classification\n",
    "---\n",
    "\n",
    "## Recap: Binary Classification with Logistic Regression\n",
    "\n",
    "- In binary classification, the goal was to create a **best-fit line** (decision boundary) that separates two classes.  \n",
    "- Example: Classes **A** and **B** divided using a line.\n",
    "\n",
    "But what if we have **more than two categories**?  \n",
    "For example, three categories: **Class 1, Class 2, Class 3**.\n",
    "\n",
    "---\n",
    "\n",
    "## Multi-Class Classification with Logistic Regression\n",
    "\n",
    "When the output feature has **three or more categories**, we need a way to extend logistic regression.  \n",
    "\n",
    "A common technique is:\n",
    "\n",
    "### **One-vs-Rest (OvR)**\n",
    "\n",
    "- For **k output categories**, create **k binary classifiers**.  \n",
    "- Each classifier decides:  \n",
    "  - **One class vs. all the others combined**  \n",
    "\n",
    "---\n",
    "\n",
    "## Example Dataset\n",
    "\n",
    "Suppose we have features $f_1, f_2, f_3$ and an output variable with **3 categories**:\n",
    "\n",
    "$$\n",
    "y \\in \\{1, 2, 3\\}\n",
    "$$\n",
    "\n",
    "### One-Hot Encoding of Output\n",
    "\n",
    "Convert categories into **one-hot encoded vectors**:\n",
    "\n",
    "- If $y = 1 \\quad \\rightarrow \\quad [1, 0, 0]$  \n",
    "- If $y = 2 \\quad \\rightarrow \\quad [0, 1, 0]$  \n",
    "- If $y = 3 \\quad \\rightarrow \\quad [0, 0, 1]$\n",
    "\n",
    "---\n",
    "\n",
    "## OvR Model Training\n",
    "\n",
    "We create **3 internal models**: $M_1, M_2, M_3$.\n",
    "\n",
    "1. **Model $M_1$**  \n",
    "   - Input: $(f_1, f_2, f_3)$  \n",
    "   - Output: $y_1 \\in \\{0,1\\}$  \n",
    "   - Learns to separate **Class 1 vs {Class 2, Class 3}**\n",
    "\n",
    "2. **Model $M_2$**  \n",
    "   - Input: $(f_1, f_2, f_3)$  \n",
    "   - Output: $y_2 \\in \\{0,1\\}$  \n",
    "   - Learns to separate **Class 2 vs {Class 1, Class 3}**\n",
    "\n",
    "3. **Model $M_3$**  \n",
    "   - Input: $(f_1, f_2, f_3)$  \n",
    "   - Output: $y_3 \\in \\{0,1\\}$  \n",
    "   - Learns to separate **Class 3 vs {Class 1, Class 2}**\n",
    "\n",
    "---\n",
    "\n",
    "## Logistic Regression Hypothesis\n",
    "\n",
    "For each model $M_k$:\n",
    "\n",
    "$$\n",
    "h_{\\theta}^{(k)}(x) = \\sigma(\\theta^{(k)T} x) = \\frac{1}{1 + e^{-\\theta^{(k)T}x}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $x = (f_1, f_2, f_3, 1)$ (with bias term)  \n",
    "- $\\theta^{(k)}$ are the parameters for model $M_k$  \n",
    "- $\\sigma(z) = \\dfrac{1}{1 + e^{-z}}$ is the sigmoid function  \n",
    "\n",
    "---\n",
    "\n",
    "## Training Objective\n",
    "\n",
    "For each classifier $M_k$, we minimize the **binary cross-entropy loss**:\n",
    "\n",
    "$$\n",
    "J(\\theta^{(k)}) = - \\frac{1}{m} \\sum_{i=1}^m \\Big[ y^{(i)}_k \\cdot \\log(h_{\\theta}^{(k)}(x^{(i)})) \\; + \\; (1 - y^{(i)}_k) \\cdot \\log(1 - h_{\\theta}^{(k)}(x^{(i)})) \\Big]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $m$ = number of training samples  \n",
    "- $y^{(i)}_k$ = 1 if sample $i$ belongs to class $k$, else 0  \n",
    "\n",
    "---\n",
    "\n",
    "## Prediction Phase\n",
    "\n",
    "For a **new test data point $x$**:\n",
    "\n",
    "1. Pass $x$ to all models $M_1, M_2, M_3$  \n",
    "2. Each model outputs a probability:  \n",
    "\n",
    "$$\n",
    "p_1 = h_{\\theta}^{(1)}(x), \\quad \n",
    "p_2 = h_{\\theta}^{(2)}(x), \\quad \n",
    "p_3 = h_{\\theta}^{(3)}(x)\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "- $p_1 = 0.25$  \n",
    "- $p_2 = 0.20$  \n",
    "- $p_3 = 0.55$\n",
    "\n",
    "3. Choose the class with **maximum probability**:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg \\max_{k \\in \\{1,2,3\\}} \\; p_k\n",
    "$$\n",
    "\n",
    "Here:\n",
    "\n",
    "$$\n",
    "\\hat{y} = 3 \\quad \\text{(since $p_3 = 0.55$ is highest)}\n",
    "$$\n",
    "\n",
    "So, the predicted category is **Class 3**.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary: One-vs-Rest (OvR) Logistic Regression\n",
    "\n",
    "- For $K$ classes, train **$K$ logistic regression models**  \n",
    "- Each model is **binary classification (one class vs rest)**  \n",
    "- During prediction, compare probabilities from all models  \n",
    "- Final prediction = **class with highest probability**  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e3d794",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
