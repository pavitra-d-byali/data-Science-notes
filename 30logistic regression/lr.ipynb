{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dad8ce1",
   "metadata": {},
   "source": [
    "# Logistic Regression: Intuition and Motivation\n",
    "\n",
    "> **Note:** Logistic regression is used for solving **binary classification problems**.\n",
    "\n",
    "We will also discuss its **use cases**, the **cost function**, and other key concepts.\n",
    "\n",
    "---\n",
    "\n",
    "## Binary Classification Problem\n",
    "\n",
    "Whenever we say **binary classification problem**, what does it mean?\n",
    "\n",
    "Consider a dataset with:\n",
    "\n",
    "- One feature: **study hours**\n",
    "- Output (dependent) feature: **pass/fail** (binary categories)\n",
    "\n",
    "Example data:\n",
    "\n",
    "| Study Hours | Result  |\n",
    "|------------|---------|\n",
    "| 2          | Fail    |\n",
    "| 3          | Fail    |\n",
    "| 4          | Fail    |\n",
    "| 5          | Pass    |\n",
    "| 6          | Pass    |\n",
    "\n",
    "The goal is:  \n",
    "\n",
    "> Given the **study hours**, predict whether the student will **pass or fail**.\n",
    "\n",
    "This is a **binary classification problem** because the output has **two categories**: 0 (fail) or 1 (pass).\n",
    "\n",
    "---\n",
    "\n",
    "## Can Linear Regression Solve This?\n",
    "\n",
    "Let's try to use **linear regression** to predict a binary outcome.\n",
    "\n",
    "- Assign **Pass = 1** and **Fail = 0**\n",
    "- Plot the data points with **x-axis = study hours** and **y-axis = output (0 or 1)**\n",
    "\n",
    "Linear regression tries to fit a **best-fit line**:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $x$ = study hours\n",
    "- $y$ = predicted probability (ideally between 0 and 1)\n",
    "- $\\beta_0$, $\\beta_1$ = coefficients\n",
    "\n",
    "---\n",
    "\n",
    "### Predicting with Linear Regression\n",
    "\n",
    "- For a new data point, predict $y$.\n",
    "- Convert it to binary output:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } y > 0.5 \\\\\n",
    "0 & \\text{if } y \\leq 0.5 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This seems to work initially.\n",
    "\n",
    "---\n",
    "\n",
    "### Problems with Linear Regression for Classification\n",
    "\n",
    "1. **Sensitive to outliers:**\n",
    "\n",
    "   - If a new data point is an outlier (e.g., 12 study hours â†’ Pass), the best-fit line shifts.\n",
    "   - This may incorrectly predict earlier points (e.g., 5 hours â†’ Fail).\n",
    "\n",
    "2. **Output can be outside [0,1]:**\n",
    "\n",
    "   - Linear regression can predict $y > 1$ or $y < 0$.\n",
    "   - Binary classification requires **predictions between 0 and 1**.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Logistic Regression?\n",
    "\n",
    "To overcome the above issues, we need a method that **squashes** the output between 0 and 1.\n",
    "\n",
    "- Logistic regression applies the **sigmoid function**:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "z = \\beta_0 + \\beta_1 x\n",
    "$$\n",
    "\n",
    "- This ensures:\n",
    "\n",
    "$$\n",
    "0 \\leq \\sigma(z) \\leq 1\n",
    "$$\n",
    "\n",
    "- We can then classify using:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } \\sigma(z) > 0.5 \\\\\n",
    "0 & \\text{if } \\sigma(z) \\leq 0.5\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Summary: Why Not Linear Regression?\n",
    "\n",
    "1. **Outliers** affect the line drastically.\n",
    "2. **Predictions can exceed [0,1]**, which is invalid for probabilities.\n",
    "3. Logistic regression **squashes** the line using the **sigmoid function**, ensuring valid probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f5382f",
   "metadata": {},
   "source": [
    "# Performance Metrics in Classification\n",
    "\n",
    "In this video, we are going to discuss **performance metrics** which are specifically used in:\n",
    "\n",
    "- **Binary Classification**\n",
    "- **Multi-class Classification**\n",
    "\n",
    "We will cover:\n",
    "\n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n",
    "\n",
    "## Logistic Regression Recap\n",
    "\n",
    "Logistic regression is used for **classification problems**.  \n",
    "We separate categories using a **decision boundary (linear line)**.  \n",
    "\n",
    "Example:  \n",
    "- If a point lies above the line â†’ category 1  \n",
    "- If a point lies below the line â†’ category 0  \n",
    "\n",
    "To evaluate how the model is performing, we need **performance metrics**.  \n",
    "\n",
    "For regression problems, we used:  \n",
    "- $R^2$ score  \n",
    "- Adjusted $R^2$ score  \n",
    "\n",
    "For classification problems, we use:  \n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "The **confusion matrix** is the foundation of classification metrics.  \n",
    "For **binary classification**, it is a **2 Ã— 2 matrix**.\n",
    "\n",
    "|                | **Predicted 1** | **Predicted 0** |\n",
    "|----------------|-----------------|-----------------|\n",
    "| **Actual 1**   | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual 0**   | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "- **TP (True Positive):** Model predicts **1**, actual is **1**  \n",
    "- **TN (True Negative):** Model predicts **0**, actual is **0**  \n",
    "- **FP (False Positive):** Model predicts **1**, actual is **0** (Type I Error)  \n",
    "- **FN (False Negative):** Model predicts **0**, actual is **1** (Type II Error)  \n",
    "\n",
    "---\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "Accuracy measures the fraction of **correct predictions**:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "Example: If  \n",
    "- $TP = 3$, $TN = 1$, $FP = 2$, $FN = 1$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{3 + 1}{3 + 1 + 2 + 1} = \\frac{4}{7}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Problem with Accuracy: Imbalanced Dataset\n",
    "\n",
    "If dataset has **imbalanced categories**, accuracy may give a **misleading result**.\n",
    "\n",
    "Example:  \n",
    "- 1000 samples â†’ 900 = Class 1, 100 = Class 0  \n",
    "- If a model predicts **all as Class 1**, accuracy = $90\\%$  \n",
    "- But the model is **useless**, since it never predicts Class 0 correctly.  \n",
    "\n",
    "Thus, in **imbalanced datasets**, we use **Precision and Recall**.\n",
    "\n",
    "---\n",
    "\n",
    "## Precision\n",
    "\n",
    "Precision focuses on **False Positives**.  \n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- Out of all **predicted positives**, how many are actually positive?  \n",
    "\n",
    "---\n",
    "\n",
    "## Recall\n",
    "\n",
    "Recall focuses on **False Negatives**.  \n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- Out of all **actual positives**, how many are correctly predicted?  \n",
    "\n",
    "---\n",
    "\n",
    "## Precision vs Recall: Use Cases\n",
    "\n",
    "### 1. Spam Classification\n",
    "- If a mail is **not spam**, but predicted as **spam** â†’ **False Positive**  \n",
    "- This is a **big blunder** (important mails lost).  \n",
    "- **Focus:** Reduce False Positives â†’ Use **Precision**  \n",
    "\n",
    "### 2. Medical Diagnosis (e.g., Diabetes Detection)\n",
    "- If a person **has diabetes**, but model predicts **no diabetes** â†’ **False Negative**  \n",
    "- This is **dangerous** (disease missed).  \n",
    "- **Focus:** Reduce False Negatives â†’ Use **Recall**\n",
    "\n",
    "---\n",
    "\n",
    "## F-beta Score\n",
    "\n",
    "When **both FP and FN are important**, we use **F-beta score**:\n",
    "\n",
    "$$\n",
    "F_\\beta = \\frac{(1 + \\beta^2) \\cdot (\\text{Precision} \\cdot \\text{Recall})}{(\\beta^2 \\cdot \\text{Precision}) + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "Special cases:\n",
    "\n",
    "1. **F1 Score** (balanced case, FP = FN important):  \n",
    "   $\\beta = 1$\n",
    "\n",
    "   $$\n",
    "   F_1 = \\frac{2 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{\\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "   â†’ Harmonic mean of Precision and Recall  \n",
    "\n",
    "2. **F0.5 Score** (FP more important than FN):  \n",
    "   $\\beta = 0.5$\n",
    "\n",
    "   $$\n",
    "   F_{0.5} = \\frac{1.25 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{0.25 \\cdot \\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "3. **F2 Score** (FN more important than FP):  \n",
    "   $\\beta = 2$\n",
    "\n",
    "   $$\n",
    "   F_2 = \\frac{5 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{4 \\cdot \\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Accuracy** â†’ Use when dataset is **balanced**  \n",
    "- **Precision** â†’ Use when **False Positives** are critical (e.g., spam classification)  \n",
    "- **Recall** â†’ Use when **False Negatives** are critical (e.g., medical diagnosis)  \n",
    "- **F-beta Score** â†’ Use when **both FP and FN are important**  \n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e12e354",
   "metadata": {},
   "source": [
    "# Performance Metrics in Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83ca601",
   "metadata": {},
   "source": [
    " **performance metrics** which are specifically used in:\n",
    "\n",
    "- **Binary Classification**\n",
    "- **Multi-class Classification**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0f1a13",
   "metadata": {},
   "source": [
    "We will cover:\n",
    "\n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n",
    "\n",
    "## Logistic Regression Recap\n",
    "\n",
    "Logistic regression is used for **classification problems**.  \n",
    "We separate categories using a **decision boundary (linear line)**.  \n",
    "\n",
    "Example:  \n",
    "- If a point lies above the line â†’ category 1  \n",
    "- If a point lies below the line â†’ category 0  \n",
    "\n",
    "To evaluate how the model is performing, we need **performance metrics**.  \n",
    "\n",
    "For regression problems, we used:  \n",
    "- $R^2$ score  \n",
    "- Adjusted $R^2$ score  \n",
    "\n",
    "For classification problems, we use:  \n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "The **confusion matrix** is the foundation of classification metrics.  \n",
    "For **binary classification**, it is a **2 Ã— 2 matrix**.\n",
    "\n",
    "|                | **Predicted 1** | **Predicted 0** |\n",
    "|----------------|-----------------|-----------------|\n",
    "| **Actual 1**   | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual 0**   | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "- **TP (True Positive):** Model predicts **1**, actual is **1**  \n",
    "- **TN (True Negative):** Model predicts **0**, actual is **0**  \n",
    "- **FP (False Positive):** Model predicts **1**, actual is **0** (Type I Error)  \n",
    "- **FN (False Negative):** Model predicts **0**, actual is **1** (Type II Error)  \n",
    "\n",
    "---\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "Accuracy measures the fraction of **correct predictions**:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "Example: If  \n",
    "- $TP = 3$, $TN = 1$, $FP = 2$, $FN = 1$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{3 + 1}{3 + 1 + 2 + 1} = \\frac{4}{7}\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b544a",
   "metadata": {},
   "source": [
    "\n",
    "## Logistic Regression Recap\n",
    "\n",
    "Logistic regression is used for **classification problems**.  \n",
    "We separate categories using a **decision boundary (linear line)**.  \n",
    "\n",
    "Example:  \n",
    "- If a point lies above the line â†’ category 1  \n",
    "- If a point lies below the line â†’ category 0  \n",
    "\n",
    "To evaluate how the model is performing, we need **performance metrics**.  \n",
    "\n",
    "For regression problems, we used:  \n",
    "- $R^2$ score  \n",
    "- Adjusted $R^2$ score  \n",
    "\n",
    "For classification problems, we use:  \n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8824e74e",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "The **confusion matrix** is the foundation of classification metrics.  \n",
    "For **binary classification**, it is a **2 Ã— 2 matrix**.\n",
    "\n",
    "|                | **Predicted 1** | **Predicted 0** |\n",
    "|----------------|-----------------|-----------------|\n",
    "| **Actual 1**   | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual 0**   | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "- **TP (True Positive):** Model predicts **1**, actual is **1**  \n",
    "- **TN (True Negative):** Model predicts **0**, actual is **0**  \n",
    "- **FP (False Positive):** Model predicts **1**, actual is **0** (Type I Error)  \n",
    "- **FN (False Negative):** Model predicts **0**, actual is **1** (Type II Error)  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a5b787",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f92f2",
   "metadata": {},
   "source": [
    "The **confusion matrix** is the foundation of classification metrics.  \n",
    "For **binary classification**, it is a **2 Ã— 2 matrix**.\n",
    "\n",
    "|                | **Predicted 1** | **Predicted 0** |\n",
    "|----------------|-----------------|-----------------|\n",
    "| **Actual 1**   | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual 0**   | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "- **TP (True Positive):** Model predicts **1**, actual is **1**  \n",
    "- **TN (True Negative):** Model predicts **0**, actual is **0**  \n",
    "- **FP (False Positive):** Model predicts **1**, actual is **0** (Type I Error)  \n",
    "- **FN (False Negative):** Model predicts **0**, actual is **1** (Type II Error)  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39014bde",
   "metadata": {},
   "source": [
    "The **confusion matrix** is the foundation of classification metrics.  \n",
    "For **binary classification**, it is a **2 Ã— 2 matrix**.\n",
    "\n",
    "\n",
    "\n",
    "- **TP (True Positive):** Model predicts **1**, actual is **1**  \n",
    "- **TN (True Negative):** Model predicts **0**, actual is **0**  \n",
    "- **FP (False Positive):** Model predicts **1**, actual is **0** (Type I Error)  \n",
    "- **FN (False Negative):** Model predicts **0**, actual is **1** (Type II Error)  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026fb1f0",
   "metadata": {},
   "source": [
    "|                | **Predicted 1** | **Predicted 0** |\n",
    "|----------------|-----------------|-----------------|\n",
    "| **Actual 1**   | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual 0**   | False Positive (FP) | True Negative (TN) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0ee3c",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Accuracy measures the fraction of **correct predictions**:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "Example: If  \n",
    "- $TP = 3$, $TN = 1$, $FP = 2$, $FN = 1$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{3 + 1}{3 + 1 + 2 + 1} = \\frac{4}{7}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Problem with Accuracy: Imbalanced Dataset\n",
    "\n",
    "If dataset has **imbalanced categories**, accuracy may give a **misleading result**.\n",
    "\n",
    "Example:  \n",
    "- 1000 samples â†’ 900 = Class 1, 100 = Class 0  \n",
    "- If a model predicts **all as Class 1**, accuracy = $90\\%$  \n",
    "- But the model is **useless**, since it never predicts Class 0 correctly.  \n",
    "\n",
    "Thus, in **imbalanced datasets**, we use **Precision and Recall**.\n",
    "\n",
    "---\n",
    "\n",
    "## Precision\n",
    "\n",
    "Precision focuses on **False Positives**.  \n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- Out of all **predicted positives**, how many are actually positive?  \n",
    "\n",
    "---\n",
    "\n",
    "## Recall\n",
    "\n",
    "Recall focuses on **False Negatives**.  \n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- Out of all **actual positives**, how many are correctly predicted?  \n",
    "\n",
    "---\n",
    "\n",
    "## Precision vs Recall: Use Cases\n",
    "\n",
    "### 1. Spam Classification\n",
    "- If a mail is **not spam**, but predicted as **spam** â†’ **False Positive**  \n",
    "- This is a **big blunder** (important mails lost).  \n",
    "- **Focus:** Reduce False Positives â†’ Use **Precision**  \n",
    "\n",
    "### 2. Medical Diagnosis (e.g., Diabetes Detection)\n",
    "- If a person **has diabetes**, but model predicts **no diabetes** â†’ **False Negative**  \n",
    "- This is **dangerous** (disease missed).  \n",
    "- **Focus:** Reduce False Negatives â†’ Use **Recall**\n",
    "\n",
    "---\n",
    "\n",
    "## F-beta Score\n",
    "\n",
    "When **both FP and FN are important**, we use **F-beta score**:\n",
    "\n",
    "$$\n",
    "F_\\beta = \\frac{(1 + \\beta^2) \\cdot (\\text{Precision} \\cdot \\text{Recall})}{(\\beta^2 \\cdot \\text{Precision}) + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "Special cases:\n",
    "\n",
    "1. **F1 Score** (balanced case, FP = FN important):  \n",
    "   $\\beta = 1$\n",
    "\n",
    "   $$\n",
    "   F_1 = \\frac{2 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{\\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "   â†’ Harmonic mean of Precision and Recall  \n",
    "\n",
    "2. **F0.5 Score** (FP more important than FN):  \n",
    "   $\\beta = 0.5$\n",
    "\n",
    "   $$\n",
    "   F_{0.5} = \\frac{1.25 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{0.25 \\cdot \\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "3. **F2 Score** (FN more important than FP):  \n",
    "   $\\beta = 2$\n",
    "\n",
    "   $$\n",
    "   F_2 = \\frac{5 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{4 \\cdot \\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Accuracy** â†’ Use when dataset is **balanced**  \n",
    "- **Precision** â†’ Use when **False Positives** are critical (e.g., spam classification)  \n",
    "- **Recall** â†’ Use when **False Negatives** are critical (e.g., medical diagnosis)  \n",
    "- **F-beta Score** â†’ Use when **both FP and FN are important**  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae5896",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16596c8b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc1a7ed6",
   "metadata": {},
   "source": [
    "# Performance Metrics in Classification\n",
    "\n",
    "- **Binary Classification**\n",
    "- **Multi-class Classification**\n",
    "\n",
    "We will cover:\n",
    "\n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n",
    "\n",
    "## Logistic Regression Recap\n",
    "\n",
    "Logistic regression is used for **classification problems**.  \n",
    "We separate categories using a **decision boundary (linear line)**.  \n",
    "\n",
    "Example:  \n",
    "- If a point lies above the line â†’ category 1  \n",
    "- If a point lies below the line â†’ category 0  \n",
    "\n",
    "To evaluate how the model is performing, we need **performance metrics**.  \n",
    "\n",
    "For regression problems, we used:  \n",
    "- $R^2$ score  \n",
    "- Adjusted $R^2$ score  \n",
    "\n",
    "For classification problems, we use:  \n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "The **confusion matrix** is the foundation of classification metrics.  \n",
    "For **binary classification**, it is a **2 Ã— 2 matrix**.\n",
    "\n",
    "|                | **Predicted 1** | **Predicted 0** |\n",
    "|----------------|-----------------|-----------------|\n",
    "| **Actual 1**   | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual 0**   | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "- **TP (True Positive):** Model predicts **1**, actual is **1**  \n",
    "- **TN (True Negative):** Model predicts **0**, actual is **0**  \n",
    "- **FP (False Positive):** Model predicts **1**, actual is **0** (Type I Error)  \n",
    "- **FN (False Negative):** Model predicts **0**, actual is **1** (Type II Error)  \n",
    "\n",
    "---\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "Accuracy measures the fraction of **correct predictions**:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "Example: If  \n",
    "- $TP = 3$, $TN = 1$, $FP = 2$, $FN = 1$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{3 + 1}{3 + 1 + 2 + 1} = \\frac{4}{7}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Problem with Accuracy: Imbalanced Dataset\n",
    "\n",
    "If dataset has **imbalanced categories**, accuracy may give a **misleading result**.\n",
    "\n",
    "Example:  \n",
    "- 1000 samples â†’ 900 = Class 1, 100 = Class 0  \n",
    "- If a model predicts **all as Class 1**, accuracy = $90\\%$  \n",
    "- But the model is **useless**, since it never predicts Class 0 correctly.  \n",
    "\n",
    "Thus, in **imbalanced datasets**, we use **Precision and Recall**.\n",
    "\n",
    "---\n",
    "\n",
    "## Precision\n",
    "\n",
    "Precision focuses on **False Positives**.  \n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- Out of all **predicted positives**, how many are actually positive?  \n",
    "\n",
    "---\n",
    "\n",
    "## Recall\n",
    "\n",
    "Recall focuses on **False Negatives**.  \n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- Out of all **actual positives**, how many are correctly predicted?  \n",
    "\n",
    "---\n",
    "\n",
    "## Precision vs Recall: Use Cases\n",
    "\n",
    "### 1. Spam Classification\n",
    "- If a mail is **not spam**, but predicted as **spam** â†’ **False Positive**  \n",
    "- This is a **big blunder** (important mails lost).  \n",
    "- **Focus:** Reduce False Positives â†’ Use **Precision**  \n",
    "\n",
    "### 2. Medical Diagnosis (e.g., Diabetes Detection)\n",
    "- If a person **has diabetes**, but model predicts **no diabetes** â†’ **False Negative**  \n",
    "- This is **dangerous** (disease missed).  \n",
    "- **Focus:** Reduce False Negatives â†’ Use **Recall**\n",
    "\n",
    "---\n",
    "\n",
    "## F-beta Score\n",
    "\n",
    "When **both FP and FN are important**, we use **F-beta score**:\n",
    "\n",
    "$$\n",
    "F_\\beta = \\frac{(1 + \\beta^2) \\cdot (\\text{Precision} \\cdot \\text{Recall})}{(\\beta^2 \\cdot \\text{Precision}) + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "Special cases:\n",
    "\n",
    "1. **F1 Score** (balanced case, FP = FN important):  \n",
    "   $\\beta = 1$\n",
    "\n",
    "   $$\n",
    "   F_1 = \\frac{2 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{\\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "   â†’ Harmonic mean of Precision and Recall  \n",
    "\n",
    "2. **F0.5 Score** (FP more important than FN):  \n",
    "   $\\beta = 0.5$\n",
    "\n",
    "   $$\n",
    "   F_{0.5} = \\frac{1.25 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{0.25 \\cdot \\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "3. **F2 Score** (FN more important than FP):  \n",
    "   $\\beta = 2$\n",
    "\n",
    "   $$\n",
    "   F_2 = \\frac{5 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{4 \\cdot \\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Accuracy** â†’ Use when dataset is **balanced**  \n",
    "- **Precision** â†’ Use when **False Positives** are critical (e.g., spam classification)  \n",
    "- **Recall** â†’ Use when **False Negatives** are critical (e.g., medical diagnosis)  \n",
    "- **F-beta Score** â†’ Use when **both FP and FN are important**  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dafe9e",
   "metadata": {},
   "source": [
    "# Performance Metrics in Classification\n",
    "\n",
    "- **Binary Classification**\n",
    "- **Multi-class Classification**\n",
    "\n",
    "We will cover:\n",
    "\n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n",
    "\n",
    "## Logistic Regression Recap\n",
    "\n",
    "Logistic regression is used for **classification problems**.  \n",
    "We separate categories using a **decision boundary (linear line)**.  \n",
    "\n",
    "Example:  \n",
    "- If a point lies above the line â†’ category 1  \n",
    "- If a point lies below the line â†’ category 0  \n",
    "\n",
    "To evaluate how the model is performing, we need **performance metrics**.  \n",
    "\n",
    "For regression problems, we used:  \n",
    "- $R^2$ score  \n",
    "- Adjusted $R^2$ score  \n",
    "\n",
    "For classification problems, we use:  \n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "The **confusion matrix** is the foundation of classification metrics.  \n",
    "For **binary classification**, it is a **2 Ã— 2 matrix**.\n",
    "\n",
    "|                | **Predicted 1** | **Predicted 0** |\n",
    "|----------------|-----------------|-----------------|\n",
    "| **Actual 1**   | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual 0**   | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "- **TP (True Positive):** Model predicts **1**, actual is **1**  \n",
    "- **TN (True Negative):** Model predicts **0**, actual is **0**  \n",
    "- **FP (False Positive):** Model predicts **1**, actual is **0** (Type I Error)  \n",
    "- **FN (False Negative):** Model predicts **0**, actual is **1** (Type II Error)  \n",
    "\n",
    "---\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "Accuracy measures the fraction of **correct predictions**:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "Example: If  \n",
    "- $TP = 3$, $TN = 1$, $FP = 2$, $FN = 1$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{3 + 1}{3 + 1 + 2 + 1} = \\frac{4}{7}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Problem with Accuracy: Imbalanced Dataset\n",
    "\n",
    "If dataset has **imbalanced categories**, accuracy may give a **misleading result**.\n",
    "\n",
    "Example:  \n",
    "- 1000 samples â†’ 900 = Class 1, 100 = Class 0  \n",
    "- If a model predicts **all as Class 1**, accuracy = $90\\%$  \n",
    "- But the model is **useless**, since it never predicts Class 0 correctly.  \n",
    "\n",
    "Thus, in **imbalanced datasets**, we use **Precision and Recall**.\n",
    "\n",
    "---\n",
    "\n",
    "## Precision\n",
    "\n",
    "Precision focuses on **False Positives**.  \n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- Out of all **predicted positives**, how many are actually positive?  \n",
    "\n",
    "---\n",
    "\n",
    "## Recall\n",
    "\n",
    "Recall focuses on **False Negatives**.  \n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- Out of all **actual positives**, how many are correctly predicted?  \n",
    "\n",
    "---\n",
    "\n",
    "## Precision vs Recall: Use Cases\n",
    "\n",
    "### 1. Spam Classification\n",
    "- If a mail is **not spam**, but predicted as **spam** â†’ **False Positive**  \n",
    "- This is a **big blunder** (important mails lost).  \n",
    "- **Focus:** Reduce False Positives â†’ Use **Precision**  \n",
    "\n",
    "### 2. Medical Diagnosis (e.g., Diabetes Detection)\n",
    "- If a person **has diabetes**, but model predicts **no diabetes** â†’ **False Negative**  \n",
    "- This is **dangerous** (disease missed).  \n",
    "- **Focus:** Reduce False Negatives â†’ Use **Recall**\n",
    "\n",
    "---\n",
    "\n",
    "## F-beta Score\n",
    "\n",
    "When **both FP and FN are important**, we use **F-beta score**:\n",
    "\n",
    "$$\n",
    "F_\\beta = \\frac{(1 + \\beta^2) \\cdot (\\text{Precision} \\cdot \\text{Recall})}{(\\beta^2 \\cdot \\text{Precision}) + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "Special cases:\n",
    "\n",
    "1. **F1 Score** (balanced case, FP = FN important):  \n",
    "   $\\beta = 1$\n",
    "\n",
    "   $$\n",
    "   F_1 = \\frac{2 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{\\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "   â†’ Harmonic mean of Precision and Recall  \n",
    "\n",
    "2. **F0.5 Score** (FP more important than FN):  \n",
    "   $\\beta = 0.5$\n",
    "\n",
    "   $$\n",
    "   F_{0.5} = \\frac{1.25 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{0.25 \\cdot \\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "3. **F2 Score** (FN more important than FP):  \n",
    "   $\\beta = 2$\n",
    "\n",
    "   $$\n",
    "   F_2 = \\frac{5 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{4 \\cdot \\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Accuracy** â†’ Use when dataset is **balanced**  \n",
    "- **Precision** â†’ Use when **False Positives** are critical (e.g., spam classification)  \n",
    "- **Recall** â†’ Use when **False Negatives** are critical (e.g., medical diagnosis)  \n",
    "- **F-beta Score** â†’ Use when **both FP and FN are important**  \n",
    "\n",
    "---\n",
    "\n",
    "> Next: We will discuss the **ROC Curve** and AUC as performance metrics. ðŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01abfef",
   "metadata": {},
   "source": [
    "# Performance Metrics in Classification\n",
    "\n",
    "In this video, we are going to discuss **performance metrics** which are specifically used in:\n",
    "\n",
    "- **Binary Classification**\n",
    "- **Multi-class Classification**\n",
    "\n",
    "We will cover:\n",
    "\n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n",
    "\n",
    "## Logistic Regression Recap\n",
    "\n",
    "Logistic regression is used for **classification problems**.  \n",
    "We separate categories using a **decision boundary (linear line)**.  \n",
    "\n",
    "Example:  \n",
    "- If a point lies above the line â†’ category 1  \n",
    "- If a point lies below the line â†’ category 0  \n",
    "\n",
    "To evaluate how the model is performing, we need **performance metrics**.  \n",
    "\n",
    "For regression problems, we used:  \n",
    "- $R^2$ score  \n",
    "- Adjusted $R^2$ score  \n",
    "\n",
    "For classification problems, we use:  \n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "The **confusion matrix** is the foundation of classification metrics.  \n",
    "For **binary classification**, it is a **2 Ã— 2 matrix**.\n",
    "\n",
    "|                | **Predicted 1** | **Predicted 0** |\n",
    "|----------------|-----------------|-----------------|\n",
    "| **Actual 1**   | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual 0**   | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "- **TP (True Positive):** Model predicts **1**, actual is **1**  \n",
    "- **TN (True Negative):** Model predicts **0**, actual is **0**  \n",
    "- **FP (False Positive):** Model predicts **1**, actual is **0** (Type I Error)  \n",
    "- **FN (False Negative):** Model predicts **0**, actual is **1** (Type II Error)  \n",
    "\n",
    "---\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "Accuracy measures the fraction of **correct predictions**:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "Example: If  \n",
    "- $TP = 3$, $TN = 1$, $FP = 2$, $FN = 1$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{3 + 1}{3 + 1 + 2 + 1} = \\frac{4}{7}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Problem with Accuracy: Imbalanced Dataset\n",
    "\n",
    "If dataset has **imbalanced categories**, accuracy may give a **misleading result**.\n",
    "\n",
    "Example:  \n",
    "- 1000 samples â†’ 900 = Class 1, 100 = Class 0  \n",
    "- If a model predicts **all as Class 1**, accuracy = $90\\%$  \n",
    "- But the model is **useless**, since it never predicts Class 0 correctly.  \n",
    "\n",
    "Thus, in **imbalanced datasets**, we use **Precision and Recall**.\n",
    "\n",
    "---\n",
    "\n",
    "## Precision\n",
    "\n",
    "Precision focuses on **False Positives**.  \n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- Out of all **predicted positives**, how many are actually positive?  \n",
    "\n",
    "---\n",
    "\n",
    "## Recall\n",
    "\n",
    "Recall focuses on **False Negatives**.  \n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- Out of all **actual positives**, how many are correctly predicted?  \n",
    "\n",
    "---\n",
    "\n",
    "## Precision vs Recall: Use Cases\n",
    "\n",
    "### 1. Spam Classification\n",
    "- If a mail is **not spam**, but predicted as **spam** â†’ **False Positive**  \n",
    "- This is a **big blunder** (important mails lost).  \n",
    "- **Focus:** Reduce False Positives â†’ Use **Precision**  \n",
    "\n",
    "### 2. Medical Diagnosis (e.g., Diabetes Detection)\n",
    "- If a person **has diabetes**, but model predicts **no diabetes** â†’ **False Negative**  \n",
    "- This is **dangerous** (disease missed).  \n",
    "- **Focus:** Reduce False Negatives â†’ Use **Recall**\n",
    "\n",
    "---\n",
    "\n",
    "## F-beta Score\n",
    "\n",
    "When **both FP and FN are important**, we use **F-beta score**:\n",
    "\n",
    "$$\n",
    "F_\\beta = \\frac{(1 + \\beta^2) \\cdot (\\text{Precision} \\cdot \\text{Recall})}{(\\beta^2 \\cdot \\text{Precision}) + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "Special cases:\n",
    "\n",
    "1. **F1 Score** (balanced case, FP = FN important):  \n",
    "   $\\beta = 1$\n",
    "\n",
    "   $$\n",
    "   F_1 = \\frac{2 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{\\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "   â†’ Harmonic mean of Precision and Recall  \n",
    "\n",
    "2. **F0.5 Score** (FP more important than FN):  \n",
    "   $\\beta = 0.5$\n",
    "\n",
    "   $$\n",
    "   F_{0.5} = \\frac{1.25 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{0.25 \\cdot \\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "3. **F2 Score** (FN more important than FP):  \n",
    "   $\\beta = 2$\n",
    "\n",
    "   $$\n",
    "   F_2 = \\frac{5 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{4 \\cdot \\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Accuracy** â†’ Use when dataset is **balanced**  \n",
    "- **Precision** â†’ Use when **False Positives** are critical (e.g., spam classification)  \n",
    "- **Recall** â†’ Use when **False Negatives** are critical (e.g., medical diagnosis)  \n",
    "- **F-beta Score** â†’ Use when **both FP and FN are important**  \n",
    "\n",
    "---\n",
    "\n",
    "> Next: We will discuss the **ROC Curve** and AUC as performance metrics. ðŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95991ec1",
   "metadata": {},
   "source": [
    "# Performance Metrics in Classification\n",
    "\n",
    "In this video, we are going to discuss **performance metrics** which are specifically used in:\n",
    "\n",
    "- **Binary Classification**\n",
    "- **Multi-class Classification**\n",
    "\n",
    "We will cover:\n",
    "\n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n",
    "\n",
    "## Logistic Regression Recap\n",
    "\n",
    "Logistic regression is used for **classification problems**.  \n",
    "We separate categories using a **decision boundary (linear line)**.  \n",
    "\n",
    "Example:  \n",
    "- If a point lies above the line â†’ category 1  \n",
    "- If a point lies below the line â†’ category 0  \n",
    "\n",
    "To evaluate how the model is performing, we need **performance metrics**.  \n",
    "\n",
    "For regression problems, we used:  \n",
    "- $R^2$ score  \n",
    "- Adjusted $R^2$ score  \n",
    "\n",
    "For classification problems, we use:  \n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "The **confusion matrix** is the foundation of classification metrics.  \n",
    "For **binary classification**, it is a **2 Ã— 2 matrix**.\n",
    "\n",
    "|                | **Predicted 1** | **Predicted 0** |\n",
    "|----------------|-----------------|-----------------|\n",
    "| **Actual 1**   | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual 0**   | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "- **TP (True Positive):** Model predicts **1**, actual is **1**  \n",
    "- **TN (True Negative):** Model predicts **0**, actual is **0**  \n",
    "- **FP (False Positive):** Model predicts **1**, actual is **0** (Type I Error)  \n",
    "- **FN (False Negative):** Model predicts **0**, actual is **1** (Type II Error)  \n",
    "\n",
    "---\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "Accuracy measures the fraction of **correct predictions**:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "Example: If  \n",
    "- $TP = 3$, $TN = 1$, $FP = 2$, $FN = 1$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{3 + 1}{3 + 1 + 2 + 1} = \\frac{4}{7}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Problem with Accuracy: Imbalanced Dataset\n",
    "\n",
    "If dataset has **imbalanced categories**, accuracy may give a **misleading result**.\n",
    "\n",
    "Example:  \n",
    "- 1000 samples â†’ 900 = Class 1, 100 = Class 0  \n",
    "- If a model predicts **all as Class 1**, accuracy = $90\\%$  \n",
    "- But the model is **useless**, since it never predicts Class 0 correctly.  \n",
    "\n",
    "Thus, in **imbalanced datasets**, we use **Precision and Recall**.\n",
    "\n",
    "---\n",
    "\n",
    "## Precision\n",
    "\n",
    "Precision focuses on **False Positives**.  \n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- Out of all **predicted positives**, how many are actually positive?  \n",
    "\n",
    "---\n",
    "\n",
    "## Recall\n",
    "\n",
    "Recall focuses on **False Negatives**.  \n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- Out of all **actual positives**, how many are correctly predicted?  \n",
    "\n",
    "---\n",
    "\n",
    "## Precision vs Recall: Use Cases\n",
    "\n",
    "### 1. Spam Classification\n",
    "- If a mail is **not spam**, but predicted as **spam** â†’ **False Positive**  \n",
    "- This is a **big blunder** (important mails lost).  \n",
    "- **Focus:** Reduce False Positives â†’ Use **Precision**  \n",
    "\n",
    "### 2. Medical Diagnosis (e.g., Diabetes Detection)\n",
    "- If a person **has diabetes**, but model predicts **no diabetes** â†’ **False Negative**  \n",
    "- This is **dangerous** (disease missed).  \n",
    "- **Focus:** Reduce False Negatives â†’ Use **Recall**\n",
    "\n",
    "---\n",
    "\n",
    "## F-beta Score\n",
    "\n",
    "When **both FP and FN are important**, we use **F-beta score**:\n",
    "\n",
    "$$\n",
    "F_\\beta = \\frac{(1 + \\beta^2) \\cdot (\\text{Precision} \\cdot \\text{Recall})}{(\\beta^2 \\cdot \\text{Precision}) + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "Special cases:\n",
    "\n",
    "1. **F1 Score** (balanced case, FP = FN important):  \n",
    "   $\\beta = 1$\n",
    "\n",
    "   $$\n",
    "   F_1 = \\frac{2 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{\\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "   â†’ Harmonic mean of Precision and Recall  \n",
    "\n",
    "2. **F0.5 Score** (FP more important than FN):  \n",
    "   $\\beta = 0.5$\n",
    "\n",
    "   $$\n",
    "   F_{0.5} = \\frac{1.25 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{0.25 \\cdot \\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "3. **F2 Score** (FN more important than FP):  \n",
    "   $\\beta = 2$\n",
    "\n",
    "   $$\n",
    "   F_2 = \\frac{5 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{4 \\cdot \\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Accuracy** â†’ Use when dataset is **balanced**  \n",
    "- **Precision** â†’ Use when **False Positives** are critical (e.g., spam classification)  \n",
    "- **Recall** â†’ Use when **False Negatives** are critical (e.g., medical diagnosis)  \n",
    "- **F-beta Score** â†’ Use when **both FP and FN are important**  \n",
    "\n",
    "---\n",
    "\n",
    "> Next: We will discuss the **ROC Curve** and AUC as performance metrics. ðŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e9eb69",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0384d785",
   "metadata": {},
   "source": [
    "# Performance Metrics in Classification\n",
    "\n",
    "In this video, we are going to discuss **performance metrics** which are specifically used in:\n",
    "\n",
    "- **Binary Classification**\n",
    "- **Multi-class Classification**\n",
    "\n",
    "We will cover:\n",
    "\n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n",
    "\n",
    "## Logistic Regression Recap\n",
    "\n",
    "Logistic regression is used for **classification problems**.  \n",
    "We separate categories using a **decision boundary (linear line)**.  \n",
    "\n",
    "Example:  \n",
    "- If a point lies above the line â†’ category 1  \n",
    "- If a point lies below the line â†’ category 0  \n",
    "\n",
    "To evaluate how the model is performing, we need **performance metrics**.  \n",
    "\n",
    "For regression problems, we used:  \n",
    "- $R^2$ score  \n",
    "- Adjusted $R^2$ score  \n",
    "\n",
    "For classification problems, we use:  \n",
    "- Confusion Matrix  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F-beta Score  \n",
    "\n",
    "---\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "The **confusion matrix** is the foundation of classification metrics.  \n",
    "For **binary classification**, it is a **2 Ã— 2 matrix**.\n",
    "\n",
    "|                | **Predicted 1** | **Predicted 0** |\n",
    "|----------------|-----------------|-----------------|\n",
    "| **Actual 1**   | True Positive (TP) | False Negative (FN) |\n",
    "| **Actual 0**   | False Positive (FP) | True Negative (TN) |\n",
    "\n",
    "- **TP (True Positive):** Model predicts **1**, actual is **1**  \n",
    "- **TN (True Negative):** Model predicts **0**, actual is **0**  \n",
    "- **FP (False Positive):** Model predicts **1**, actual is **0** (Type I Error)  \n",
    "- **FN (False Negative):** Model predicts **0**, actual is **1** (Type II Error)  \n",
    "\n",
    "---\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "Accuracy measures the fraction of **correct predictions**:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "Example: If  \n",
    "- $TP = 3$, $TN = 1$, $FP = 2$, $FN = 1$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{3 + 1}{3 + 1 + 2 + 1} = \\frac{4}{7}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Problem with Accuracy: Imbalanced Dataset\n",
    "\n",
    "If dataset has **imbalanced categories**, accuracy may give a **misleading result**.\n",
    "\n",
    "Example:  \n",
    "- 1000 samples â†’ 900 = Class 1, 100 = Class 0  \n",
    "- If a model predicts **all as Class 1**, accuracy = $90\\%$  \n",
    "- But the model is **useless**, since it never predicts Class 0 correctly.  \n",
    "\n",
    "Thus, in **imbalanced datasets**, we use **Precision and Recall**.\n",
    "\n",
    "---\n",
    "\n",
    "## Precision\n",
    "\n",
    "Precision focuses on **False Positives**.  \n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- Out of all **predicted positives**, how many are actually positive?  \n",
    "\n",
    "---\n",
    "\n",
    "## Recall\n",
    "\n",
    "Recall focuses on **False Negatives**.  \n",
    "\n",
    "Formula:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "- Out of all **actual positives**, how many are correctly predicted?  \n",
    "\n",
    "---\n",
    "\n",
    "## Precision vs Recall: Use Cases\n",
    "\n",
    "### 1. Spam Classification\n",
    "- If a mail is **not spam**, but predicted as **spam** â†’ **False Positive**  \n",
    "- This is a **big blunder** (important mails lost).  \n",
    "- **Focus:** Reduce False Positives â†’ Use **Precision**  \n",
    "\n",
    "### 2. Medical Diagnosis (e.g., Diabetes Detection)\n",
    "- If a person **has diabetes**, but model predicts **no diabetes** â†’ **False Negative**  \n",
    "- This is **dangerous** (disease missed).  \n",
    "- **Focus:** Reduce False Negatives â†’ Use **Recall**\n",
    "\n",
    "---\n",
    "\n",
    "## F-beta Score\n",
    "\n",
    "When **both FP and FN are important**, we use **F-beta score**:\n",
    "\n",
    "$$\n",
    "F_\\beta = \\frac{(1 + \\beta^2) \\cdot (\\text{Precision} \\cdot \\text{Recall})}{(\\beta^2 \\cdot \\text{Precision}) + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "Special cases:\n",
    "\n",
    "1. **F1 Score** (balanced case, FP = FN important):  \n",
    "   $\\beta = 1$\n",
    "\n",
    "   $$\n",
    "   F_1 = \\frac{2 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{\\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "   â†’ Harmonic mean of Precision and Recall  \n",
    "\n",
    "2. **F0.5 Score** (FP more important than FN):  \n",
    "   $\\beta = 0.5$\n",
    "\n",
    "   $$\n",
    "   F_{0.5} = \\frac{1.25 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{0.25 \\cdot \\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "3. **F2 Score** (FN more important than FP):  \n",
    "   $\\beta = 2$\n",
    "\n",
    "   $$\n",
    "   F_2 = \\frac{5 \\cdot (\\text{Precision} \\cdot \\text{Recall})}{4 \\cdot \\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Accuracy** â†’ Use when dataset is **balanced**  \n",
    "- **Precision** â†’ Use when **False Positives** are critical (e.g., spam classification)  \n",
    "- **Recall** â†’ Use when **False Negatives** are critical (e.g., medical diagnosis)  \n",
    "- **F-beta Score** â†’ Use when **both FP and FN are important**  \n",
    "\n",
    "---\n",
    "\n",
    "> Next: We will discuss the **ROC Curve** and AUC as performance metrics. ðŸš€\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bcaa5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
