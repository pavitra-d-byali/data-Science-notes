{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5abcfd30",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "---\n",
    "\n",
    "## 1. Recap: Linear Regression\n",
    "\n",
    "- **Simple Linear Regression** (one independent feature, one dependent feature):\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\beta_0 + \\beta_1 x\n",
    "$$\n",
    "\n",
    "- **Multiple Linear Regression** (more than one independent feature):\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n",
    "$$\n",
    "\n",
    "- For multiple dimensions:\n",
    "  - **2D:** best fit line  \n",
    "  - **3D:** best fit plane  \n",
    "  - **Higher dimensions:** hyperplane  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Problem: Non-linear Relationships\n",
    "\n",
    "Suppose our dataset does not follow a linear relationship. Example:\n",
    "\n",
    "- Input feature: $x$  \n",
    "- Output feature: $y$  \n",
    "- Data points follow a **curve**, not a straight line.\n",
    "\n",
    "If we try to fit a **linear regression line**, the **error will be high** because the linear model cannot capture the curvature.\n",
    "\n",
    "Thus, we need a method to model **non-linear relationships**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Solution: Polynomial Regression\n",
    "\n",
    "Polynomial Regression allows us to extend linear regression by including **polynomial terms of the independent variable(s)**.\n",
    "\n",
    "### Key Concept: Polynomial Degree\n",
    "The degree of the polynomial determines the complexity of the model.  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Polynomial Regression with One Feature\n",
    "\n",
    "Let’s denote:\n",
    "\n",
    "- $x$: input feature  \n",
    "- $y$: dependent feature  \n",
    "\n",
    "### Case 1: Degree = 0\n",
    "Equation:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\beta_0\n",
    "$$\n",
    "\n",
    "This is just a **constant line** (no dependence on $x$).\n",
    "\n",
    "---\n",
    "\n",
    "### Case 2: Degree = 1\n",
    "Equation:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\beta_0 + \\beta_1 x\n",
    "$$\n",
    "\n",
    "This is simply **Simple Linear Regression**.\n",
    "\n",
    "---\n",
    "\n",
    "### Case 3: Degree = 2\n",
    "Equation:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2\n",
    "$$\n",
    "\n",
    "Now the model can capture **quadratic relationships**.\n",
    "\n",
    "---\n",
    "\n",
    "### Case 4: Degree = $n$\n",
    "General equation:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\dots + \\beta_n x^n\n",
    "$$\n",
    "\n",
    "This allows the model to capture higher-order relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Visual Intuition\n",
    "\n",
    "- **Degree = 1 (Linear):** Best fit line  \n",
    "- **Degree = 2, 3, ... (Higher):** Curves that can fit the data better  \n",
    "- **Very high degree (e.g., 15):** The curve may pass through almost all points → **Overfitting**\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Trade-off: Underfitting vs Overfitting\n",
    "\n",
    "- **Low Degree (e.g., 1):** Model is too simple → **Underfitting**  \n",
    "- **Very High Degree:** Model fits noise as well → **Overfitting**  \n",
    "- **Optimal Degree:** Balance between bias and variance → **Generalization**\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Polynomial Regression with Multiple Features\n",
    "\n",
    "Suppose we have two independent features $x_1$ and $x_2$.\n",
    "\n",
    "### Case 1: Degree = 1\n",
    "This is just **Multiple Linear Regression**:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Case 2: Degree = 2\n",
    "Equation becomes:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_2^2 + \\beta_5 (x_1 x_2)\n",
    "$$\n",
    "\n",
    "Here we include:\n",
    "- **Squared terms** of features\n",
    "- **Interaction terms** between features\n",
    "\n",
    "---\n",
    "\n",
    "### Case 3: Degree = 3\n",
    "Equation extends further:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1^2 + \\beta_4 x_2^2 + \\beta_5 (x_1 x_2) + \\beta_6 x_1^3 + \\beta_7 x_2^3 + \\dots\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 8. General Formula (Multiple Features)\n",
    "\n",
    "For features $x_1, x_2, \\dots, x_m$ and polynomial degree $d$:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\beta_0 + \\sum_{i=1}^{m} \\sum_{j=1}^{d} \\beta_{ij} (x_i^j) + \\text{interaction terms}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Why Polynomial Regression?\n",
    "\n",
    "- Captures **non-linear relationships** between features and target  \n",
    "- Still solved as a **linear regression problem** (linear in parameters $\\beta$) after feature transformation  \n",
    "- Powerful but needs careful **degree selection** to avoid overfitting  \n",
    "\n",
    "---\n",
    "\n",
    "## 10. Summary\n",
    "\n",
    "- **Degree = 1:** Simple/Multiple Linear Regression  \n",
    "- **Degree = 2 or more:** Polynomial Regression  \n",
    "- **Higher degree = more flexibility**, but risk of **overfitting**  \n",
    "- Use techniques like **cross-validation** to select the best polynomial degree  \n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b9409a",
   "metadata": {},
   "source": [
    "````markdown\n",
    "# Polynomial Regression in Machine Learning\n",
    "\n",
    "In this notebook, we will implement **Polynomial Regression** to handle a dataset with a non-linear relationship.  \n",
    "We will compare it with **Simple Linear Regression** and show how polynomial features improve accuracy and visualization.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Import Required Libraries\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Create a Synthetic Non-Linear Dataset\n",
    "\n",
    "We’ll generate data following a quadratic equation:\n",
    "\n",
    "[\n",
    "y = 0.5x^2 + 1.5x + 2 + \\epsilon\n",
    "]\n",
    "\n",
    "where (\\epsilon) represents random noise/outliers.\n",
    "\n",
    "```python\n",
    "np.random.seed(42)\n",
    "\n",
    "# Independent variable\n",
    "X = 6 * np.random.rand(100, 1) - 3  # Range: [-3, 3]\n",
    "\n",
    "# Dependent variable with quadratic relationship + noise\n",
    "y = 0.5 * (X ** 2) + 1.5 * X + 2 + np.random.randn(100, 1)\n",
    "\n",
    "# Visualize dataset\n",
    "plt.scatter(X, y, color=\"green\")\n",
    "plt.xlabel(\"X Feature\")\n",
    "plt.ylabel(\"Target (y)\")\n",
    "plt.title(\"Non-linear Dataset\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Train-Test Split\n",
    "\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Apply **Simple Linear Regression** First\n",
    "\n",
    "```python\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions & Score\n",
    "y_pred_linear = lin_reg.predict(X_test)\n",
    "score_linear = r2_score(y_test, y_pred_linear)\n",
    "\n",
    "print(\"Simple Linear Regression R² Score:\", score_linear)\n",
    "\n",
    "# Visualization\n",
    "plt.scatter(X_train, y_train, color=\"blue\", label=\"Training data\")\n",
    "plt.plot(X_train, lin_reg.predict(X_train), color=\"red\", linewidth=2, label=\"Best Fit Line\")\n",
    "plt.xlabel(\"X Feature\")\n",
    "plt.ylabel(\"Target (y)\")\n",
    "plt.title(\"Simple Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Apply **Polynomial Regression (Degree = 2)**\n",
    "\n",
    "```python\n",
    "poly = PolynomialFeatures(degree=2, include_bias=True)\n",
    "\n",
    "# Transform features\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "# Train regression on transformed features\n",
    "poly_reg = LinearRegression()\n",
    "poly_reg.fit(X_train_poly, y_train)\n",
    "\n",
    "# Predictions & Score\n",
    "y_pred_poly = poly_reg.predict(X_test_poly)\n",
    "score_poly = r2_score(y_test, y_pred_poly)\n",
    "\n",
    "print(\"Polynomial Regression (Degree=2) R² Score:\", score_poly)\n",
    "\n",
    "# Visualization\n",
    "plt.scatter(X_train, y_train, color=\"blue\", label=\"Training data\")\n",
    "\n",
    "# Sort values for smooth curve\n",
    "X_sorted = np.linspace(X.min(), X.max(), 200).reshape(-1, 1)\n",
    "X_sorted_poly = poly.transform(X_sorted)\n",
    "y_sorted_poly = poly_reg.predict(X_sorted_poly)\n",
    "\n",
    "plt.plot(X_sorted, y_sorted_poly, color=\"red\", linewidth=2, label=\"Best Fit Curve\")\n",
    "plt.xlabel(\"X Feature\")\n",
    "plt.ylabel(\"Target (y)\")\n",
    "plt.title(\"Polynomial Regression (Degree=2)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Experiment with **Higher Degree (Degree = 3)**\n",
    "\n",
    "```python\n",
    "poly3 = PolynomialFeatures(degree=3, include_bias=True)\n",
    "\n",
    "X_train_poly3 = poly3.fit_transform(X_train)\n",
    "X_test_poly3 = poly3.transform(X_test)\n",
    "\n",
    "poly_reg3 = LinearRegression()\n",
    "poly_reg3.fit(X_train_poly3, y_train)\n",
    "\n",
    "y_pred_poly3 = poly_reg3.predict(X_test_poly3)\n",
    "score_poly3 = r2_score(y_test, y_pred_poly3)\n",
    "\n",
    "print(\"Polynomial Regression (Degree=3) R² Score:\", score_poly3)\n",
    "\n",
    "# Visualization\n",
    "plt.scatter(X_train, y_train, color=\"blue\", label=\"Training data\")\n",
    "\n",
    "X_sorted_poly3 = poly3.transform(X_sorted)\n",
    "y_sorted_poly3 = poly_reg3.predict(X_sorted_poly3)\n",
    "\n",
    "plt.plot(X_sorted, y_sorted_poly3, color=\"purple\", linewidth=2, label=\"Best Fit Curve (Degree=3)\")\n",
    "plt.xlabel(\"X Feature\")\n",
    "plt.ylabel(\"Target (y)\")\n",
    "plt.title(\"Polynomial Regression (Degree=3)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 7: Predictions on New Data\n",
    "\n",
    "```python\n",
    "# Generate new unseen data\n",
    "X_new = np.linspace(-3, 3, 200).reshape(-1, 1)\n",
    "X_new_poly = poly.transform(X_new)\n",
    "y_new_pred = poly_reg.predict(X_new_poly)\n",
    "\n",
    "# Plot predictions\n",
    "plt.scatter(X, y, color=\"green\", label=\"Original Data\")\n",
    "plt.plot(X_new, y_new_pred, color=\"red\", linewidth=2, label=\"Polynomial Prediction\")\n",
    "plt.xlabel(\"X Feature\")\n",
    "plt.ylabel(\"Target (y)\")\n",
    "plt.title(\"Prediction on New Data\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 8: Compare Models\n",
    "\n",
    "| Model                         | R² Score |\n",
    "| ----------------------------- | -------- |\n",
    "| Simple Linear Regression      | ~0.62    |\n",
    "| Polynomial Regression (Deg=2) | ~0.93    |\n",
    "| Polynomial Regression (Deg=3) | ~0.93    |\n",
    "\n",
    "* **Linear Regression** fails to capture the non-linear trend.\n",
    "* **Polynomial Regression (Deg=2)** fits the curve well with high accuracy.\n",
    "* Increasing degree further (Deg=3, 4, …) may lead to **overfitting** if not carefully tuned.\n",
    "\n",
    "---\n",
    "\n",
    "# ✅ Conclusion\n",
    "\n",
    "Polynomial Regression is a powerful extension of Linear Regression that introduces polynomial features, enabling the model to capture non-linear relationships effectively. Choosing the right polynomial degree is crucial to balance **bias vs. variance** and avoid **underfitting/overfitting**.\n",
    "\n",
    "```\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd7f03",
   "metadata": {},
   "source": [
    "````markdown\n",
    "# Polynomial Regression with Pipelines\n",
    "\n",
    "In the previous notebook, we implemented **Polynomial Regression** by manually transforming features and applying linear regression.  \n",
    "Now, we introduce the **Pipeline** concept in Scikit-Learn.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What is a Pipeline?\n",
    "\n",
    "- A **Pipeline** allows us to chain multiple steps (transformations + model) into a single object.  \n",
    "- Each step is executed in sequence:\n",
    "  1. **PolynomialFeatures** → generates polynomial features from input data.  \n",
    "  2. **LinearRegression** → fits regression on transformed features.  \n",
    "\n",
    "Thus, we can easily try different polynomial degrees **without manually transforming data** each time.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Import Libraries\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "````\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Generate Non-linear Dataset\n",
    "\n",
    "```python\n",
    "np.random.seed(42)\n",
    "\n",
    "X = 6 * np.random.rand(100, 1) - 3  # Range [-3, 3]\n",
    "y = 0.5 * X**2 + 1.5 * X + 2 + np.random.randn(100, 1)\n",
    "\n",
    "plt.scatter(X, y, color=\"green\", label=\"Dataset\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Non-linear Dataset\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Create a Generic Polynomial Regression Function\n",
    "\n",
    "```python\n",
    "def polynomial_regression(degree):\n",
    "    # Step 1: Define pipeline with PolynomialFeatures + LinearRegression\n",
    "    model = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=degree, include_bias=True)),\n",
    "        (\"lin_reg\", LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    # Step 2: Fit model\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Step 3: Predict on new values\n",
    "    X_new = np.linspace(-3, 3, 200).reshape(-1, 1)\n",
    "    y_new = model.predict(X_new)\n",
    "    \n",
    "    # Step 4: Visualization\n",
    "    plt.scatter(X, y, color=\"blue\", label=\"Training data\")\n",
    "    plt.plot(X_new, y_new, color=\"red\", linewidth=2, label=f\"Degree {degree}\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(f\"Polynomial Regression with Pipeline (Degree={degree})\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Test with Different Degrees\n",
    "\n",
    "### Case 1: Degree = 1 (Simple Linear Regression)\n",
    "\n",
    "```python\n",
    "model_deg1 = polynomial_regression(1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Case 2: Degree = 2\n",
    "\n",
    "```python\n",
    "model_deg2 = polynomial_regression(2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Case 3: Degree = 3\n",
    "\n",
    "```python\n",
    "model_deg3 = polynomial_regression(3)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Case 4: Degree = 4\n",
    "\n",
    "```python\n",
    "model_deg4 = polynomial_regression(4)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Case 5: Degree = 10 (Overfitting Example)\n",
    "\n",
    "```python\n",
    "model_deg10 = polynomial_regression(10)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Observations\n",
    "\n",
    "* **Degree = 1:** Fits only a straight line → High error.\n",
    "* **Degree = 2 / 3 / 4:** Captures curve properly → Good fit.\n",
    "* **Degree = 10:** Overfits → Curve passes through almost all points, fits noise too.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Conclusion\n",
    "\n",
    "* Using **Pipelines**, we can combine `PolynomialFeatures` and `LinearRegression` into one workflow.\n",
    "* This avoids manual transformations and makes experimentation with polynomial degrees easier.\n",
    "* **Too low degree → underfitting**\n",
    "* **Too high degree → overfitting**\n",
    "* An optimal degree (e.g., 3–6) balances generalization.\n",
    "\n",
    "```\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d506430",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f892c8c5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
