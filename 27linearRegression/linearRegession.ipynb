{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9525aa53",
   "metadata": {},
   "source": [
    "# Simple Linear Regression\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Problem Statement\n",
    "\n",
    "**Simple Linear Regression** is used to solve **regression problems** in supervised machine learning.\n",
    "\n",
    "- Example dataset:  \n",
    "  Features: `Weight`  \n",
    "  Target: `Height`  \n",
    "\n",
    "| Weight (kg) | Height (cm) |\n",
    "|-------------|-------------|\n",
    "| 74          | 170         |\n",
    "| 80          | 180         |\n",
    "| 75          | 175.5       |\n",
    "\n",
    "**Goal:**  \n",
    "Train a model such that, for a **new weight**, it can predict the corresponding **height**.\n",
    "\n",
    "- `Weight` → Independent Feature (Input)  \n",
    "- `Height` → Dependent Feature (Output)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Why \"Simple\" Linear Regression?\n",
    "\n",
    "- **Simple Linear Regression:** 1 input feature + 1 output feature  \n",
    "- **Multiple Linear Regression:** Multiple input features + 1 output feature  \n",
    "\n",
    "Learning simple linear regression first helps understand the **terminology and mathematics**, which also applies to multiple linear regression.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Geometric Interpretation\n",
    "\n",
    "- Plot the data points on a graph: `Weight` (x-axis) vs `Height` (y-axis)\n",
    "- Aim: Create a **best fit line** through the points\n",
    "- Purpose: Predict the height for a **new weight**\n",
    "\n",
    "**Prediction process:**\n",
    "\n",
    "1. Draw the **best fit line** that minimizes the distance between the **true points** (actual heights) and **predicted points** (points on the line).  \n",
    "   - These distances represent the **errors**.\n",
    "2. Minimize the **sum of all errors** to find the optimal line.\n",
    "3. For a new weight:\n",
    "   - Locate the weight on the x-axis\n",
    "   - See where it intersects the best fit line\n",
    "   - Read the predicted height on the y-axis\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Summary\n",
    "\n",
    "- **Simple Linear Regression** predicts a dependent variable (height) using a single independent variable (weight).\n",
    "- The **best fit line** is chosen by minimizing the **total prediction error**.\n",
    "- Once trained, the model can predict the output for **new inputs** accurately.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50556f0e",
   "metadata": {},
   "source": [
    "# Simple Linear Regression: Notations and Concepts\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setting Up the Data\n",
    "\n",
    "- **X-axis:** Weight (independent feature)  \n",
    "- **Y-axis:** Height (dependent feature)  \n",
    "- **Data points:** Represent our dataset\n",
    "\n",
    "**Goal:** Create the **best fit line** to predict height for new weight values.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Equation of the Best Fit Line\n",
    "\n",
    "The equation can be written in different forms:\n",
    "\n",
    "1. Standard straight line:  \n",
    "   $$\n",
    "   y = mx + c\n",
    "   $$\n",
    "\n",
    "2. Regression notation:  \n",
    "   $$\n",
    "   y = \\beta_0 + \\beta_1 x\n",
    "   $$\n",
    "\n",
    "3. Andrew Ng’s notation (used here):  \n",
    "   $$\n",
    "   h_\\theta(x) = \\theta_0 + \\theta_1 x\n",
    "   $$\n",
    "\n",
    "- $x$ → Independent feature (Weight)  \n",
    "- $h_\\theta(x)$ → Predicted value (Height)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Parameters of the Line\n",
    "\n",
    "### 3.1 Intercept ($\\theta_0$)\n",
    "- Also called **bias term**  \n",
    "- Represents where the line meets the **y-axis** when $x = 0$  \n",
    "- Determines the **baseline value** of the predicted output\n",
    "\n",
    "### 3.2 Slope / Coefficient ($\\theta_1$)\n",
    "- Represents the **rate of change** in $y$ for a unit change in $x$  \n",
    "- Determines how steep the line is  \n",
    "\n",
    "> For multiple features:  \n",
    "> $$\n",
    "> h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\dots + \\theta_n x_n\n",
    "> $$  \n",
    "> Each feature has its own slope ($\\theta_i$)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Predictions\n",
    "\n",
    "- A **new data point** is projected onto the **best fit line**  \n",
    "- The **predicted output** is the corresponding y-value:  \n",
    "  $$\n",
    "  \\hat{y} = h_\\theta(x)\n",
    "  $$  \n",
    "- $\\hat{y}$ represents the predicted point on the line\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Error\n",
    "\n",
    "- **Error** measures the difference between the **true output** and the **predicted output**:  \n",
    "  $$\n",
    "  \\text{Error} = y - \\hat{y}\n",
    "  $$  \n",
    "- Goal: Minimize the **sum of squared errors** for all data points  \n",
    "- The **best fit line** is the one that **minimizes the total error**  \n",
    "\n",
    "---\n",
    "\n",
    "## 6. Cost Function (Mean Squared Error)\n",
    "\n",
    "To formalize error minimization, we define the **cost function**:\n",
    "\n",
    "$$\n",
    "J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} \\big( h_\\theta(x_i) - y_i \\big)^2\n",
    "$$\n",
    "\n",
    "- $m$ → Number of data points  \n",
    "- $h_\\theta(x_i)$ → Predicted value  \n",
    "- $y_i$ → Actual value  \n",
    "\n",
    "**Goal:** Minimize $J(\\theta_0, \\theta_1)$\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Gradient Descent: Optimization Concept\n",
    "\n",
    "Instead of randomly trying lines:\n",
    "\n",
    "1. Initialize $\\theta_0$ and $\\theta_1$  \n",
    "2. Compute the cost function $J(\\theta_0, \\theta_1)$  \n",
    "3. Compute **partial derivatives** w.r.t each parameter  \n",
    "4. Update parameters iteratively:\n",
    "\n",
    "### Update Rules\n",
    "\n",
    "- **Intercept ($\\theta_0$):**\n",
    "$$\n",
    "\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\big( h_\\theta(x_i) - y_i \\big)\n",
    "$$\n",
    "\n",
    "- **Slope ($\\theta_1$):**\n",
    "$$\n",
    "\\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\big( h_\\theta(x_i) - y_i \\big) x_i\n",
    "$$\n",
    "\n",
    "- $\\alpha$ → **Learning rate**, controls step size  \n",
    "\n",
    "Repeat until convergence (global minimum reached).\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Key Takeaways\n",
    "\n",
    "- $\\theta_0$ → Intercept / Bias term  \n",
    "- $\\theta_1$ → Slope / Coefficient  \n",
    "- $h_\\theta(x)$ → Predicted value ($\\hat{y}$)  \n",
    "- **Error** → Difference between actual and predicted values  \n",
    "- **Cost function** → Measures total squared error  \n",
    "- **Gradient descent** → Iteratively adjusts parameters to minimize cost  \n",
    "- **Learning rate ($\\alpha$)** → Controls convergence speed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764812b9",
   "metadata": {},
   "source": [
    "# Simple Linear Regression: Cost Function & Gradient Descent\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Cost Function\n",
    "\n",
    "To find the optimal line, we define a **cost function** \\(J(\\theta_0, \\theta_1)\\) as:\n",
    "\n",
    "\\[\n",
    "J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} \\Big(h_\\theta(x^{(i)}) - y^{(i)}\\Big)^2\n",
    "\\]\n",
    "\n",
    "Where:  \n",
    "- \\(h_\\theta(x^{(i)})\\) → Predicted value for the \\(i^{th}\\) point  \n",
    "- \\(y^{(i)}\\) → True value for the \\(i^{th}\\) point  \n",
    "- \\(m\\) → Total number of points  \n",
    "- Squared error is used to **penalize larger errors more heavily**  \n",
    "\n",
    "This is also known as the **Mean Squared Error (MSE)**.\n",
    "\n",
    "> Other cost functions exist, such as Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE), but MSE is widely used for linear regression.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Our Aim\n",
    "\n",
    "- **Goal:** Minimize the cost function by adjusting \\(\\theta_0\\) (intercept) and \\(\\theta_1\\) (slope).  \n",
    "- When \\(J(\\theta_0, \\theta_1)\\) is minimal, we have found the **best fit line**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Simplifying for Visualization\n",
    "\n",
    "- Assume \\(\\theta_0 = 0\\) (line passes through origin)  \n",
    "- Equation becomes:  \n",
    "\\[\n",
    "h_\\theta(x) = \\theta_1 x\n",
    "\\]\n",
    "\n",
    "- Example dataset:\n",
    "\n",
    "| x | y |\n",
    "|---|---|\n",
    "| 1 | 1 |\n",
    "| 2 | 2 |\n",
    "| 3 | 3 |\n",
    "\n",
    "- Initialize \\(\\theta_1 = 1\\), then predictions exactly match true points:  \n",
    "\n",
    "\\[\n",
    "h_\\theta(1) = 1, \\quad h_\\theta(2) = 2, \\quad h_\\theta(3) = 3\n",
    "\\]\n",
    "\n",
    "- Cost function:  \n",
    "\\[\n",
    "J(\\theta_1) = \\frac{1}{2 \\cdot 3} \\big((1-1)^2 + (2-2)^2 + (3-3)^2\\big) = 0\n",
    "\\]\n",
    "\n",
    "✅ Perfect fit, cost function = 0\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Changing \\(\\theta_1\\)\n",
    "\n",
    "- Example: \\(\\theta_1 = 0.5\\)  \n",
    "\n",
    "Predictions:\n",
    "\n",
    "| x | \\(h_\\theta(x)\\) | y | Error |\n",
    "|---|----------------|---|-------|\n",
    "| 1 | 0.5            | 1 | 0.5   |\n",
    "| 2 | 1              | 2 | 1     |\n",
    "| 3 | 1.5            | 3 | 1.5   |\n",
    "\n",
    "- Cost function:\n",
    "\n",
    "\\[\n",
    "J(0.5) = \\frac{1}{2 \\cdot 3} \\big((0.5)^2 + (1)^2 + (1.5)^2\\big) \\approx 0.58\n",
    "\\]\n",
    "\n",
    "- Another example: \\(\\theta_1 = 0\\)  \n",
    "\n",
    "Predictions:\n",
    "\n",
    "| x | \\(h_\\theta(x)\\) | y | Error |\n",
    "|---|----------------|---|-------|\n",
    "| 1 | 0              | 1 | 1     |\n",
    "| 2 | 0              | 2 | 2     |\n",
    "| 3 | 0              | 3 | 3     |\n",
    "\n",
    "- Cost function:\n",
    "\n",
    "\\[\n",
    "J(0) = \\frac{1}{2 \\cdot 3} \\big(1^2 + 2^2 + 3^2\\big) \\approx 2.33\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Gradient Descent & Global Minimum\n",
    "\n",
    "- Plotting \\(J(\\theta_1)\\) vs \\(\\theta_1\\) gives a **U-shaped curve**  \n",
    "- The **minimum point** of the curve corresponds to the **best fit line** (global minimum)  \n",
    "- Goal: Adjust \\(\\theta_1\\) and \\(\\theta_0\\) iteratively to reach the **global minimum**  \n",
    "- This iterative optimization process is called **gradient descent**  \n",
    "\n",
    "> Gradient descent is also widely used in **deep learning** to optimize weights of neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Key Takeaways:**\n",
    "\n",
    "1. **Cost Function:** Measures error between predicted and true values (MSE).  \n",
    "2. **Goal:** Minimize the cost function to find the best fit line.  \n",
    "3. **Gradient Descent:** Iteratively updates \\(\\theta_0\\) and \\(\\theta_1\\) to reach global minimum.  \n",
    "4. **Predicted Points:** \\(h_\\theta(x) = \\hat{y}\\)  \n",
    "5. **Error:** \\(y - \\hat{y}\\)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c01a8",
   "metadata": {},
   "source": [
    "# Simple Linear Regression: Convergence Algorithm (Gradient Descent)\n",
    "\n",
    "We discuss the **convergence algorithm**, an **optimized way to reach the global minimum** of the cost function using **gradient descent**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Cost Function\n",
    "\n",
    "For $m$ training examples $(x_i, y_i)$, the cost function is:\n",
    "\n",
    "$$\n",
    "J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} \\big( h_\\theta(x_i) - y_i \\big)^2\n",
    "$$\n",
    "\n",
    "Where the hypothesis is:\n",
    "\n",
    "$$\n",
    "h_\\theta(x_i) = \\theta_0 + \\theta_1 x_i\n",
    "$$\n",
    "\n",
    "- $\\theta_0$ → Intercept  \n",
    "- $\\theta_1$ → Slope\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Gradient Descent Update Rules\n",
    "\n",
    "The **general update formula** for any parameter $\\theta_j$ is:\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta_0, \\theta_1)}{\\partial \\theta_j}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $j = 0$ or $1$  \n",
    "- $\\alpha$ → **Learning rate**, controls step size  \n",
    "- $\\frac{\\partial J}{\\partial \\theta_j}$ → Derivative of the cost function w.r.t $\\theta_j$  \n",
    "\n",
    "**Goal:** Repeat until convergence (global minimum is reached).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Partial Derivatives\n",
    "\n",
    "### a) Derivative w.r.t $\\theta_0$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^{m} \\big( h_\\theta(x_i) - y_i \\big)\n",
    "$$\n",
    "\n",
    "**Explanation:**  \n",
    "- θ₀ is multiplied by 1 in the hypothesis.  \n",
    "- The derivative sums the errors and averages them over $m$ examples.\n",
    "\n",
    "### b) Derivative w.r.t $\\theta_1$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial \\theta_1} = \\frac{1}{m} \\sum_{i=1}^{m} \\big( h_\\theta(x_i) - y_i \\big) x_i\n",
    "$$\n",
    "\n",
    "**Explanation:**  \n",
    "- θ₁ is multiplied by $x_i$ in the hypothesis.  \n",
    "- The derivative sums the **error multiplied by $x_i$** and averages over $m$ examples.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Gradient Descent Updates (Explicit)\n",
    "\n",
    "**Update θ₀:**\n",
    "\n",
    "$$\n",
    "\\theta_0 := \\theta_0 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\big( h_\\theta(x_i) - y_i \\big)\n",
    "$$\n",
    "\n",
    "**Update θ₁:**\n",
    "\n",
    "$$\n",
    "\\theta_1 := \\theta_1 - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\big( h_\\theta(x_i) - y_i \\big) x_i\n",
    "$$\n",
    "\n",
    "- Repeat until the **cost function is minimized**.  \n",
    "- Both parameters converge **simultaneously**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Understanding the Slope (Derivative)\n",
    "\n",
    "- **Slope indicates direction:**\n",
    "  - **Negative slope:** increase θ  \n",
    "  - **Positive slope:** decrease θ  \n",
    "- **Tangent line:** Use the slope at the current point to decide the update direction.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Learning Rate (α)\n",
    "\n",
    "- Controls **speed of convergence**.  \n",
    "- **Too small:** slow convergence.  \n",
    "- **Too large:** may overshoot or fail to converge.  \n",
    "- Typical value: $\\alpha = 0.001$ for linear regression.  \n",
    "\n",
    "---\n",
    "\n",
    "## 7. 3D Gradient Descent Perspective\n",
    "\n",
    "- With θ₀ and θ₁ both varying, the **cost function surface is 3D**.  \n",
    "- Gradient descent moves iteratively **down the slope** toward the **global minimum**.  \n",
    "- Think of it as **descending an inverted mountain** to reach the **lowest point**.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Step-by-Step Algorithm\n",
    "\n",
    "1. Initialize θ₀ and θ₁ (e.g., zero or random).  \n",
    "2. Compute the cost function $J(\\theta_0, \\theta_1)$.  \n",
    "3. Compute partial derivatives w.r.t each θ.  \n",
    "4. Update θ₀ and θ₁ using the formulas above.  \n",
    "5. Repeat until convergence (minimal cost).  \n",
    "6. Final θ₀ and θ₁ define the **best fit line**.\n",
    "\n",
    "---\n",
    "\n",
    "✅ **Key Takeaways**\n",
    "\n",
    "- **Convergence algorithm** updates parameters systematically.  \n",
    "- **Gradient descent** iteratively moves parameters toward the global minimum.  \n",
    "- **Learning rate α** controls convergence speed.  \n",
    "- Once converged, θ₀ and θ₁ define the **best fit line**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220daff3",
   "metadata": {},
   "source": [
    "# Simple Linear Regression: Convergence Algorithm with θ₀ and θ₁\n",
    "\n",
    "We extend gradient descent to **both parameters**:\n",
    "\n",
    "- θ₀ → Intercept  \n",
    "- θ₁ → Slope  \n",
    "\n",
    "The goal is to **converge to the global minimum** of the cost function $J(θ₀, θ₁)$.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Cost Function\n",
    "\n",
    "For $m$ training examples $(x_i, y_i)$:\n",
    "\n",
    "$$\n",
    "J(θ₀, θ₁) = \\frac{1}{2m} \\sum_{i=1}^{m} \\big( h_θ(x_i) - y_i \\big)^2\n",
    "$$\n",
    "\n",
    "Where the hypothesis is:\n",
    "\n",
    "$$\n",
    "h_θ(x_i) = θ₀ + θ₁ x_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Gradient Descent Update Rules\n",
    "\n",
    "The **general convergence algorithm**:\n",
    "\n",
    "$$\n",
    "\\text{Repeat until convergence:} \\quad \n",
    "θ_j := θ_j - α \\frac{\\partial J(θ₀, θ₁)}{\\partial θ_j}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $α$ → Learning rate  \n",
    "- $j = 0, 1$ → Corresponds to θ₀ and θ₁\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Partial Derivatives\n",
    "\n",
    "### a) Derivative with respect to θ₀:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(θ₀, θ₁)}{\\partial θ₀} = \\frac{1}{m} \\sum_{i=1}^{m} \\big( h_θ(x_i) - y_i \\big)\n",
    "$$\n",
    "\n",
    "**Explanation:**  \n",
    "- θ₀ is multiplied by 1 in the hypothesis, so derivative is simply the sum of errors divided by $m$.\n",
    "\n",
    "### b) Derivative with respect to θ₁:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(θ₀, θ₁)}{\\partial θ₁} = \\frac{1}{m} \\sum_{i=1}^{m} \\big( h_θ(x_i) - y_i \\big) x_i\n",
    "$$\n",
    "\n",
    "**Explanation:**  \n",
    "- θ₁ is multiplied by $x_i$, so derivative is sum of **errors multiplied by $x_i$** divided by $m$.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Final Gradient Descent Updates\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "θ₀ := θ₀ - α \\frac{1}{m} \\sum_{i=1}^{m} \\big( h_θ(x_i) - y_i \\big)\n",
    "}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "θ₁ := θ₁ - α \\frac{1}{m} \\sum_{i=1}^{m} \\big( h_θ(x_i) - y_i \\big) x_i\n",
    "}\n",
    "$$\n",
    "\n",
    "- **Repeat** these updates until the **global minimum** is reached.  \n",
    "- This ensures that θ₀ and θ₁ **converge simultaneously**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 3D Gradient Descent Perspective\n",
    "\n",
    "- When both θ₀ and θ₁ are variables, the **cost function surface is a 3D curve**.  \n",
    "- Gradient descent moves iteratively **down the slope** of this surface toward the **global minimum**.  \n",
    "- Think of it like descending an **inverted mountain** to reach the **lowest point**.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary\n",
    "\n",
    "1. Start with initial values of θ₀ and θ₁ (often zero or random).  \n",
    "2. Compute the cost function $J(θ₀, θ₁)$.  \n",
    "3. Update θ₀ and θ₁ using gradient descent formulas.  \n",
    "4. Repeat until **convergence** (minimal cost).  \n",
    "5. Final θ₀ and θ₁ define the **best fit line**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a409ca47",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression: Introduction and Concepts\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Recap: Simple Linear Regression\n",
    "\n",
    "- **Single input feature example:** Predict height based on weight  \n",
    "- **Equation:**  \n",
    "  $$\n",
    "  h_\\theta(x) = \\theta_0 + \\theta_1 x\n",
    "  $$\n",
    "  - $\\theta_0$ → Intercept  \n",
    "  - $\\theta_1$ → Slope / Coefficient  \n",
    "- **Goal:** Update $\\theta_0$ and $\\theta_1$ using **gradient descent** to minimize cost function $J(\\theta)$  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Multiple Input Features\n",
    "\n",
    "- **Example dataset:** House Pricing Dataset  \n",
    "  Features:  \n",
    "  1. Number of rooms → $x_1$  \n",
    "  2. Size of the house → $x_2$  \n",
    "  3. Location → $x_3$  \n",
    "- **Output / Target feature:** Price of the house → $y$  \n",
    "\n",
    "> **Independent features (inputs):** $x_1, x_2, x_3$  \n",
    "> **Dependent feature (output):** $y$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Equation of Multiple Linear Regression\n",
    "\n",
    "- **General form:**  \n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\dots + \\theta_n x_n\n",
    "$$\n",
    "\n",
    "- **Where:**  \n",
    "  - $\\theta_0$ → Intercept (always one)  \n",
    "  - $\\theta_1, \\theta_2, \\dots, \\theta_n$ → Coefficients / slopes for each input feature  \n",
    "\n",
    "- **Example (house pricing):**  \n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1 (\\text{rooms}) + \\theta_2 (\\text{size}) + \\theta_3 (\\text{location})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Key Differences: Simple vs Multiple Linear Regression\n",
    "\n",
    "| Feature                  | Simple Linear Regression | Multiple Linear Regression |\n",
    "|---------------------------|------------------------|---------------------------|\n",
    "| Number of input features  | 1                      | 2 or more                |\n",
    "| Parameters                | $\\theta_0, \\theta_1$   | $\\theta_0, \\theta_1, \\dots, \\theta_n$ |\n",
    "| Equation                  | $h_\\theta(x) = \\theta_0 + \\theta_1 x$ | $h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\dots + \\theta_n x_n$ |\n",
    "| Visualization             | 2D line                | 3D or higher dimensional plane |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Gradient Descent in Multiple Linear Regression\n",
    "\n",
    "- **Goal:** Minimize cost function $J(\\theta_0, \\theta_1, \\dots, \\theta_n)$  \n",
    "- **Cost function:**  \n",
    "$$\n",
    "J(\\theta_0, \\theta_1, \\dots, \\theta_n) = \\frac{1}{2m} \\sum_{i=1}^{m} \\Big( h_\\theta(x^{(i)}) - y^{(i)} \\Big)^2\n",
    "$$\n",
    "\n",
    "- **Parameter updates (for each $\\theta_j$):**  \n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta_0, \\theta_1, \\dots, \\theta_n)}{\\partial \\theta_j} \\quad \\text{for } j = 0, 1, \\dots, n\n",
    "$$\n",
    "\n",
    "- **Gradient descent intuition:**  \n",
    "  - Each $\\theta_j$ moves toward reducing the cost  \n",
    "  - All parameters are updated simultaneously until convergence (global minimum)\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Visualization (Conceptual)\n",
    "\n",
    "- **2D:** Single feature → line  \n",
    "- **3D:** Two features → plane  \n",
    "- **Higher dimensions:** n features → hyperplane  \n",
    "\n",
    "- **Gradient descent:**  \n",
    "  - Imagine all $\\theta_j$ starting from random points  \n",
    "  - Iteratively adjust all $\\theta_j$ simultaneously  \n",
    "  - Converge to **global minimum** of $J(\\theta)$\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Summary\n",
    "\n",
    "- **Simple Linear Regression:** 1 input → line  \n",
    "- **Multiple Linear Regression:** 2+ inputs → plane/hyperplane  \n",
    "- **Equation:** $h_\\theta(x) = \\theta_0 + \\theta_1 x_1 + \\dots + \\theta_n x_n$  \n",
    "- **Parameters:** 1 intercept + n coefficients  \n",
    "- **Optimization:** Gradient descent updates all parameters together  \n",
    "\n",
    "> Next, we will learn **assumptions of linear regression**, which are crucial for correct modeling and interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3266fc8d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
