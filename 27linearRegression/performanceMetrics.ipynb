{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "146b2d16",
   "metadata": {},
   "source": [
    "# Performance Metrics in Linear Regression: R² and Adjusted R²\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Why Performance Metrics?\n",
    "\n",
    "- After building a regression model, we need to evaluate **how good the model is**.  \n",
    "- Common metrics for linear regression:  \n",
    "  1. **R squared (R²)**  \n",
    "  2. **Adjusted R squared (Adjusted R²)**  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. R Squared (R²)\n",
    "\n",
    "### Definition\n",
    "\n",
    "R² measures how much of the **variation in the dependent variable** is explained by the independent variables:\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\text{SS}_{\\text{residual}}}{\\text{SS}_{\\text{total}}}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $\\text{SS}_{\\text{residual}}$ = Sum of squared residuals (errors)  \n",
    "- $\\text{SS}_{\\text{total}}$ = Total sum of squares  \n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 Residuals\n",
    "\n",
    "- Residual = difference between **true value** and **predicted value**:\n",
    "\n",
    "$$\n",
    "\\text{Residual for } i\\text{th data point: } e_i = y_i - \\hat{y}_i\n",
    "$$\n",
    "\n",
    "- Sum of squared residuals:\n",
    "\n",
    "$$\n",
    "\\text{SS}_{\\text{residual}} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Total Sum of Squares\n",
    "\n",
    "- Measures deviation from the mean of the true values:\n",
    "\n",
    "$$\n",
    "\\text{SS}_{\\text{total}} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "$$\n",
    "\n",
    "Where $\\bar{y}$ is the mean of all $y_i$ values.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Intuition\n",
    "\n",
    "- **R² close to 1:** Model explains most of the variance → Good fit  \n",
    "- **R² close to 0:** Model explains very little variance → Poor fit  \n",
    "\n",
    "**Example:**  \n",
    "- R² = 0.75 → 75% of the variance is explained by the model  \n",
    "- R² = 0.90 → 90% of the variance is explained by the model  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Problem with R²\n",
    "\n",
    "- **Adding more features** (even irrelevant ones) can **increase R²**, even if they are not correlated with the target.  \n",
    "- Example: Adding a \"gender of resident\" feature to predict house price might slightly increase R², even though it has no real correlation with price.  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Adjusted R Squared (Adjusted R²)\n",
    "\n",
    "- **Adjusted R²** compensates for the addition of irrelevant features by **penalizing unnecessary variables**.  \n",
    "\n",
    "### Formula\n",
    "\n",
    "$$\n",
    "R^2_{\\text{adj}} = 1 - \\left( 1 - R^2 \\right) \\frac{n-1}{n-p-1}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $n$ → Number of data points  \n",
    "- $p$ → Number of independent features  \n",
    "\n",
    "---\n",
    "\n",
    "### 4.1 Intuition\n",
    "\n",
    "- If a **new feature improves the model** → Adjusted R² **increases**  \n",
    "- If a **new feature is irrelevant** → Adjusted R² **decreases**  \n",
    "\n",
    "**Example:**\n",
    "| Features Added | R² | Adjusted R² | Interpretation |\n",
    "|----------------|----|-------------|----------------|\n",
    "| Size           | 0.75 | 0.75       | Good correlation |\n",
    "| Size + Bedrooms | 0.80 | 0.78       | Both correlated |\n",
    "| Size + Bedrooms + Gender | 0.82 | 0.76 | Gender irrelevant → penalized |\n",
    "\n",
    "- Adjusted R² gives a **more reliable measure** of model performance when **multiple features** are used.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Summary\n",
    "\n",
    "- **R²**: Measures proportion of variance explained; can be misleading with extra features  \n",
    "- **Adjusted R²**: Corrects R² by penalizing irrelevant features  \n",
    "- **Higher value (closer to 1)** → Better model  \n",
    "- **Use Adjusted R²** for multiple linear regression with many features  \n",
    "\n",
    "---\n",
    "\n",
    "✅ **Key Takeaways:**\n",
    "\n",
    "1. R² evaluates **model fit**, but can increase with irrelevant features.  \n",
    "2. Adjusted R² **penalizes unnecessary features**, making it more robust for multiple regression.  \n",
    "3. Both metrics help in **selecting and evaluating regression models**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991e8c40",
   "metadata": {},
   "source": [
    "# Error Metrics in Linear Regression: MSE, MAE, RMSE\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "- In linear regression, besides **R² and Adjusted R²**, we can evaluate the **error per data point** using:  \n",
    "  1. **Mean Squared Error (MSE)**  \n",
    "  2. **Mean Absolute Error (MAE)**  \n",
    "  3. **Root Mean Squared Error (RMSE)**  \n",
    "\n",
    "- These metrics quantify how far predictions are from true values.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Mean Squared Error (MSE)\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- \\(y_i\\) = true value  \n",
    "- \\(\\hat{y}_i\\) = predicted value  \n",
    "- \\(n\\) = number of data points  \n",
    "\n",
    "---\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- Measures **average squared deviation** of predictions from true values.  \n",
    "- Squaring emphasizes larger errors.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Differentiable** – allows gradient descent to find **global minimum**.  \n",
    "2. **Convex function** – only one global minimum, no local minima.  \n",
    "3. **Faster convergence** in optimization because of the smooth gradient.\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Not robust to outliers** – large errors are heavily penalized due to squaring.  \n",
    "2. **Units are squared** – if target is in dollars, MSE is in dollars², making interpretation harder.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Mean Absolute Error (MAE)\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Robust to outliers** – errors are not squared, so extreme points have less influence.  \n",
    "2. **Same units as target** – easier to interpret.\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Non-differentiable at 0** – gradient at zero error is undefined.  \n",
    "2. **Convergence is slower** – optimization is more complex; often solved using **subgradients**.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Root Mean Squared Error (RMSE)\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "- Simply the **square root of MSE**.  \n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Same units as target** – easier interpretation than MSE.  \n",
    "2. **Differentiable** – suitable for gradient-based optimization.  \n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Not robust to outliers** – large deviations still heavily penalized.  \n",
    "\n",
    "---\n",
    "\n",
    "## 5. Choosing Between Metrics\n",
    "\n",
    "| Metric | Advantage | Disadvantage | When to Use |\n",
    "|--------|-----------|-------------|-------------|\n",
    "| **MSE** | Differentiable, convex, fast convergence | Not robust, units squared | Optimization via gradient descent |\n",
    "| **MAE** | Robust to outliers, same units | Slower convergence, non-differentiable at 0 | When outliers are present |\n",
    "| **RMSE** | Same units, differentiable | Not robust to outliers | Combines MSE benefits with interpretable units |\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Summary\n",
    "\n",
    "- **MSE, MAE, RMSE** measure error per data point.  \n",
    "- **R² and Adjusted R²** measure variance explained by the model.  \n",
    "- **MSE** → fast, smooth optimization, sensitive to outliers.  \n",
    "- **MAE** → robust to outliers, slower convergence.  \n",
    "- **RMSE** → interpretable units, sensitive to outliers.  \n",
    "\n",
    "> **Interview Tip:**  \n",
    "> - MSE is preferred for optimization (quadratic loss).  \n",
    "> - MAE is preferred when robustness to outliers is important.  \n",
    "> - Compare with R² / Adjusted R² to evaluate overall model fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155f7516",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
