{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09879c98",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "\n",
    "* For classification → **Support Vector Classifier (SVC)**\n",
    "* For regression → **Support Vector Regression (SVR)**\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Pre-requisite\n",
    "\n",
    "Before diving into SVM, it is **very important to understand Logistic Regression** (mathematical intuition and implementation).\n",
    "If you skipped logistic regression, please go through it first.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Logistic Regression Recap\n",
    "\n",
    "* In **binary classification** (2 categories of points), logistic regression creates a **decision boundary** (line in 2D, plane in 3D, hyperplane in nD).\n",
    "* Example:\n",
    "\n",
    "  * In 2D → best fit **line** separates the two classes.\n",
    "  * In 3D → best fit **plane** separates the classes.\n",
    "  * In nD → best fit **hyperplane** is created.\n",
    "\n",
    "### Example:\n",
    "\n",
    "* Features: $$ (x_1, x_2) $$ → line\n",
    "* Features: $$ (x_1, x_2, x_3) $$ → plane\n",
    "* Features: $$ (x_1, x_2, \\dots, x_n) $$ → hyperplane\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Support Vector Machine (SVM)\n",
    "\n",
    "SVM extends the concept of decision boundaries by not only finding a **separating hyperplane**, but also maximizing the **margin**.\n",
    "\n",
    "### Key Idea:\n",
    "\n",
    "* Create a **best fit line (or plane/hyperplane)**.\n",
    "* Along with it, create **two marginal planes** that are equidistant from the separating hyperplane.\n",
    "* The goal is to maximize the distance between these marginal planes.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Margins in SVM\n",
    "\n",
    "* Suppose the distance between marginal planes is $$ D $$.\n",
    "* We want to **maximize $$ D $$**.\n",
    "* The marginal planes should touch the **nearest data points** from each class.\n",
    "\n",
    "These nearest points are called **Support Vectors**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Geometric Intuition\n",
    "\n",
    "* **2D Case:**\n",
    "\n",
    "  * Best fit line + 2 parallel marginal lines.\n",
    "  * Margins pass through the closest data points.\n",
    "\n",
    "* **3D Case:**\n",
    "\n",
    "  * Decision boundary becomes a **plane**.\n",
    "  * Two parallel planes act as **marginal planes**.\n",
    "\n",
    "* **nD Case:**\n",
    "\n",
    "  * Decision boundary is a **hyperplane**.\n",
    "  * Marginal hyperplanes are placed on either side.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Classification with SVM\n",
    "\n",
    "* If a **new test point** falls:\n",
    "\n",
    "  * Above the hyperplane → assign to one class.\n",
    "  * Below the hyperplane → assign to another class.\n",
    "\n",
    "SVM ensures **clear classification** by maximizing margin.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Support Vectors\n",
    "\n",
    "* The **data points lying on the marginal planes** are called **Support Vectors**.\n",
    "* These points are crucial since they define the margin.\n",
    "* Removing non-support vectors does not change the decision boundary.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Problem Statement (Simplified)\n",
    "\n",
    "* Logistic Regression → finds a separating hyperplane.\n",
    "* Support Vector Machine → finds a separating hyperplane **with maximum margin**.\n",
    "\n",
    "Thus, SVM improves classification robustness.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e2c2b7",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) – Hard Margin vs Soft Margin\n",
    "\n",
    "In the previous discussion, we understood the **main idea of SVM** for classification:\n",
    "\n",
    "* Find a **best fit hyperplane**.\n",
    "* Create **marginal planes**.\n",
    "* Ensure the **margin is maximized**.\n",
    "\n",
    "Now, let us extend this understanding to **real-world scenarios** using **Hard Margin** and **Soft Margin**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Hard Margin\n",
    "\n",
    "* Hard Margin assumes that **all data points are perfectly separable** by a linear boundary.\n",
    "* The separating hyperplane is chosen such that:\n",
    "\n",
    "  * **No misclassification occurs.**\n",
    "  * **All points lie outside or on the margin.**\n",
    "\n",
    "### Key Characteristics:\n",
    "\n",
    "* Works only when data is **linearly separable**.\n",
    "* Margin must perfectly divide the classes without errors.\n",
    "* Sensitive to outliers (a single outlier can break separability).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Soft Margin\n",
    "\n",
    "* In real-world datasets, classes are **rarely linearly separable**.\n",
    "* Some points **overlap** across classes.\n",
    "* To handle this, SVM introduces **Soft Margin**.\n",
    "\n",
    "### Idea:\n",
    "\n",
    "* Allow **some errors (misclassifications)**.\n",
    "* Introduce **slack variables** $$ \\xi_i \\geq 0 $$ to measure the degree of misclassification for each data point.\n",
    "\n",
    "### Optimization Goal:\n",
    "\n",
    "* Maximize the margin\n",
    "* While minimizing the total misclassification error\n",
    "\n",
    "Mathematically, the optimization balances between:\n",
    "$$\n",
    "\\text{Minimize: } \\frac{1}{2} | w |^2 + C \\sum_{i=1}^n \\xi_i\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "*  w  → weight vector (orientation of hyperplane)\n",
    "*  b  → bias (shift of hyperplane)\n",
    "*  C  → regularization (trade-off: margin vs error)\n",
    "*  xi_i  → slack variable (misclassification measure)\n",
    "\n",
    "\n",
    "\n",
    "## 3. Hard Margin vs Soft Margin – Comparison\n",
    "\n",
    "| Aspect            | Hard Margin SVM                  | Soft Margin SVM                          |\n",
    "| ----------------- | -------------------------------- | ---------------------------------------- |\n",
    "| Data separability | Assumes perfectly separable data | Handles overlapping/non-separable data   |\n",
    "| Errors            | No misclassification allowed     | Allows some misclassifications           |\n",
    "| Outliers          | Very sensitive                   | More robust                              |\n",
    "| Use case          | Ideal for clean, simple datasets | Practical for noisy, real-world datasets |\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Key Insight\n",
    "\n",
    "* **Hard Margin** → Only works in theory when datasets are perfectly separable.\n",
    "* **Soft Margin** → Practical version of SVM, used in real-world problems with noise and overlap.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Next Step\n",
    "\n",
    "In the next section, we will cover the **mathematical intuition** of how to derive the best fit line and margins:\n",
    "\n",
    "* Equation of a straight line (in 2D).\n",
    "* Equation of a plane (in 3D).\n",
    "* Distance from a point to a hyperplane.\n",
    "* Optimization using Lagrange multipliers.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e3214",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) – Mathematical Intuition\n",
    "\n",
    "In the last discussion, we saw the geometric idea of SVM. Now, let’s dive into the **mathematics**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Equation of the Hyperplane\n",
    "\n",
    "In 2D, the best fit line is given by:\n",
    "\n",
    "$$\n",
    "w^T x + b = 0\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $$ w $$ → weight vector (perpendicular to the hyperplane)\n",
    "* $$ b $$ → bias term\n",
    "\n",
    "If the line passes through the origin, $$ b = 0 $$. Otherwise, $$ b \\neq 0 $$.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Distance of a Point from Hyperplane\n",
    "\n",
    "The signed distance of a point $$ x $$ from the hyperplane is:\n",
    "\n",
    "$$\n",
    "d = \\frac{w^T x + b}{| w |}\n",
    "$$\n",
    "\n",
    "* $$ d > 0 $$ → point lies **above** the hyperplane\n",
    "* $$ d < 0 $$ → point lies **below** the hyperplane\n",
    "\n",
    "This ensures positive distance for one class and negative distance for the other.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Marginal Hyperplanes\n",
    "\n",
    "Along with the main hyperplane, we define two marginal planes:\n",
    "\n",
    "* Upper margin:\n",
    "  $$\n",
    "  w^T x + b = +1\n",
    "  $$\n",
    "\n",
    "* Lower margin:\n",
    "  $$\n",
    "  w^T x + b = -1\n",
    "  $$\n",
    "\n",
    "The **margin width** is the distance between them:\n",
    "\n",
    "$$\n",
    "\\text{Margin} = \\frac{2}{| w |}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Optimization Objective\n",
    "\n",
    "We want to **maximize the margin** → equivalently **minimize**:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} | w |^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Classification Constraint\n",
    "\n",
    "For correct classification:\n",
    "\n",
    "* If $$ y_i = +1 $$, then $$ w^T x_i + b \\geq +1 $$\n",
    "* If $$ y_i = -1 $$, then $$ w^T x_i + b \\leq -1 $$\n",
    "\n",
    "This can be compactly written as:\n",
    "\n",
    "$$\n",
    "y_i , (w^T x_i + b) \\geq 1, \\quad \\forall i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Final Optimization Problem (Hard Margin SVM)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Minimize: } & \\frac{1}{2} | w |^2 \\\n",
    "\\text{Subject to: } & y_i (w^T x_i + b) \\geq 1, \\quad \\forall i\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Key Idea\n",
    "\n",
    "* **Margin width** = $$ \\frac{2}{| w |} $$\n",
    "* **Objective** = maximize margin (or minimize $$ | w |^2 $$)\n",
    "* **Constraints** = all points classified correctly\n",
    "\n",
    "This is the **core formulation of SVM** for linearly separable data (Hard Margin).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a38b53",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) – Soft Margin & Hinge Loss\n",
    "\n",
    "In real-world data, perfect linear separation is rare. To handle **overlapping points and misclassifications**, we introduce slack variables and hinge loss.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Hard Margin Cost Function\n",
    "\n",
    "For linearly separable data:\n",
    "\n",
    "$$\n",
    "\\min_{w, b} \\ \\frac{1}{2} | w |^2\n",
    "\\quad \\text{subject to: } y_i (w^T x_i + b) \\geq 1, \\ \\forall i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Introducing Slack Variables (Soft Margin)\n",
    "\n",
    "To allow some misclassification, we introduce slack variables $$ \\xi_i \\geq 0 $$:\n",
    "\n",
    "$$\n",
    "y_i (w^T x_i + b) \\geq 1 - \\xi_i, \\quad \\forall i\n",
    "$$\n",
    "\n",
    "*  \\xi_i = 0  → correctly classified point\n",
    "*  \\xi_i > 0  → misclassified or inside margin\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Regularization Parameter $$ C $$\n",
    "\n",
    "We modify the cost function to include a penalty for slack:\n",
    "\n",
    "$$\n",
    "\\min_{w, b, \\xi} \\ \\frac{1}{2} | w |^2 + C \\sum_{i=1}^{n} \\xi_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "*  C  → regularization parameter (controls trade-off between margin width and error)\n",
    "* Large  C  → fewer misclassifications, narrower margin\n",
    "* Small  C  → wider margin, more tolerance to errors\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Hinge Loss\n",
    "\n",
    "The error term for each point is defined using **hinge loss**:\n",
    "\n",
    "$$\n",
    "L_i = \\max \\big(0, \\ 1 - y_i (w^T x_i + b) \\big)\n",
    "$$\n",
    "\n",
    "So the full objective can also be written as:\n",
    "\n",
    "$$\n",
    "\\min_{w, b} \\ \\frac{1}{2} | w |^2 + C \\sum_{i=1}^{n} L_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Key Idea\n",
    "\n",
    "* **Hard Margin SVM** → assumes perfect separation\n",
    "* **Soft Margin SVM** → allows some errors using slack variables\n",
    "* **Hinge Loss** → penalizes misclassified or margin-violating points\n",
    "* **Goal** → balance **margin maximization** and **error minimization**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35788b9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
