{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f0699d",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) Algorithm\n",
    "\n",
    "In this discussion, we will learn about the **K-Nearest Neighbor (KNN)** algorithm in Machine Learning. KNN is one of the simplest algorithms that can solve both **classification** and **regression** problems.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Classification with KNN\n",
    "\n",
    "Consider a dataset with features \\(F_1, F_2\\) and output \\(y\\):\n",
    "\n",
    "- \\(y\\) can be **binary** (0 or 1) or **multi-class**.\n",
    "- \\(F_1, F_2\\) are the feature values.\n",
    "- Example dataset:\n",
    "\n",
    "| F1  | F2  | y |\n",
    "|-----|-----|---|\n",
    "| 1.2 | 3.4 | 0 |\n",
    "| 2.1 | 1.8 | 1 |\n",
    "| 0.5 | 2.2 | 0 |\n",
    "\n",
    "We want to predict the category of a **new test point** using KNN.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Initialize k value**  \n",
    "   - The **k** value is a hyperparameter representing the number of nearest neighbors.\n",
    "   - Typical values: \\(k = 1, 3, 5, 10, \\dots\\)\n",
    "   - Select k using **hyperparameter tuning** to maximize model accuracy.\n",
    "\n",
    "2. **Find k nearest neighbors**  \n",
    "   - Compute the distance between the **test point** and all **training points**.\n",
    "   - Select the top k points with the smallest distances.\n",
    "\n",
    "3. **Vote for category**  \n",
    "   - Count how many neighbors belong to each category.\n",
    "   - Assign the test point to the category with the **majority vote**.\n",
    "\n",
    "Example:\n",
    "\n",
    "- \\(k = 5\\)  \n",
    "- Neighbor categories: [0, 0, 1, 1, 1]  \n",
    "- Majority: 1 → Predicted category = 1\n",
    "\n",
    "---\n",
    "\n",
    "### Distance Metrics\n",
    "\n",
    "To find nearest neighbors, we need a **distance metric**:\n",
    "\n",
    "#### 1. Euclidean Distance\n",
    "Distance between two points \\((x_1, y_1)\\) and \\((x_2, y_2)\\):\n",
    "\n",
    "$$\n",
    "d = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n",
    "$$\n",
    "\n",
    "For n-dimensional points:\n",
    "\n",
    "$$\n",
    "d = \\sqrt{\\sum_{i=1}^{n} (x_{2i} - x_{1i})^2}\n",
    "$$\n",
    "\n",
    "#### 2. Manhattan Distance\n",
    "Distance along axes (like a grid):\n",
    "\n",
    "$$\n",
    "d = |x_2 - x_1| + |y_2 - y_1|\n",
    "$$\n",
    "\n",
    "- **Use-case**: City blocks or grid-based paths.\n",
    "- Decide between Euclidean or Manhattan based on problem and accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Regression with KNN\n",
    "\n",
    "For regression, the steps are the same as classification:\n",
    "\n",
    "1. Find the **k nearest neighbors** of the test point.\n",
    "2. Compute the **average** of their target values for prediction:\n",
    "\n",
    "$$\n",
    "y_{\\text{pred}} = \\frac{1}{k} \\sum_{i=1}^{k} y_i\n",
    "$$\n",
    "\n",
    "- Optionally, use **median** instead of mean to reduce outlier impact.\n",
    "\n",
    "Example: Predict house price based on house size:\n",
    "\n",
    "- Training data: \\([50, 60, 70, 80, 90]\\) → prices \\([150, 180, 210, 240, 270]\\)  \n",
    "- k = 3 nearest neighbors for size = 75 → prices = [180, 210, 240]  \n",
    "- Predicted price = \\((180 + 210 + 240)/3 = 210\\)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. KNN Variants for Optimization\n",
    "\n",
    "- **Problem**: Naive KNN computes distance from the test point to all training points → **O(n)** time complexity.\n",
    "- **Solution**: Use tree-based structures to reduce computation:\n",
    "  1. **k-d Tree**\n",
    "  2. **Ball Tree**\n",
    "\n",
    "- These structures partition the data space, reducing unnecessary distance calculations.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Python Implementation Example\n",
    "\n",
    "```python\n",
    "# Import libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "X = np.array([[1.2, 3.4], [2.1, 1.8], [0.5, 2.2], [3.3, 0.4], [2.8, 3.5]])\n",
    "y_class = np.array([0, 1, 0, 1, 1])  # Classification labels\n",
    "y_reg = np.array([10, 20, 15, 25, 30])  # Regression values\n",
    "\n",
    "# Classification\n",
    "knn_class = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "knn_class.fit(X, y_class)\n",
    "y_pred_class = knn_class.predict([[2.5, 2.0]])\n",
    "print(\"Predicted class:\", y_pred_class)\n",
    "\n",
    "# Regression\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=3, metric='euclidean')\n",
    "knn_reg.fit(X, y_reg)\n",
    "y_pred_reg = knn_reg.predict([[2.5, 2.0]])\n",
    "print(\"Predicted value:\", y_pred_reg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752fc877",
   "metadata": {},
   "source": [
    "# KNN Variants: K-d Tree and Ball Tree\n",
    "\n",
    "In the previous lecture, we discussed the **theoretical intuition of KNN**. In this lecture, we focus on **KNN variants** to optimize its time complexity.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Problem with Naive KNN\n",
    "\n",
    "- In naive KNN, for a query point, we calculate the distance to **all training points**.\n",
    "- Time complexity: **O(n)** for each query.\n",
    "- For large datasets, this becomes computationally expensive.\n",
    "- **Solution**: Use optimized structures like **K-d Tree** and **Ball Tree**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. K-d Tree (k-dimensional tree)\n",
    "\n",
    "### Idea:\n",
    "- K-d tree partitions points into a **binary tree structure**, reducing the number of distance calculations.\n",
    "- Each split alternates between **x-axis (F1)** and **y-axis (F2)**.\n",
    "\n",
    "### Construction Steps:\n",
    "\n",
    "1. **Compute median of F1 (x-axis)**\n",
    "   - Example F1 values: [2, 4, 5, 7, 8, 9]\n",
    "   - Median ≈ 7 → use as the root split.\n",
    "   \n",
    "2. **Divide the points into two regions**\n",
    "   - Left: F1 < 7\n",
    "   - Right: F1 ≥ 7\n",
    "\n",
    "3. **Next split uses F2 (y-axis)**\n",
    "   - Compute median of F2 in each region.\n",
    "   - Continue alternating x-axis and y-axis for further splits.\n",
    "\n",
    "4. **Recursively split until all points are assigned**\n",
    "   - Each node in the tree represents a point and a split.\n",
    "   - Points are grouped into **regions**, so nearest neighbor search is limited to relevant regions.\n",
    "\n",
    "### Nearest Neighbor Search:\n",
    "- Traverse the tree to find the region containing the query point.\n",
    "- Compute distances within the region.\n",
    "- **Backtracking**: Move up the tree to check if closer neighbors exist in other regions.\n",
    "- Example query:  \n",
    "  Query point = (5, 7)  \n",
    "  Nearest points found via tree traversal → reduces distance computations compared to brute-force.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Ball Tree\n",
    "\n",
    "### Idea:\n",
    "- Ball Tree clusters points hierarchically based on distance (like nested circles or \"balls\").\n",
    "- Reduces backtracking compared to K-d Tree in high dimensions.\n",
    "\n",
    "### Construction Steps:\n",
    "1. Group nearby points into **clusters** (e.g., pairs or triples).\n",
    "   - Example: Group1 = {2,3}, Group2 = {3,4}, Group3 = {5,6,7}, Group4 = {8,9}\n",
    "2. Combine nearest clusters to form higher-level groups:\n",
    "   - G5 = G1 + G2\n",
    "   - G6 = G3 + G4\n",
    "   - G7 = G5 + G6 (root of tree)\n",
    "3. Each node stores points inside a \"ball\" (radius enclosing the group).\n",
    "\n",
    "### Nearest Neighbor Search:\n",
    "- Determine which **ball contains the query point**.\n",
    "- Compute distances only within that ball.\n",
    "- Move to next nearest balls if needed.\n",
    "- Reduces distance computations significantly.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Comparison\n",
    "\n",
    "| Feature           | K-d Tree                  | Ball Tree                   |\n",
    "|------------------|--------------------------|----------------------------|\n",
    "| Structure         | Binary tree              | Hierarchical clusters      |\n",
    "| Split             | Axis-aligned (x, y alternately) | Based on distance (balls) |\n",
    "| Backtracking      | Required                 | Less required              |\n",
    "| Best for          | Low-dimensional data     | High-dimensional data      |\n",
    "| Distance checks   | Reduced vs brute-force   | Further reduced            |\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Python Example\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "X = np.array([[2,3],[4,7],[5,4],[7,2],[8,1],[9,6]])\n",
    "y = np.array([0, 0, 1, 1, 1, 0])\n",
    "\n",
    "# KNN with Ball Tree\n",
    "knn = KNeighborsClassifier(n_neighbors=3, algorithm='ball_tree')\n",
    "knn.fit(X, y)\n",
    "\n",
    "query_point = np.array([[5,7]])\n",
    "prediction = knn.predict(query_point)\n",
    "print(\"Predicted category:\", prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910bde29",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
