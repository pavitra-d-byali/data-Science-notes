{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00a638a9",
   "metadata": {},
   "source": [
    "# Ensemble Techniques in Machine Learning\n",
    "\n",
    "In this video, we are going to discuss something called **Ensemble Techniques**. Specifically, we will focus on **Bagging** and **Boosting**.\n",
    "\n",
    "By the end of this video, you will understand:\n",
    "\n",
    "- What exactly Bagging is\n",
    "- Which algorithms are covered under Bagging\n",
    "- What exactly Boosting is\n",
    "\n",
    "---\n",
    "\n",
    "## What Are Ensemble Techniques?\n",
    "\n",
    "So far, you have learned many individual machine learning algorithms. But we have never seen a scenario where multiple algorithms are combined for prediction.  \n",
    "\n",
    "**Ensemble Techniques** help us combine multiple algorithms, train them, and generate a more accurate output.\n",
    "\n",
    "- Combining multiple models generally improves accuracy.\n",
    "- Ensemble methods are widely used in platforms like **Kaggle** and other hackathons.\n",
    "- Techniques like **Bagging** and **Boosting** can significantly improve prediction performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Bagging\n",
    "\n",
    "### Idea Behind Bagging\n",
    "\n",
    "In **Bagging**:\n",
    "\n",
    "1. We have a dataset \\(D\\) with features and data points.\n",
    "2. We create **multiple base learners**, which can be:\n",
    "   - Different algorithms (e.g., Decision Tree, Logistic Regression)\n",
    "   - Same algorithm repeated multiple times\n",
    "3. Each base learner is trained on a **different sample** of the dataset (bootstrap sampling).\n",
    "4. After training, predictions are made on the test data:\n",
    "   - **Classification:** Use majority voting.\n",
    "   - **Regression:** Take the average of outputs.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. Take dataset \\(D\\)\n",
    "2. Create base learners \\(M_1, M_2, \\dots, M_N\\)\n",
    "3. Train each model on a different sample of the data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052ebc82",
   "metadata": {},
   "source": [
    "D'1 → M1\n",
    "\n",
    "D'2 → M2\n",
    "\n",
    "...\n",
    "\n",
    "D'N → MN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4667ae9e",
   "metadata": {},
   "source": [
    "\n",
    "4. For a new test instance:\n",
    "- Each model predicts\n",
    "- **Classification:** Majority vote\n",
    "- **Regression:** Average of outputs\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- Base learners are trained **in parallel**.\n",
    "- Bagging reduces **variance** and helps prevent **overfitting**.\n",
    "- Example algorithm: **Random Forest**\n",
    "\n",
    "---\n",
    "\n",
    "## Boosting\n",
    "\n",
    "### Idea Behind Boosting\n",
    "\n",
    "In **Boosting**:\n",
    "\n",
    "- We combine multiple **weak learners** sequentially.\n",
    "- Weak learners focus on correcting the mistakes of previous models.\n",
    "- The final combination results in a **strong learner**.\n",
    "\n",
    "### Workflow\n",
    "\n",
    "1. Take dataset \\(D\\)\n",
    "2. Train the first model \\(M_1\\) on the dataset.\n",
    "3. Identify the records that \\(M_1\\) predicted incorrectly.\n",
    "4. Train the next model \\(M_2\\) on the incorrectly predicted records (and possibly additional data).\n",
    "5. Repeat sequentially for \\(M_3, M_4, \\dots, M_n\\).\n",
    "6. Combine predictions of all models:\n",
    "- **Classification:** Majority vote\n",
    "- **Regression:** Average\n",
    "\n",
    "### Analogy\n",
    "\n",
    "- Each weak learner is like a domain expert:\n",
    "- Model 1: Expert in Geography\n",
    "- Model 2: Expert in Physics\n",
    "- Model 3: Expert in Chemistry\n",
    "- Combining all weak learners gives a strong overall prediction.\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- Weak learners are trained **sequentially**.\n",
    "- Boosting reduces **bias** and often improves accuracy on challenging datasets.\n",
    "- Example algorithms:\n",
    "- AdaBoost\n",
    "- Gradient Boosting\n",
    "- XGBoost (Extreme Gradient Boost)\n",
    "\n",
    "---\n",
    "\n",
    "## Bagging vs Boosting\n",
    "\n",
    "| Feature                 | Bagging                         | Boosting                        |\n",
    "|-------------------------|---------------------------------|--------------------------------|\n",
    "| Learners                | Base learners                  | Weak learners                  |\n",
    "| Training                | Parallel                       | Sequential                     |\n",
    "| Focus                   | Reducing variance              | Reducing bias                  |\n",
    "| Output                  | Majority vote / Average        | Weighted combination           |\n",
    "| Example Algorithm       | Random Forest                  | AdaBoost, Gradient Boosting, XGBoost |\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Bagging:** Parallel training, reduces variance, uses majority vote/average.\n",
    "- **Boosting:** Sequential training, reduces bias, combines weak learners into a strong learner.\n",
    "- Both techniques improve prediction accuracy for classification and regression problems.\n",
    "\n",
    "---\n",
    "\n",
    "Next, we will start with **Random Forest**, a bagging technique, and explore how it works for both regression and classification, including implementation and mathematical intuition. Then, we will move on to Boosting algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "Thank you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3417e72a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
