{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b4ade2",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm Variants\n",
    "\n",
    "In this discussion, we will cover the **three main variants of Naive Bayes**:\n",
    "\n",
    "1. **Bernoulli Naive Bayes**  \n",
    "2. **Multinomial Naive Bayes**  \n",
    "3. **Gaussian Naive Bayes**\n",
    "\n",
    "These variants are important because the choice of the algorithm depends on the type of dataset you have.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Bernoulli Naive Bayes\n",
    "\n",
    "**When to use:**  \n",
    "Use Bernoulli Naive Bayes when the **features follow a Bernoulli distribution** (binary outcomes: 0 or 1).\n",
    "\n",
    "**Bernoulli distribution:**  \n",
    "- Only two outcomes: success/failure, yes/no, pass/fail, heads/tails.  \n",
    "- Example: Tossing a coin results in either 0 or 1.\n",
    "\n",
    "**Example Dataset Features:**\n",
    "\n",
    "| Feature | Values         |\n",
    "|---------|----------------|\n",
    "| F1      | Yes, Yes, No, Yes |\n",
    "| F2      | Pass, Fail, Pass, Fail |\n",
    "| F3      | Male, Female, Male, Female |\n",
    "\n",
    "- Output: Binary classification (Yes/No) or multi-class classification.  \n",
    "- If most features are binary, **Bernoulli Naive Bayes** should be used.\n",
    "\n",
    "**Key Idea:**  \n",
    "\n",
    "$$\n",
    "P(Y|X_1, X_2, \\dots, X_n) \\propto P(Y) \\cdot \\prod_{i=1}^{n} P(X_i|Y)\n",
    "$$\n",
    "\n",
    "Here, each feature \\(X_i\\) is binary (0 or 1).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Multinomial Naive Bayes\n",
    "\n",
    "**When to use:**  \n",
    "Use Multinomial Naive Bayes when the **input features are in the form of counts**, typically for **text data**.\n",
    "\n",
    "**Example Problem:** Spam Classification  \n",
    "- Input: Email body (text)  \n",
    "- Output: Spam or Ham (Not Spam)\n",
    "\n",
    "**Text Conversion:**  \n",
    "Text data must be converted into **numerical vectors** before applying the algorithm. Common techniques include:\n",
    "\n",
    "1. **Bag of Words (BoW)**\n",
    "2. **TF-IDF**\n",
    "3. **Word2Vec**  \n",
    "\n",
    "**Example:**\n",
    "\n",
    "| Email Body                    | Output |\n",
    "|-------------------------------|--------|\n",
    "| \"You have $1 million lottery\" | Spam   |\n",
    "| \"Krish, you have done a good job\" | Ham    |\n",
    "\n",
    "- Count-based features: Number of occurrences of each word.  \n",
    "- Suitable for **discrete features** where each value represents a count.\n",
    "\n",
    "**Key Idea:**  \n",
    "\n",
    "$$\n",
    "P(Y|X_1, X_2, \\dots, X_n) \\propto P(Y) \\cdot \\prod_{i=1}^{n} P(X_i|Y)\n",
    "$$\n",
    "\n",
    "- \\(X_i\\) represents the **count of the word** in the document.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Gaussian Naive Bayes\n",
    "\n",
    "**When to use:**  \n",
    "Use Gaussian Naive Bayes when the **features are continuous** and approximately follow a **Gaussian (Normal) distribution**.\n",
    "\n",
    "**Gaussian (Normal) Distribution:** Bell-shaped curve\n",
    "\n",
    "**Example Dataset:** Iris Dataset  \n",
    "- Features: Sepal length, Sepal width, Petal length, Petal width (continuous values)  \n",
    "- Output: Flower species (multi-class classification)\n",
    "\n",
    "**Feature Examples (continuous values):**\n",
    "\n",
    "| Sepal Length | Sepal Width | Petal Length | Petal Width | Species |\n",
    "|--------------|-------------|--------------|-------------|---------|\n",
    "| 5.1          | 3.5         | 1.4          | 0.2         | Setosa  |\n",
    "| 7.0          | 3.2         | 4.7          | 1.4         | Versicolor |\n",
    "\n",
    "- Continuous features like age, height, weight, etc., are suitable for Gaussian Naive Bayes.  \n",
    "- The algorithm uses **mean** and **standard deviation** of each feature to calculate probabilities.\n",
    "\n",
    "**Probability Calculation for Continuous Feature \\(X_i\\) given class \\(Y\\):**\n",
    "\n",
    "$$\n",
    "P(X_i|Y) = \\frac{1}{\\sqrt{2\\pi\\sigma_Y^2}} \\exp\\Bigg(-\\frac{(X_i - \\mu_Y)^2}{2\\sigma_Y^2}\\Bigg)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\(\\mu_Y\\) = mean of feature \\(X_i\\) for class \\(Y\\)  \n",
    "- \\(\\sigma_Y\\) = standard deviation of feature \\(X_i\\) for class \\(Y\\)\n",
    "\n",
    "**Key Idea:**  \n",
    "\n",
    "$$\n",
    "P(Y|X_1, X_2, \\dots, X_n) \\propto P(Y) \\cdot \\prod_{i=1}^{n} P(X_i|Y)\n",
    "$$\n",
    "\n",
    "- Here, \\(X_i\\) is continuous and probabilities are calculated using the Gaussian formula.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Choosing the Right Variant\n",
    "\n",
    "| Variant      | Feature Type                     | Example Use Case        |\n",
    "|-------------|----------------------------------|------------------------|\n",
    "| Bernoulli   | Binary (0/1)                     | Pass/Fail, Yes/No      |\n",
    "| Multinomial | Count-based / Text               | Spam Classification    |\n",
    "| Gaussian    | Continuous (Real Values)         | Iris Dataset, Age, Height |\n",
    "\n",
    "**Guideline:**  \n",
    "- If most features are binary → Bernoulli Naive Bayes  \n",
    "- If features are counts or text → Multinomial Naive Bayes  \n",
    "- If features are continuous → Gaussian Naive Bayes  \n",
    "\n",
    "---\n",
    "\n",
    "**References:**\n",
    "\n",
    "- Bayes Theorem:  \n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A) \\cdot P(B|A)}{P(B)}\n",
    "$$\n",
    "\n",
    "- Naive Bayes Classification:  \n",
    "\n",
    "$$\n",
    "P(Y|X_1, \\dots, X_n) \\propto P(Y) \\cdot \\prod_{i=1}^{n} P(X_i|Y)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d756cc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
