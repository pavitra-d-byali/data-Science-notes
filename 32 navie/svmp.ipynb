{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a571889",
   "metadata": {},
   "source": [
    "# Naive Bayes Algorithm - Detailed Explanation\n",
    "\n",
    "Naive Bayes algorithm is specifically used to solve **classification problems**, including **binary** and **multi-class classification**.  \n",
    "To understand Naive Bayes, we first need a basic understanding of **probability concepts**, especially **conditional probability**, which leads to **Bayes Theorem**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Probability Concepts\n",
    "\n",
    "### 1.1 Independent Events\n",
    "An **independent event** is one where the outcome of one event **does not affect** the probability of another event.\n",
    "\n",
    "**Example:** Rolling a dice.  \n",
    "Possible outcomes: \\(1,2,3,4,5,6\\)\n",
    "\n",
    "$$\n",
    "P(\\text{1}) = \\frac{1}{6}, \\quad P(\\text{2}) = \\frac{1}{6}, \\dots\n",
    "$$\n",
    "\n",
    "**Reason:** Probability of one outcome does not change the probability of another outcome.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Dependent Events\n",
    "A **dependent event** is one where the outcome of one event **affects** the probability of another.\n",
    "\n",
    "**Example:** Drawing marbles from a bag.\n",
    "\n",
    "- Bag contains 3 orange and 2 yellow marbles.  \n",
    "- Event 1: Draw an orange marble\n",
    "\n",
    "$$\n",
    "P(\\text{Orange}) = \\frac{3}{5}\n",
    "$$\n",
    "\n",
    "- Event 2: Draw a yellow marble **after removing an orange marble**\n",
    "\n",
    "$$\n",
    "P(\\text{Yellow | Orange}) = \\frac{2}{4} = \\frac{1}{2}\n",
    "$$\n",
    "\n",
    "**Combined probability** (dependent events):\n",
    "\n",
    "$$\n",
    "P(\\text{Orange and Yellow}) = P(\\text{Orange}) \\cdot P(\\text{Yellow | Orange}) = \\frac{3}{5} \\cdot \\frac{1}{2} = \\frac{3}{10}\n",
    "$$\n",
    "\n",
    "**General formula for dependent events:**\n",
    "\n",
    "$$\n",
    "P(A \\text{ and } B) = P(A) \\cdot P(B|A)\n",
    "$$\n",
    "\n",
    "This is called **conditional probability**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bayes Theorem\n",
    "\n",
    "Using conditional probability, **Bayes Theorem** is derived as:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(A) \\cdot P(B|A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\(P(A|B)\\) = Probability of event \\(A\\) given \\(B\\) has occurred  \n",
    "- \\(P(A)\\) = Probability of event \\(A\\)  \n",
    "- \\(P(B|A)\\) = Probability of event \\(B\\) given \\(A\\) has occurred  \n",
    "- \\(P(B)\\) = Probability of event \\(B\\)\n",
    "\n",
    "This theorem is the **foundation of Naive Bayes algorithm**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Naive Bayes in Machine Learning\n",
    "\n",
    "In ML, we have:\n",
    "\n",
    "- **Independent features**: \\(X_1, X_2, X_3, \\dots, X_n\\)  \n",
    "- **Dependent feature (target)**: \\(Y\\)\n",
    "\n",
    "We want to predict \\(Y\\) given the features:\n",
    "\n",
    "$$\n",
    "P(Y|X_1, X_2, X_3, \\dots, X_n) = \\frac{P(Y) \\cdot P(X_1, X_2, \\dots, X_n | Y)}{P(X_1, X_2, \\dots, X_n)}\n",
    "$$\n",
    "\n",
    "### 3.1 Naive Assumption\n",
    "\n",
    "Naive Bayes assumes **feature independence**:\n",
    "\n",
    "$$\n",
    "P(X_1, X_2, \\dots, X_n | Y) = P(X_1|Y) \\cdot P(X_2|Y) \\cdot \\dots \\cdot P(X_n|Y)\n",
    "$$\n",
    "\n",
    "Thus, the prediction formula becomes:\n",
    "\n",
    "$$\n",
    "P(Y|X_1, X_2, \\dots, X_n) \\propto P(Y) \\cdot \\prod_{i=1}^{n} P(X_i|Y)\n",
    "$$\n",
    "\n",
    "> Denominator \\(P(X_1, X_2, \\dots, X_n)\\) is constant for all classes, so we can ignore it for comparison.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Example: Tennis Dataset\n",
    "\n",
    "### Dataset Features:\n",
    "\n",
    "- Outlook: Sunny, Overcast, Rain  \n",
    "- Temperature: Hot, Mild, Cool  \n",
    "- Output: Play Tennis (Yes/No)\n",
    "\n",
    "### Step 1: Calculate Probabilities\n",
    "\n",
    "#### 4.1 Target Probability\n",
    "\n",
    "$$\n",
    "P(\\text{Yes}) = \\frac{9}{14}, \\quad P(\\text{No}) = \\frac{5}{14}\n",
    "$$\n",
    "\n",
    "#### 4.2 Conditional Probabilities (example for Outlook)\n",
    "\n",
    "| Outlook   | P(Outlook|Yes) | P(Outlook|No) |\n",
    "|-----------|----------------|---------------|\n",
    "| Sunny     | 2/9            | 3/5           |\n",
    "| Overcast  | 4/9            | 0/5           |\n",
    "| Rain      | 3/9            | 2/5           |\n",
    "\n",
    "For Temperature:\n",
    "\n",
    "| Temperature | P(Temp|Yes) | P(Temp|No) |\n",
    "|-------------|-------------|------------|\n",
    "| Hot         | 2/9         | 2/5        |\n",
    "| Mild        | 4/9         | 2/5        |\n",
    "| Cool        | 3/9         | 1/5        |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Predict for new data\n",
    "\n",
    "**Test Data:** Outlook = Sunny, Temperature = Hot\n",
    "\n",
    "#### Probability of Yes:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\text{Yes | Sunny, Hot}) &\\propto P(\\text{Yes}) \\cdot P(\\text{Sunny | Yes}) \\cdot P(\\text{Hot | Yes}) \\\\\n",
    "&= \\frac{9}{14} \\cdot \\frac{2}{9} \\cdot \\frac{2}{9} \\\\\n",
    "&= \\frac{4}{126} \\approx 0.0317\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Probability of No:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(\\text{No | Sunny, Hot}) &\\propto P(\\text{No}) \\cdot P(\\text{Sunny | No}) \\cdot P(\\text{Hot | No}) \\\\\n",
    "&= \\frac{5}{14} \\cdot \\frac{3}{5} \\cdot \\frac{2}{5} \\\\\n",
    "&= \\frac{6}{70} \\approx 0.0857\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Normalize Probabilities\n",
    "\n",
    "$$\n",
    "P(\\text{Yes | Sunny, Hot}) = \\frac{0.0317}{0.0317 + 0.0857} \\approx 0.27\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{No | Sunny, Hot}) = \\frac{0.0857}{0.0317 + 0.0857} \\approx 0.73\n",
    "$$\n",
    "\n",
    "**Conclusion:**  \n",
    "- Output = **No** (not play tennis) because probability of No is higher.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Summary\n",
    "\n",
    "1. Naive Bayes uses **Bayes Theorem** to calculate probabilities.  \n",
    "2. Assumes **feature independence**.  \n",
    "3. Computes conditional probabilities \\(P(X_i|Y)\\) for all features.  \n",
    "4. Multiplies with class probability \\(P(Y)\\) to get the predicted class.  \n",
    "5. Can be used for **binary** and **multi-class classification**.\n",
    "\n",
    "---\n",
    "\n",
    "**References:**  \n",
    "\n",
    "- Bayes Theorem:  \n",
    "$$\n",
    "P(A|B) = \\frac{P(A) \\cdot P(B|A)}{P(B)}\n",
    "$$\n",
    "\n",
    "- Naive Bayes Classification:  \n",
    "$$\n",
    "P(Y|X_1, \\dots, X_n) \\propto P(Y) \\cdot \\prod_{i=1}^{n} P(X_i|Y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91856122",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
