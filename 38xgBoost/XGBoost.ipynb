{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1db56e3",
   "metadata": {},
   "source": [
    "# XGBoost (Extreme Gradient Boosting) Classifier\n",
    "\n",
    "In this video, we are going to discuss a new machine learning algorithm called **XGBoost**, which stands for **Extreme Gradient Boosting**. This algorithm can solve both **classification** and **regression** problems.\n",
    "\n",
    "We will focus on a **classification example** and understand how a decision tree is constructed sequentially in XGBoost, including important parameters and the function equation for calculating the final output.\n",
    "\n",
    "---\n",
    "\n",
    "## Dataset\n",
    "\n",
    "- Input Features: `Salary` and `Credit Score`\n",
    "- Output Feature: `Credit Card Approval` (binary classification)\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Create Base Model\n",
    "\n",
    "For **binary classification**, the base model outputs a **probability**:\n",
    "\n",
    "$$\n",
    "\\hat{y}_0 = 0.5\n",
    "$$\n",
    "\n",
    "This ensures that the base model is **unbiased**.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Compute Residuals\n",
    "\n",
    "Residuals for the first decision tree are computed as:\n",
    "\n",
    "$$\n",
    "r_i = y_i - \\hat{y}_i\n",
    "$$\n",
    "\n",
    "Example residuals:\n",
    "\n",
    "| Record | $y_i$ | $\\hat{y}_0$ | $r_i$ |\n",
    "|--------|-------|-------------|-------|\n",
    "| 1      | 0     | 0.5         | -0.5  |\n",
    "| 2      | 1     | 0.5         | 0.5   |\n",
    "| 3      | 1     | 0.5         | 0.5   |\n",
    "| 4      | 0     | 0.5         | -0.5  |\n",
    "| 5      | 1     | 0.5         | 0.5   |\n",
    "| 6      | 1     | 0.5         | 0.5   |\n",
    "| 7      | 1     | 0.5         | 0.5   |\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Construct Decision Tree\n",
    "\n",
    "- Input features: `Salary` and `Credit Score`\n",
    "- Output feature: Residuals \\( r_i \\)\n",
    "\n",
    "Split example:\n",
    "\n",
    "- Feature: `Salary`\n",
    "- Threshold: `<= 50K` and `> 50K`\n",
    "\n",
    "Residuals for splits:\n",
    "\n",
    "- **Left Child (<= 50K):** `[-0.5, 0.5, 0.5, 0.5]`\n",
    "- **Right Child (> 50K):** `[-0.5, 0.5, 0.5]`\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Calculate Similarity Score\n",
    "\n",
    "Similarity score formula:\n",
    "\n",
    "$$\n",
    "\\text{Similarity Score} = \\frac{\\sum_i r_i^2}{\\sum_i p_i (1 - p_i)}\n",
    "$$\n",
    "\n",
    "Where \\( p_i \\) is the probability from the base model.\n",
    "\n",
    "- **Left Child:** \n",
    "\n",
    "$$\n",
    "\\text{Similarity} = \\frac{(-0.5)^2 + 0.5^2 + 0.5^2 + 0.5^2}{0.5(1-0.5) \\times 4} = 0\n",
    "$$\n",
    "\n",
    "- **Right Child:** \n",
    "\n",
    "$$\n",
    "\\text{Similarity} = \\frac{(-0.5)^2 + 0.5^2 + 0.5^2}{0.5(1-0.5) \\times 3} = 0.33\n",
    "$$\n",
    "\n",
    "- **Root Node:** \n",
    "\n",
    "$$\n",
    "\\text{Similarity} = 0.14\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Calculate Gain\n",
    "\n",
    "$$\n",
    "\\text{Gain} = \\text{Similarity}_{\\text{Left}} + \\text{Similarity}_{\\text{Right}} - \\text{Similarity}_{\\text{Root}}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "\\text{Gain} = 0 + 0.33 - 0.14 = 0.19\n",
    "$$\n",
    "\n",
    "Choose the feature with **highest gain** for splitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Further Splits\n",
    "\n",
    "- Next feature: `Credit Score`\n",
    "- Consider splits like `Bad` vs `Good/Normal`\n",
    "- Compute similarity score and gain for each split\n",
    "- Continue until stopping criteria (e.g., **cover value** threshold) is reached\n",
    "\n",
    "Cover value formula:\n",
    "\n",
    "$$\n",
    "\\text{Cover} = p (1 - p)\n",
    "$$\n",
    "\n",
    "- Stop splitting if similarity weight < cover value\n",
    "\n",
    "---\n",
    "\n",
    "## Step 7: Predicting New Data\n",
    "\n",
    "1. Pass new data through **base model** and compute **log-odds**:\n",
    "\n",
    "$$\n",
    "\\text{log-odds} = \\log\\frac{p}{1-p}\n",
    "$$\n",
    "\n",
    "- For base probability 0.5: \n",
    "\n",
    "$$\n",
    "\\text{log-odds} = \\log\\frac{0.5}{0.5} = 0\n",
    "$$\n",
    "\n",
    "2. Pass through **decision tree(s)** sequentially, apply **learning rate** \\( \\alpha \\):\n",
    "\n",
    "$$\n",
    "F(x) = \\hat{y}_0 + \\alpha_1 H_1(x) + \\alpha_2 H_2(x) + \\dots\n",
    "$$\n",
    "\n",
    "3. Apply **sigmoid activation function** to get probability:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "- Example:\n",
    "\n",
    "$$\n",
    "\\hat{y}_1 = \\sigma(0 + 0.1 \\times 1) = 0.52\n",
    "$$\n",
    "\n",
    "- For another record with residual 0.33:\n",
    "\n",
    "$$\n",
    "\\hat{y}_2 = \\sigma(0 + 0.1 \\times 0.33) = 0.508\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 8: Repeat\n",
    "\n",
    "- Compute new residuals \\( R_2, R_3, \\dots \\)\n",
    "- Construct next decision tree using residuals as output\n",
    "- Continue until the desired number of trees is constructed\n",
    "- Learning rate \\( \\alpha \\) helps prevent overfitting\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "The **XGBoost classifier** prediction is:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma\\Big(\\hat{y}_0 + \\alpha_1 H_1(x) + \\alpha_2 H_2(x) + \\dots + \\alpha_n H_n(x)\\Big)\n",
    "$$\n",
    "\n",
    "- Base model: un-biased probability\n",
    "- Sequential trees trained on residuals\n",
    "- Similarity score and gain used to select splits\n",
    "- Learning rate and cover value prevent overfitting\n",
    "- Sigmoid function converts log-odds to probability\n",
    "- For multiclass, sigmoid → softmax\n",
    "\n",
    "---\n",
    "\n",
    "This process applies similarly to **regression**, with changes only in **similarity weight** and **gain formulas**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46322c94",
   "metadata": {},
   "source": [
    "# XGBoost Regression Machine Learning Algorithm\n",
    "\n",
    "In this video, we are going to discuss the **XGBoost Regression** machine learning algorithm.\n",
    "\n",
    "Similar to the **XGBoost Classification** algorithm, we will take a simple dataset and see how **sequential decision trees** are created. Then we will compare the differences between both.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Problem Statement\n",
    "\n",
    "We have a **regression dataset**:\n",
    "\n",
    "- **Input Features:** `experience` and `gap`\n",
    "- **Output Feature (Dependent):** `salary`\n",
    "\n",
    "Goal: Predict **salary** based on `experience` and `gap`.\n",
    "\n",
    "This is a **regression problem**, and XGBoost regressor can solve it efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. XGBoost Classifier Recap\n",
    "\n",
    "For XGBoost classifier:\n",
    "\n",
    "- We use **similarity weight** to construct decision trees.\n",
    "- **Similarity weight formula (classification):**\n",
    "\n",
    "$$\n",
    "w = \\frac{\\sum (\\text{residual})^2}{\\sum p(1-p) + \\lambda}\n",
    "$$\n",
    "\n",
    "- **Gain** is calculated to decide the best split.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. XGBoost Regressor: Steps\n",
    "\n",
    "### Step 1: Base Model\n",
    "- Base model predicts the **average** of the output:\n",
    "\n",
    "$$\n",
    "\\text{Base Model Output} = \\frac{40 + 42 + 52 + 60 + 62}{5} \\approx 51\n",
    "$$\n",
    "\n",
    "- Let this predicted value be $\\hat{y} = 51$ K.\n",
    "\n",
    "### Step 2: Compute Residuals\n",
    "- Residuals ($r_1$) = Actual Salary - Base Model Output\n",
    "\n",
    "| Salary | Residual |\n",
    "|--------|----------|\n",
    "| 40     | -11      |\n",
    "| 42     | -9       |\n",
    "| 52     | 1        |\n",
    "| 60     | 9        |\n",
    "| 62     | 11       |\n",
    "\n",
    "### Step 3: Construct Decision Tree\n",
    "- Inputs: $x_i = \\text{experience, gap}$\n",
    "- Output: $r_1$\n",
    "\n",
    "**Example Split:**\n",
    "- Feature: `experience`\n",
    "- Threshold: 2\n",
    "\n",
    "| Condition          | Residuals      |\n",
    "|-------------------|----------------|\n",
    "| experience ≤ 2     | -11             |\n",
    "| experience > 2     | -9, 1, 9, 11   |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Calculate Similarity Weight\n",
    "**For Regression:**\n",
    "\n",
    "$$\n",
    "w = \\frac{\\sum (\\text{residual})^2}{\\text{number of residuals} + \\lambda}\n",
    "$$\n",
    "\n",
    "- **Left child (≤2):**\n",
    "\n",
    "$$\n",
    "w_\\text{left} = \\frac{(-11)^2}{1 + 1} = \\frac{121}{2} = 60.5\n",
    "$$\n",
    "\n",
    "- **Right child (>2):**\n",
    "\n",
    "$$\n",
    "w_\\text{right} = \\frac{(-9+1+9+11)^2}{4 + 1} = \\frac{142^2?}{5} \\approx 28.5\n",
    "$$\n",
    "\n",
    "- **Root Node:**\n",
    "\n",
    "$$\n",
    "w_\\text{root} = 1.16\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Calculate Gain\n",
    "**Gain formula:**\n",
    "\n",
    "$$\n",
    "\\text{Gain} = w_\\text{left} + w_\\text{right} - w_\\text{root}\n",
    "$$\n",
    "\n",
    "- Example Calculation:\n",
    "\n",
    "$$\n",
    "\\text{Gain} = 65.5 + 28.5 - 0.16 = 98.34\n",
    "$$\n",
    "\n",
    "- If a different threshold gives higher gain (e.g., 143.42), choose that split.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 6: Further Splitting\n",
    "- Further splits can be based on `gap`.\n",
    "- Base learner output = 51 K\n",
    "- Decision Tree 1 output depends on splits and residual averages.\n",
    "- Example for `experience > 2.5` and `gap = no`:\n",
    "\n",
    "$$\n",
    "\\text{Decision Tree Output} = \\text{Average}(1, 9) = 5\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 7: Predicting New Values\n",
    "- **Predicted output** formula:\n",
    "\n",
    "$$\n",
    "\\hat{y}_\\text{new} = \\text{Base Model Output} + \\alpha \\cdot (\\text{Decision Tree Output})\n",
    "$$\n",
    "\n",
    "- Example (learning rate $\\alpha = 0.1$):\n",
    "\n",
    "$$\n",
    "\\hat{y}_\\text{new} = 51 + 0.1 \\cdot 5 = 51.5\n",
    "$$\n",
    "\n",
    "- For record with `experience = 2` and `gap = yes`:\n",
    "\n",
    "$$\n",
    "\\hat{y}_\\text{new} = 51 + 0.1 \\cdot (-10) = 49.9\n",
    "$$\n",
    "\n",
    "- Similarly, compute predicted values for all records.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 8: Constructing Next Tree\n",
    "- Use **residuals from previous tree** as the new output $r_2$.\n",
    "- Repeat steps with new residuals until multiple decision trees are created sequentially.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Summary\n",
    "\n",
    "- **XGBoost Regressor** builds **sequential decision trees** using residuals.\n",
    "- **Similarity Weight (Regression):**\n",
    "\n",
    "$$\n",
    "w = \\frac{\\text{residual}^2}{\\text{number of residuals} + \\lambda}\n",
    "$$\n",
    "\n",
    "- **Similarity Weight (Classification):**\n",
    "\n",
    "$$\n",
    "w = \\frac{\\text{residual}^2}{\\sum p(1-p) + \\lambda}\n",
    "$$\n",
    "\n",
    "- **Gain** is used to select the best split.\n",
    "- **Final prediction**:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{Base Model} + \\sum \\alpha_i \\cdot (\\text{Tree}_i \\text{ output})\n",
    "$$\n",
    "\n",
    "- **Hyperparameters:** Learning rate $\\alpha$, lambda $\\lambda$, number of trees, etc.\n",
    "\n",
    "---\n",
    "\n",
    "This process is similar to XGBoost classification but tailored for **regression problems**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f5bc91",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
