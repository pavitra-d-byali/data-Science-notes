{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a2969c",
   "metadata": {},
   "source": [
    "# Gradient Boosting Machine Learning Algorithm\n",
    "\n",
    "In this video, we are going to learn about a new machine learning algorithm called the **Gradient Boosting Machine Learning algorithm**. This algorithm can solve both **regression** and **classification** use cases.\n",
    "\n",
    "Gradient boosting is part of a **boosting ensemble technique**, where we create decision trees **sequentially**. By combining all the weak learners together, we finally get a **strong learner**.\n",
    "\n",
    "We have already discussed **AdaBoost** and seen how decision trees are constructed in AdaBoost. Now, let's move on to **gradient boosting**.\n",
    "\n",
    "---\n",
    "\n",
    "## Regression Problem Statement\n",
    "\n",
    "We will take a regression problem and construct **gradient boosting decision trees** on this dataset.\n",
    "\n",
    "**Dataset Description:**\n",
    "\n",
    "| Feature       | Type        |\n",
    "|---------------|------------|\n",
    "| Experience    | Independent|\n",
    "| Degree        | Independent|\n",
    "| Salary        | Dependent  |\n",
    "\n",
    "- **Experience** and **Degree** are independent features.\n",
    "- **Salary** is the continuous dependent feature (output).\n",
    "\n",
    "---\n",
    "\n",
    "## Steps to Create a Gradient Boosting Model\n",
    "\n",
    "### Step 1: Create a Base Model\n",
    "\n",
    "The **base model** should not be biased to any value; it provides a default value.  \n",
    "\n",
    "To find this default value, compute the **average of the salary output**:\n",
    "\n",
    "$$\n",
    "\\bar{y} = \\frac{50 + 70 + 80 + 100}{4} = 75 \\text{ K}\n",
    "$$\n",
    "\n",
    "So, the base model always predicts:\n",
    "\n",
    "$$\n",
    "\\hat{y}_0 = 75 \\text{ K}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Compute Residuals\n",
    "\n",
    "The **residuals** or **errors** are computed as:\n",
    "\n",
    "$$\n",
    "r_i = y_i - \\hat{y}_i\n",
    "$$\n",
    "\n",
    "For our dataset:\n",
    "\n",
    "| True Salary ($y$) | Predicted ($\\hat{y}_0$) | Residual ($r_1$) |\n",
    "|-----------------|------------------------|-----------------|\n",
    "| 50              | 75                     | -25             |\n",
    "| 70              | 75                     | -5              |\n",
    "| 80              | 75                     | 5               |\n",
    "| 100             | 75                     | 25              |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Construct a Decision Tree\n",
    "\n",
    "Now, construct a **decision tree** using:\n",
    "\n",
    "- Inputs: \\(X_i\\) (Experience, Degree)  \n",
    "- Output: Residuals \\(r_1\\)\n",
    "\n",
    "This decision tree is trained on the **residuals** from the base model. The output of this tree gives us a new residual \\(r_2\\) for each record.\n",
    "\n",
    "Example outputs for the first tree:\n",
    "\n",
    "| Record | Tree Output ($h_1(x)$) |\n",
    "|--------|-----------------------|\n",
    "| 1      | -23                   |\n",
    "| 2      | -3                    |\n",
    "| 3      | 3                     |\n",
    "| 4      | 20                    |\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: Update Predicted Output with Learning Rate\n",
    "\n",
    "To avoid overfitting, we introduce a **learning rate** \\(\\alpha\\) (between 0 and 1).  \n",
    "\n",
    "Updated prediction:\n",
    "\n",
    "$$\n",
    "\\hat{y}_1 = \\hat{y}_0 + \\alpha \\cdot h_1(x)\n",
    "$$\n",
    "\n",
    "Example (with \\(\\alpha = 0.1\\)):\n",
    "\n",
    "| Record | Base ($\\hat{y}_0$) | Tree Output ($h_1$) | Updated Prediction ($\\hat{y}_1$) |\n",
    "|--------|------------------|--------------------|-------------------------------|\n",
    "| 1      | 75               | -23                | 75 + 0.1 * (-23) = 72.7      |\n",
    "| 2      | 75               | -3                 | 75 + 0.1 * (-3) = 74.7       |\n",
    "\n",
    "Residuals for the next tree:\n",
    "\n",
    "$$\n",
    "r_3 = y - \\hat{y}_1\n",
    "$$\n",
    "\n",
    "Repeat the process:\n",
    "\n",
    "1. Train a new tree on residuals \\(r_3\\)\n",
    "2. Update predictions with learning rate\n",
    "\n",
    "---\n",
    "\n",
    "### Final Function of Gradient Boosting\n",
    "\n",
    "The final prediction function after \\(n\\) trees:\n",
    "\n",
    "$$\n",
    "f(x) = H_0(x) + \\alpha_1 H_1(x) + \\alpha_2 H_2(x) + \\dots + \\alpha_n H_n(x)\n",
    "$$\n",
    "\n",
    "Or in summation notation:\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{i=0}^{n} \\alpha_i H_i(x)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\(H_0(x)\\) = base model  \n",
    "- \\(H_i(x)\\) = output of the \\(i^{th}\\) decision tree  \n",
    "- \\(\\alpha_i\\) = learning rate\n",
    "\n",
    "---\n",
    "\n",
    "### Notes:\n",
    "\n",
    "- Decision trees can be constructed with **MSE**, **variance reduction**, and **gain** criteria.  \n",
    "- Pre-pruning and hyperparameter tuning can be applied to prevent overfitting.  \n",
    "- The same process works for **classification** problems with appropriate loss functions.\n",
    "\n",
    "---\n",
    "\n",
    "This concludes the explanation of **gradient boosting regression**. The next step is **practical implementation** using Python or Scala.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dcff38",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
