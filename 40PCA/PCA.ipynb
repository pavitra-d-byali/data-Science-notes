{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46cf3642",
   "metadata": {},
   "source": [
    "# Introduction to Principal Component Analysis (PCA)\n",
    "\n",
    "Hello guys,  \n",
    "\n",
    "We are going to start a new machine learning algorithm called **Principal Component Analysis (PCA)**, also known as **dimensionality reduction**.\n",
    "\n",
    "---\n",
    "\n",
    "## Why PCA? Understanding the Curse of Dimensionality\n",
    "\n",
    "Before understanding PCA, we need to understand **why we should use PCA** and **what problem it solves** — the **curse of dimensionality**.\n",
    "\n",
    "---\n",
    "\n",
    "### Curse of Dimensionality Example\n",
    "\n",
    "Suppose we have multiple machine learning models:\n",
    "\n",
    "- Model \\( M_1 \\)  \n",
    "- Model \\( M_2 \\)  \n",
    "- Model \\( M_3 \\)  \n",
    "- Model \\( M_4 \\)  \n",
    "- Model \\( M_5 \\)  \n",
    "- Model \\( M_6 \\)  \n",
    "\n",
    "We have a dataset with **500 features**.  \n",
    "\n",
    "**Dimensionality = Number of features**  \n",
    "\n",
    "We want to **predict house prices**, with features such as:\n",
    "\n",
    "- House size  \n",
    "- Number of bedrooms  \n",
    "- Number of bathrooms  \n",
    "- Other features (totaling 500)  \n",
    "\n",
    "---\n",
    "\n",
    "### Accuracy vs Number of Features\n",
    "\n",
    "| Model | Number of Features | Accuracy        |\n",
    "|-------|-----------------|----------------|\n",
    "| M1    | 3               | Accuracy 1     |\n",
    "| M2    | 6               | Accuracy 2 > Accuracy 1 |\n",
    "| M3    | 15              | Accuracy 3 > Accuracy 2 |\n",
    "| M4    | 50              | Accuracy 4 < Accuracy 3 |\n",
    "| M5    | 100             | Accuracy 5 < Accuracy 4 |\n",
    "| M6    | 500             | Accuracy 6 < Accuracy 5 |\n",
    "\n",
    "**Observation:**\n",
    "\n",
    "- Initially, increasing the number of features improves accuracy.  \n",
    "- After a certain point, adding more features **decreases accuracy**.  \n",
    "- This happens because the model gets **overfitted** and confused by less important or redundant features.  \n",
    "\n",
    "---\n",
    "\n",
    "### Intuition: Human Analogy\n",
    "\n",
    "Imagine asking a person to estimate a house price:\n",
    "\n",
    "1. You give **Location** → Person guesses $450k–$500k  \n",
    "2. Add **3 BHK requirement** → Price updated $500k–$600k  \n",
    "3. Add **Beach proximity** → Price increases  \n",
    "4. Add **Celebrity neighbor** → Price increases further  \n",
    "5. Add **Nearby grocery shops, schools, etc.** → Person gets confused → cannot accurately predict  \n",
    "\n",
    "**Lesson:** Too many features confuse the model or expert.  \n",
    "\n",
    "This illustrates the **curse of dimensionality**.\n",
    "\n",
    "---\n",
    "\n",
    "## How to Prevent the Curse of Dimensionality\n",
    "\n",
    "Two main approaches:\n",
    "\n",
    "1. **Feature Selection**\n",
    "\n",
    "   - Select the **most important features**.  \n",
    "   - Train the model using these features only.  \n",
    "\n",
    "2. **Dimensionality Reduction (Feature Extraction)**\n",
    "\n",
    "   - Derive **new features** from the original features.  \n",
    "   - Capture the **essence/variance** of the original features in fewer dimensions.  \n",
    "   - This is what PCA does.\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Extraction Example\n",
    "\n",
    "Suppose original features are \\( F_1, F_2, F_3 \\).  \n",
    "\n",
    "PCA can derive new features:\n",
    "\n",
    "- \\( D_1, D_2 \\) (lesser dimensions)  \n",
    "\n",
    "These **new features** capture the important information from \\( F_1, F_2, F_3 \\) and can be used to predict the output effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Curse of Dimensionality:** Too many features confuse the model and degrade performance.  \n",
    "- **Solution:**  \n",
    "  1. Feature Selection → Pick important features  \n",
    "  2. Dimensionality Reduction → Feature Extraction (PCA)  \n",
    "\n",
    "In the next session, we will **deep dive into PCA**, including:\n",
    "\n",
    "- Geometric intuition  \n",
    "- Mathematical explanation  \n",
    "- Practical implementation  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e730d0c",
   "metadata": {},
   "source": [
    "# Feature Selection vs Feature Extraction\n",
    "\n",
    "In this section, we will discuss the differences between **feature selection** and **feature extraction**.  \n",
    "Both are techniques used in **dimensionality reduction**, which helps us reduce the number of features or extract important features from the existing dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Perform Dimensionality Reduction?\n",
    "\n",
    "Dimensionality reduction is performed for several reasons, often asked in interviews:\n",
    "\n",
    "1. **Prevent Curse of Dimensionality**  \n",
    "2. **Improve Model Performance**  \n",
    "\n",
    "   - More features (dimensions) → More computation  \n",
    "   - Training time increases  \n",
    "   - Dimensionality reduction improves model efficiency  \n",
    "\n",
    "3. **Data Visualization**  \n",
    "\n",
    "   - Humans can visualize up to **3D**  \n",
    "   - High-dimensional data (e.g., 100D) cannot be visualized  \n",
    "   - Reduce dimensions to **2D or 3D** to better understand the data  \n",
    "\n",
    "**Summary:** Dimensionality reduction helps with understanding data, improving performance, and preventing overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Selection\n",
    "\n",
    "**Definition:** Process of selecting the most important features that help predict the output.  \n",
    "\n",
    "### Example: Relationship Between Features\n",
    "\n",
    "Suppose:\n",
    "\n",
    "- Input feature: \\( X \\)  \n",
    "- Output feature: \\( Y \\)  \n",
    "\n",
    "Types of relationships:\n",
    "\n",
    "1. **Positive Linear Relationship:**  \n",
    "   - As \\( X \\) increases, \\( Y \\) increases  \n",
    "   - As \\( X \\) decreases, \\( Y \\) decreases  \n",
    "\n",
    "2. **Negative Linear Relationship:**  \n",
    "   - As \\( X \\) increases, \\( Y \\) decreases  \n",
    "   - As \\( X \\) decreases, \\( Y \\) increases  \n",
    "\n",
    "3. **No Relationship:**  \n",
    "   - \\( X \\) and \\( Y \\) are independent  \n",
    "\n",
    "---\n",
    "\n",
    "### Quantifying Relationships\n",
    "\n",
    "#### Covariance\n",
    "\n",
    "The **covariance** between \\( X \\) and \\( Y \\) is:\n",
    "\n",
    "$$\n",
    "\\text{Cov}(X, Y) = \\frac{\\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\n",
    "$$\n",
    "\n",
    "- **Positive** → Positive linear relationship  \n",
    "- **Negative** → Negative linear relationship  \n",
    "- **Zero** → No relationship  \n",
    "\n",
    "#### Pearson Correlation\n",
    "\n",
    "$$\n",
    "r_{XY} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n",
    "$$\n",
    "\n",
    "- Ranges between **-1 and +1**  \n",
    "- Close to +1 → Strong positive correlation  \n",
    "- Close to -1 → Strong negative correlation  \n",
    "- Close to 0 → No correlation  \n",
    "\n",
    "> Covariance and correlation help identify **important features** for feature selection.\n",
    "\n",
    "---\n",
    "\n",
    "### Housing Dataset Example\n",
    "\n",
    "Features:\n",
    "\n",
    "- Independent: `house_size`, `fountain_size`  \n",
    "- Dependent: `price`  \n",
    "\n",
    "**Observation:**\n",
    "\n",
    "- `house_size` → Strong linear relationship with price → **important feature**  \n",
    "- `fountain_size` → Weak or no relationship → **can be dropped**  \n",
    "\n",
    "This illustrates **feature selection**: keeping the most relevant features.\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Extraction\n",
    "\n",
    "**Definition:** Creating new features from existing features, useful when **all original features are important**.  \n",
    "\n",
    "### Example\n",
    "\n",
    "Suppose features:\n",
    "\n",
    "- `room_size`  \n",
    "- `number_of_rooms`  \n",
    "- Output: `price`  \n",
    "\n",
    "Goal: Reduce from **2 features → 1 feature**  \n",
    "\n",
    "- Both features are important → cannot drop any  \n",
    "- Apply a **transformation** to combine features:  \n",
    "\n",
    "$$\n",
    "\\text{house_size} = f(\\text{room_size}, \\text{number_of_rooms})\n",
    "$$\n",
    "\n",
    "- New feature: `house_size`  \n",
    "- Still predictive of `price`  \n",
    "\n",
    "> This is **feature extraction**: deriving new features to reduce dimensions while retaining information.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "| Aspect                  | Feature Selection                     | Feature Extraction                  |\n",
    "|-------------------------|-------------------------------------|------------------------------------|\n",
    "| Goal                    | Select important features            | Derive new features                 |\n",
    "| Method                  | Drop irrelevant features             | Transform/combine features          |\n",
    "| Use Case                | Some features are irrelevant         | All features are relevant           |\n",
    "| Example                 | Drop `fountain_size`                 | Combine `room_size` & `num_rooms` → `house_size` |\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Dimensionality Reduction** helps prevent the curse of dimensionality, improve performance, and visualize data  \n",
    "- **Feature Selection**: Keep most important features (covariance/correlation)  \n",
    "- **Feature Extraction**: Create new features from existing ones (PCA, transformations)  \n",
    "\n",
    "Next, we will discuss **Principal Component Analysis (PCA)** and its **geometric intuition**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94338765",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
